Index: Listen_Your_Emotion/model_data.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import PIL\r\nimport os\r\nimport cv2\r\nimport torch\r\nimport torchvision\r\nimport torch.nn as nn\r\nimport numpy as np\r\nimport torch.nn.functional as F\r\nfrom torchvision.datasets import ImageFolder\r\nfrom torch.utils.data import DataLoader\r\nimport torchvision.transforms as tt\r\nfrom torchvision.utils import make_grid\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ndef get_default_device():\r\n    if torch.cuda.is_available():\r\n        return torch.device('cuda')\r\n    else:\r\n        return torch.device('cpu')\r\n\r\n\r\ndef to_device(data, device):\r\n    if isinstance(data, (list, tuple)):\r\n        return [to_device(x, device) for x in data]\r\n    return data.to(device, non_blocking=True)\r\n\r\n\r\nclass DeviceDataLoader():\r\n    def __init__(self, dl, device):\r\n        self.dl = dl\r\n        self.device = device\r\n\r\n    def __iter__(self):\r\n        for b in self.dl:\r\n            yield to_device(b, self.device)\r\n\r\n    def __len__(self):\r\n        return len(self.dl)\r\n\r\n\r\ndef accuracy(outputs, labels):\r\n    _, preds = torch.max(outputs, dim=1)\r\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\r\n\r\n\r\nclass ImageClassificationBase(nn.Module):\r\n    def training_step(self, batch):\r\n        images, labels = batch\r\n        out = self(images)\r\n        loss = F.cross_entropy(out, labels)\r\n        return loss\r\n\r\n    def validation_step(self, batch):\r\n        images, labels = batch\r\n        out = self(images)\r\n        loss = F.cross_entropy(out, labels)\r\n        acc = accuracy(out, labels)\r\n        return {'val_loss': loss.detach(), 'val_acc': acc}\r\n\r\n    def pred_step(self, batch):\r\n        out = self(batch)\r\n        return out\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        batch_losses = [x['val_loss'] for x in outputs]\r\n        epoch_loss = torch.stack(batch_losses).mean()\r\n        batch_accs = [x['val_acc'] for x in outputs]\r\n        epoch_acc = torch.stack(batch_accs).mean()\r\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\r\n\r\n\r\n\r\n\r\ndef conv_block(in_channels, out_channels, pool=False):\r\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\r\n              nn.BatchNorm2d(out_channels),\r\n              nn.ELU(inplace=True)]\r\n    if pool: layers.append(nn.MaxPool2d(2))\r\n    return nn.Sequential(*layers)\r\n\r\n\r\nclass ResNet(ImageClassificationBase):\r\n    def __init__(self, in_channels, num_classes):\r\n        super().__init__()\r\n\r\n        self.conv1 = conv_block(in_channels, 128)\r\n        self.conv2 = conv_block(128, 128, pool=True)\r\n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\r\n        self.drop1 = nn.Dropout(0.5)\r\n\r\n        self.conv3 = conv_block(128, 256)\r\n        self.conv4 = conv_block(256, 256, pool=True)\r\n        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\r\n        self.drop2 = nn.Dropout(0.5)\r\n\r\n        self.conv5 = conv_block(256, 512)\r\n        self.conv6 = conv_block(512, 512, pool=True)\r\n        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\r\n        self.drop3 = nn.Dropout(0.5)\r\n\r\n        self.classifier = nn.Sequential(nn.MaxPool2d(6),\r\n                                        nn.Flatten(),\r\n                                        nn.Linear(512, num_classes))\r\n\r\n    def forward(self, xb):\r\n        out = self.conv1(xb)\r\n        out = self.conv2(out)\r\n        out = self.res1(out) + out\r\n        out = self.drop1(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.conv4(out)\r\n        out = self.res2(out) + out\r\n        out = self.drop2(out)\r\n\r\n        out = self.conv5(out)\r\n        out = self.conv6(out)\r\n        out = self.res3(out) + out\r\n        out = self.drop3(out)\r\n\r\n        out = self.classifier(out)\r\n        return out\r\ndef predict(model, pred_loader):\r\n    model.eval()\r\n    outputs = [model.pred_step(batch) for batch in pred_loader]\r\n    return [torch.max(el, dim=1)[1] for el in outputs]\r\n\r\n\r\n@torch.no_grad()\r\ndef evaluate(model, val_loader):\r\n    model.eval()\r\n    outputs = [model.validation_step(batch) for batch in val_loader]\r\n    return model.validation_epoch_end(outputs)\r\n\r\n\r\ndef predict(model, pred_loader):\r\n    model.eval()\r\n    outputs = [model.pred_step(batch) for batch in pred_loader]\r\n    return [torch.max(el, dim=1)[1] for el in outputs]\r\n\r\n\r\ndef get_lr(optimizer):\r\n    for param_group in optimizer.param_groups:\r\n        return param_group['lr']\r\n\r\n\r\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\r\n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\r\n    torch.cuda.empty_cache()\r\n    history = []\r\n\r\n    # Set up custom optimizer with weight decay\r\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\r\n    # Set up one-cycle learning rate scheduler\r\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\r\n                                                steps_per_epoch=len(train_loader))\r\n\r\n    for epoch in range(epochs):\r\n        # Training Phase\r\n        model.train()\r\n        train_losses = []\r\n        lrs = []\r\n        for batch in train_loader:\r\n            loss = model.training_step(batch)\r\n            train_losses.append(loss)\r\n            loss.backward()\r\n\r\n            # Gradient clipping\r\n            if grad_clip:\r\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\r\n\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n            # Record & update learning rate\r\n            lrs.append(get_lr(optimizer))\r\n            sched.step()\r\n\r\n        # Validation phase\r\n        result = evaluate(model, val_loader)\r\n\r\n        result['train_loss'] = torch.stack(train_losses).mean().item()\r\n        result['lrs'] = lrs\r\n        model.epoch_end(epoch, result)\r\n        history.append(result)\r\n    return history\r\n\r\n\r\ndef train_model(epochs, max_lr, model, train_loader, val_loader, device,\r\n                weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\r\n    # The training configurations were not carefully selected.\r\n\r\n    model.to(device)\r\n\r\n    torch.cuda.empty_cache()\r\n    history = []\r\n\r\n    # Set up custom optimizer with weight decay\r\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\r\n    # Set up one-cycle learning rate scheduler\r\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\r\n                                                steps_per_epoch=len(train_loader))\r\n\r\n    for epoch in range(epochs):\r\n        # Training Phase\r\n        model.train()\r\n        train_losses = []\r\n        lrs = []\r\n        for batch in train_loader:\r\n            loss = model.training_step(batch)\r\n            train_losses.append(loss)\r\n            loss.backward()\r\n\r\n            # Gradient clipping\r\n            if grad_clip:\r\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\r\n\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n            # Record & update learning rate\r\n            lrs.append(get_lr(optimizer))\r\n            sched.step()\r\n\r\n        # Validation phase\r\n        result = evaluate(model, val_loader)\r\n        print(result)\r\n        # model.epoch_end(epoch, result)\r\n\r\n    return model
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Listen_Your_Emotion/model_data.py b/Listen_Your_Emotion/model_data.py
--- a/Listen_Your_Emotion/model_data.py	(revision 1f7cf861b51d44cc954127487319f82cd1994473)
+++ b/Listen_Your_Emotion/model_data.py	(date 1717058415213)
@@ -31,6 +31,7 @@
         self.dl = dl
         self.device = device
 
+
     def __iter__(self):
         for b in self.dl:
             yield to_device(b, self.device)
Index: Listen_Your_Emotion/app_first.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import sys, os\r\nimport sqlite3\r\nimport hashlib\r\nimport time\r\nimport os\r\nfrom PyQt5 import QtCore, QtGui, QtWidgets\r\nfrom PyQt5.QtCore import QUrl, QSize, QRect\r\nfrom PyQt5.QtWidgets import (QWidget, QPushButton, QApplication, QSlider, QStyle,\r\n                             QSizePolicy, QHBoxLayout, QLabel, QVBoxLayout,\r\n                             QSplashScreen, QMainWindow, QFrame)\r\nfrom PyQt5.QtCore import Qt, QTimer, QPropertyAnimation, QEasingCurve, QUrl, QDir\r\nfrom PyQt5.QtGui import QPixmap, QPalette, QColor, QPainter, QBitmap, QIcon\r\nfrom PyQt5.QtWidgets import QApplication, QDialog, QStackedWidget\r\nfrom PyQt5.QtMultimedia import QMediaContent, QMediaPlayer\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QLineEdit, QLabel, QMessageBox\r\n\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QLineEdit, QLabel, QMessageBox\r\nfrom PyQt5.QtCore import Qt, QTimer, QPropertyAnimation, QUrl, QEasingCurve, QDir\r\nfrom PyQt5.QtGui import QPixmap, QPalette, QColor\r\nfrom PyQt5.QtMultimedia import QMediaContent, QMediaPlayer\r\nfrom PyQt5.QtWidgets import QApplication, QDialog, QPushButton\r\nfrom databases import Database\r\nfrom PyQt5.QtCore import pyqtSignal\r\n\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QLineEdit, QLabel, QMessageBox\r\nfrom PyQt5.QtCore import pyqtSignal\r\nfrom PyQt5.uic import loadUi\r\nfrom PyQt5 import QtWidgets\r\nfrom PyQt5.QtWidgets import QDialog, QApplication, QWidget\r\nfrom PyQt5.QtGui import QPixmap\r\nfrom PyQt5.uic import loadUi\r\nfrom PyQt5.QtWidgets import QDialog, QApplication, QPushButton, QVBoxLayout, QLineEdit, QLabel, QMessageBox\r\nfrom PyQt5.QtCore import Qt, QTimer\r\nfrom PyQt5.uic import loadUi\r\nfrom PyQt5.QtGui import QColor, QPainter\r\nfrom PyQt5.QtCore import QTimer, Qt\r\nfrom music_storage.queue_maker import update_queue\r\nfrom model_data import *\r\nfrom face_getter import make_photo\r\n\r\nglobals()\r\nmain_user = None\r\nPLAYER_PATH = \"./player/\"\r\nCURRENT_EMOTION = \"Calm\"\r\n\r\n\r\nclass WidgetControl():\r\n    def __init__(self):\r\n        self.widget = QStackedWidget()\r\n        self.welcome = WelcomeScreen()\r\n        self.widget.addWidget(self.welcome)\r\n\r\n        self.login = LoginScreen()\r\n        self.widget.addWidget(self.login)\r\n        self.create = CreateAccScreen()\r\n        self.widget.addWidget(self.create)\r\n\r\n        self.mainWindow = MainWindow()\r\n        self.widget.addWidget(self.mainWindow)\r\n        self.widget.setFixedHeight(450)\r\n        self.widget.setFixedWidth(600)\r\n        self.widget.show()\r\n\r\n        self.splash = self.mainWindow.show_splash()\r\n        self.splash.show()\r\n\r\n\r\nclass WelcomeScreen(QDialog):\r\n\r\n    def __init__(self):\r\n        super(WelcomeScreen, self).__init__()\r\n        loadUi(PLAYER_PATH + \"LYE.ui\", self)\r\n\r\n        self.login.clicked.connect(self.login_account)\r\n        self.create.clicked.connect(self.create_account)\r\n\r\n    # for login connection\r\n    def login_account(self):\r\n        open_login_widget = widget.currentIndex() + 1\r\n        widget.setCurrentIndex(open_login_widget)\r\n        index = widget.indexOf(self.login)\r\n\r\n    # for create connection\r\n    def create_account(self):\r\n        open_login_widget = widget.currentIndex() + 2\r\n        widget.setCurrentIndex(open_login_widget)\r\n        index = widget.indexOf(self.create)\r\n\r\n\r\nclass LoginScreen(QDialog):\r\n\r\n    def __init__(self):\r\n        super(LoginScreen, self).__init__()\r\n        loadUi(PLAYER_PATH + \"LYElog.ui\", self)\r\n        self.passwordd.setEchoMode(QtWidgets.QLineEdit.Password)\r\n        self.loginn.clicked.connect(self.login_function)\r\n        self.prev.clicked.connect(self.prev_function)\r\n\r\n    # back button\r\n    def prev_function(self):\r\n        self.error_message1.clear()\r\n        current_index = widget.currentIndex()\r\n\r\n        if current_index > 0:\r\n            return_welcome_widget = current_index - 1\r\n            widget.setCurrentIndex(return_welcome_widget)\r\n\r\n    def login_function(self):\r\n        user = self.name.text()\r\n        password = self.passwordd.text()\r\n\r\n        if len(user) == 0 or len(password) == 0:\r\n            self.error_message1.setText(\"Not all fields are filled in.\")\r\n            return\r\n\r\n        else:\r\n            conn = sqlite3.connect(PLAYER_PATH + \"shop_data.db\")\r\n            cur = conn.cursor()\r\n            data = 'SELECT password FROM login_info WHERE username =\\'' + user + \"\\'\"\r\n            cur.execute(data)\r\n            result_pass = cur.fetchone()\r\n\r\n            if result_pass != None and result_pass[0] == password:\r\n\r\n                global main_user\r\n                main_user = user\r\n                current_index = widget.currentIndex()\r\n                open_main_window = current_index + 2\r\n                if current_index > 0:\r\n                    widget.setCurrentIndex(open_main_window)\r\n            else:\r\n                self.error_message1.setText(\"Invalid filling\")\r\n\r\n\r\nclass CreateAccScreen(QMainWindow):\r\n\r\n    def __init__(self):\r\n        super(CreateAccScreen, self).__init__()\r\n        loadUi(PLAYER_PATH + \"LYEcreate.ui\", self)\r\n\r\n        self.passwordd.setEchoMode(QtWidgets.QLineEdit.Password)\r\n        self.passwordd_2.setEchoMode(QtWidgets.QLineEdit.Password)\r\n        self.signup.clicked.connect(self.signupfunction)\r\n        self.prev.clicked.connect(self.prevfunction)\r\n\r\n    # back button\r\n    def prevfunction(self):\r\n        self.error_message.clear()\r\n        current_index = widget.currentIndex()\r\n        if current_index > 0:\r\n            widget.setCurrentIndex(current_index - 2)\r\n\r\n    def signupfunction(self):\r\n        user = self.name.text()\r\n        password = self.passwordd.text()\r\n        confirmpassword = self.passwordd_2.text()\r\n\r\n        if len(user) == 0 or len(password) == 0 or len(confirmpassword) == 0:\r\n            self.error_message.setText(\"Not all fields are filled in.\")\r\n\r\n        elif password != confirmpassword:\r\n            self.error_message.setText(\"Passwords mismatch.\")\r\n        else:\r\n            conn = sqlite3.connect(PLAYER_PATH + \"shop_data.db\")\r\n            cur = conn.cursor()\r\n            user_info = [user, password]\r\n\r\n            cur.execute('INSERT INTO login_info (username, password) VALUES (?,?)', user_info)\r\n\r\n            self.error_message.setText(\"Creation is comlete. Go to LOGIN.\")\r\n            conn.commit()\r\n            conn.close()\r\n\r\n\r\n# profile LYE\r\nclass Profile(QDialog):\r\n\r\n    def __init__(self):\r\n        super(Profile, self).__init__()\r\n        loadUi(PLAYER_PATH + \"profile.ui\", self)\r\n        self.save_ch.clicked.connect(self.save_chh())\r\n        self.prevv.clicked.connect(self.prevfunction())\r\n        self.user_namename.setText(main_user)\r\n\r\n    def prevfunction(self):\r\n        self.error_message1.clear()\r\n        current_index = widget.currentIndex()\r\n        if current_index > 0:\r\n            widget.setCurrentIndex(current_index - 2)\r\n\r\n    def save_chh(self):\r\n        user1 = self.firstname.text()\r\n        user2 = self.lastname.text()\r\n\r\n        if len(user1) == 0 or len(user2) == 0:\r\n            self.label_2.setText(\"Not all fields are filled in.\")\r\n\r\n        else:\r\n            conn = sqlite3.connect(PLAYER_PATH + \"shop_data.db\")\r\n            cur = conn.cursor()\r\n            user_info = [user1, user2]\r\n            cur.execute('INSERT INTO login_info_app (first_name, last_name) VALUES (?,?)', user_info)\r\n\r\n            conn.commit()\r\n            conn.close()\r\n            current_index = widget.currentIndex()\r\n            if current_index > 0:\r\n                widget.setCurrentIndex(current_index - 2)\r\n\r\n\r\ndef model_init():\r\n    global BATCH_SIZE\r\n    global result_tfms\r\n    global model\r\n    global classes\r\n    global device\r\n    BATCH_SIZE = 200\r\n    classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Calm', 'Sad', 'Surprise']\r\n    result_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])\r\n    device = get_default_device()\r\n    device = 'cpu'\r\n    #model = ResNet(1, len(classes))\r\n    #model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\r\n    model = torch.load('./models/quant_model_scripted.pt', map_location='cpu')\r\n    model = to_device(model, device)\r\n\r\n\r\n\r\ndef emotion_convert(emotion):\r\n    if emotion.lower() in ['angry', 'disgust', 'fear', 'sad']:\r\n        return 'sad'\r\n    elif emotion.lower() == 'calm':\r\n        return 'calm'\r\n    else:\r\n        return 'happy'\r\n\r\n\r\ndef choose_playlist(number_):  # for connection with other members\r\n\r\n    global choose_em_playlist\r\n\r\n    if number_ == 1:\r\n        choose_em_playlist = {\r\n            \"voila.mp3\": \"sad.jpg\",\r\n            \"sad_snow.mp3\": \"sad.jpg\",\r\n            \"sad_pretend.mp3\": \"sad.jpg\"\r\n        }\r\n    elif number_ == 2:\r\n        choose_em_playlist = {\r\n            \"happy_love.mp3\": \"happy.jpg\",\r\n            \"sedaja.mp3\": \"happy.jpg\",\r\n            \"happy_ball.mp3\": \"happy.jpg\"\r\n        }\r\n    elif number_ == 3:\r\n        choose_em_playlist = {\r\n            \"aach.mp3\": \"neu.jpg\",\r\n            \"neu_sen.mp3\": \"neu.jpg\",\r\n            \"neu_shopen.mp3\": \"neu.jpg\"\r\n        }\r\n\r\n\r\n# emotion addition\r\nclass EmotionPanel(QWidget):\r\n    def __init__(self, light_theme=True):\r\n        super().__init__()\r\n        self.light_theme = light_theme\r\n        self.initUI()\r\n\r\n    def initUI(self):\r\n        def_layout = QVBoxLayout()  # for inscription\r\n        emotion_layout = QHBoxLayout()  # for emotion buttons\r\n\r\n        # inscription\r\n        self.label = QLabel(\"Listen your emotion\")\r\n        self.label.setAlignment(Qt.AlignmentFlag.AlignCenter)\r\n\r\n        if self.light_theme:\r\n            self.label.setStyleSheet(\r\n                \"font-family: 'Snell Roundhand', cursive; font-size: 25px; font-stretch: normal; color: #222222;\")\r\n        else:\r\n            self.label.setStyleSheet(\r\n                \"font-family: 'Snell Roundhand', cursive; font-size: 25px; font-stretch: normal; color: #FFFFFF;\")\r\n\r\n        def_layout.addWidget(self.label)\r\n\r\n        # EMOTION PANEL\r\n        self.btn_sad = QPushButton('Sad')\r\n        self.btn_neutral = QPushButton('Neutral')\r\n        self.btn_happy = QPushButton('Happy')\r\n\r\n        emotion_layout.addWidget(self.btn_sad)\r\n        emotion_layout.addWidget(self.btn_neutral)\r\n        emotion_layout.addWidget(self.btn_happy)\r\n\r\n        # touch response\r\n        self.btn_sad.clicked.connect(self.set_sad)\r\n        self.btn_neutral.clicked.connect(self.set_neu)\r\n        self.btn_happy.clicked.connect(self.set_happy)\r\n\r\n        def_layout.addLayout(emotion_layout)\r\n        self.setLayout(def_layout)\r\n\r\n    def reset_colors(self):  # reset prev style\r\n        self.btn_sad.setStyleSheet(\"\")\r\n        self.btn_neutral.setStyleSheet(\"\")\r\n        self.btn_happy.setStyleSheet(\"\")\r\n\r\n    def set_sad(self):\r\n        global CURRENT_EMOTION\r\n        CURRENT_EMOTION = \"Sad\"\r\n\r\n        self.reset_colors()\r\n        self.btn_sad.setStyleSheet(\"background-color: #9370DB;\")\r\n\r\n    def set_neu(self):\r\n        global CURRENT_EMOTION\r\n        CURRENT_EMOTION = \"Calm\"\r\n\r\n        self.reset_colors()\r\n        self.btn_neutral.setStyleSheet(\"background-color: #87CEEB;\")\r\n\r\n    def set_happy(self):\r\n        global CURRENT_EMOTION\r\n        CURRENT_EMOTION = \"Happy\"\r\n\r\n        self.reset_colors()\r\n        self.btn_happy.setStyleSheet(\"background-color: #90EE90;\")\r\n\r\n    def update_theme(self, light_theme):\r\n        self.light_theme = light_theme\r\n        if self.light_theme:\r\n            self.label.setStyleSheet(\r\n                \"font-family: 'Snell Roundhand', cursive; font-size: 25px; font-stretch: normal; color: #222222;\")\r\n        else:\r\n            self.label.setStyleSheet(\r\n                \"font-family: 'Snell Roundhand', cursive; font-size: 25px; font-stretch: normal; color: #FFFFFF;\")\r\n\r\n\r\nclass MainWindow(QWidget):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.tracks = {\r\n            PLAYER_PATH + \"aach.mp3\": PLAYER_PATH + \"neu.jpg\",\r\n            PLAYER_PATH + \"sedaja.mp3\": PLAYER_PATH + \"happy.jpg\",\r\n            PLAYER_PATH + \"voila.mp3\": PLAYER_PATH + \"sad.jpg\"\r\n        }\r\n\r\n        self.setGeometry(300, 300, 600, 400)\r\n        self.setWindowTitle('LYE')\r\n\r\n        self.current_index = 0  # for songs\r\n        self.current_playlist = 3  # neutral - basic\r\n        self.playlist_index = {}\r\n\r\n        # color theme for now\r\n        self.light_theme = True\r\n\r\n        # button to save your favorite tracks\r\n        self.heart_button = QPushButton(\"❤\")\r\n        self.heart_button.setFixedSize(30, 30)\r\n        self.heart_button.setToolTip(\"Add to favorites\")\r\n        self.heart_button.setStyleSheet(\"background-color: #BA55D3; border-radius: 5px;\")\r\n        self.heart_button.clicked.connect(self.show_message)\r\n\r\n        self.center_screen()\r\n\r\n        # side buttons\r\n\r\n        self.create_tag(\"LYE\")\r\n        self.create_profile_button(\"Your Lye\")\r\n        self.initUI()\r\n        self.set_theme(self.light_theme)\r\n        self.show()\r\n\r\n    def final_predict(self):\r\n\r\n        global result_tfms\r\n        global classes\r\n        global device\r\n        global model\r\n\r\n        make_photo()\r\n\r\n        def local_pred_step(batch):\r\n\r\n            images = batch\r\n            out = model(images)\r\n\r\n            return out\r\n\r\n        def local_predict( pred_loader):\r\n            outputs = [local_pred_step(batch) for batch in pred_loader]\r\n            return [torch.max(el, dim=1)[1] for el in outputs]\r\n        data = [result_tfms(PIL.Image.open('./photos/' + os.listdir('./photos/')[-1]).resize((48, 48)))]\r\n        data_dl = DataLoader(data, 200,  pin_memory=True)\r\n        data_dl = DeviceDataLoader(data_dl, device)\r\n        start_time = time.time()\r\n\r\n        pred = local_predict(data_dl)[0][0]\r\n        #pred = predict(model, data_dl)[0][0]\r\n        print(\"--- %s seconds to predict ---\\n\" % (time.time() - start_time))\r\n        print(pred)\r\n\r\n\r\n        # save the model and check the model size\r\n        def print_size_of_model(model, label=\"\"):\r\n            torch.save(model.state_dict(), \"temp.p\")\r\n            size = os.path.getsize(\"temp.p\")\r\n            print(\"model: \", label, ' \\t', 'Size (KB):', size / 1e3)\r\n            os.remove('temp.p')\r\n            return size\r\n        print_size_of_model(model)\r\n\r\n        return classes[pred]\r\n\r\n    def get_fourth_track_path(self, directory):\r\n        temp_directory = f\"./music_storage/music_queues/{directory}\"\r\n\r\n        files = [os.path.join(temp_directory, f) for f in os.listdir(temp_directory)]\r\n        files = [f for f in files if os.path.isfile(f)]\r\n        files.sort(key=lambda x: os.path.getmtime(x))\r\n        # всегда возвращаем путь к четвертому файлу; можно создать очередь как массив названий и обновлять тут\r\n\r\n        return files[3]\r\n\r\n    def nextTrack(self):\r\n        global CURRENT_EMOTION\r\n\r\n        CURRENT_EMOTION = emotion_convert(self.final_predict()).capitalize()  # Перенести в конец, брать из текущей эмоции для ускорения\r\n\r\n        update_queue(CURRENT_EMOTION)\r\n\r\n        self.open_file()\r\n\r\n    # central location\r\n    def center_screen(self):\r\n        app_geometry = QApplication.desktop().availableGeometry()\r\n        wind_geometry = self.frameGeometry()\r\n        wind_geometry.moveCenter(app_geometry.center())\r\n        self.move(wind_geometry.topLeft())\r\n\r\n    # tag\r\n    def create_tag(self, text):\r\n        button = QPushButton(text, self)\r\n        button_size = 45\r\n        button.setGeometry(10, 10, button_size + 20, button_size)\r\n        button.setStyleSheet(\r\n            \"QPushButton {\"\r\n            \"    background-color: qlineargradient(x1:0, y1:0, x2:0, y2:1, stop:0 #FFFFFF, stop:1 #AAAAAA);\"\r\n            \"    border: none;\"\r\n            \"    font-weight: bold;\"\r\n            \"    text-shadow: 2px 2px 2px rgba(0, 0, 0, 0.5);\"\r\n            \"    color: #222222;\"\r\n            f\"    border-radius: {button_size // 2}px;\"\r\n            \"}\"\r\n        )\r\n\r\n    def create_profile_button(self, text):\r\n        profile_button = QPushButton(text, self)\r\n        button_size = 65\r\n        profile_button.setGeometry(10, 60, button_size, 45)\r\n        profile_button.setStyleSheet(\r\n            \"QPushButton {\"\r\n            \"    background-color: qlineargradient(x1:0, y1:0, x2:0, y2:1, stop:0 #FFFFFF, stop:1 #AAAAAA);\"\r\n            \"    border: none;\"\r\n            \"    text-shadow: 2px 2px 2px rgba(0, 0, 0, 0.5);\"\r\n            \"    color: #222222;\"\r\n            f\"    border-radius: 15px;\"\r\n            \"}\"\r\n            \"QPushButton:hover {\"\r\n            \"    background-color: #87CEEB;\"\r\n            \"}\"\r\n        )\r\n\r\n        profile_button.clicked.connect(self.fill)\r\n\r\n    def initUI(self):\r\n\r\n        self.initTracks()\r\n\r\n        self.player = QMediaPlayer()\r\n        self.layout = QVBoxLayout()\r\n\r\n        self.setLayout(self.layout)\r\n        button_area = QWidget()\r\n        button_layout = QHBoxLayout(button_area)\r\n        button_area.setStyleSheet(\"background-color: white; border-radius: 10px;\")\r\n        button_area.setFixedHeight(50)\r\n        volumeControl = QHBoxLayout()\r\n        musicControl = QHBoxLayout()\r\n\r\n        # images\r\n        self.image_label = QLabel()\r\n        self.image_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\r\n        self.layout.addWidget(self.image_label)\r\n\r\n        default_image = PLAYER_PATH + \"aaa.jpg\"\r\n        pixmap = QPixmap(default_image)\r\n        pixmap = pixmap.scaled(400, 300, Qt.AspectRatioMode.KeepAspectRatio)\r\n        self.image_label.setPixmap(pixmap)\r\n        self.image_label.adjustSize()\r\n\r\n        # buttons\r\n        btn5 = QPushButton('Next', clicked=self.nextTrack)\r\n        btn6 = QPushButton('Prev', clicked=self.prev_m)\r\n        btn4 = QPushButton(clicked=self.volume_st)\r\n        btn4.setIcon(self.style().standardIcon(QStyle.SP_MediaPause))\r\n        btn1 = QPushButton(clicked=self.open_file)\r\n        btn1.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\r\n\r\n        # slider\r\n        self.volume_slider = QSlider(Qt.Horizontal)\r\n        self.volume_slider.setRange(0, 100)\r\n        self.volume_slider.setValue(20)\r\n        self.volume_slider.valueChanged.connect(self.change_volume)\r\n        volume_layout = QHBoxLayout()\r\n        volume_layout.addWidget(self.volume_slider)\r\n\r\n        # play\r\n        self.btn_play = QPushButton()\r\n        self.btn_play.setEnabled(False)\r\n        self.btn_play.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\r\n\r\n        # label\r\n        self.label = QLabel()\r\n        self.layout.addLayout(musicControl)\r\n        self.layout.addLayout(volumeControl)\r\n        self.layout.addLayout(button_layout)\r\n\r\n        # add buttons\r\n        button_layout.addWidget(btn6)\r\n        button_layout.addWidget(btn4)\r\n        button_layout.addWidget(btn1)\r\n        button_layout.addWidget(btn5)\r\n        button_layout.addWidget(self.volume_slider)\r\n        button_layout.addWidget(self.heart_button)\r\n\r\n        # Desired order for arrangement\r\n\r\n        button_layout.setStretch(0, 1)\r\n        button_layout.setStretch(1, 1)\r\n        button_layout.setStretch(2, 1)\r\n        button_layout.setStretch(3, 1)\r\n        button_layout.setStretch(4, 4)\r\n        button_layout.setStretch(5, 1)\r\n\r\n        # emotional buttons\r\n\r\n        self.emotion_panel = EmotionPanel()  # Define emotion_panel as an instance attribute\r\n        self.layout.addWidget(self.emotion_panel)\r\n\r\n        # for low panel\r\n\r\n        button_area.setFixedSize(550, 50)\r\n        self.layout.addWidget(button_area)\r\n\r\n        # connect emotional\r\n\r\n        self.emotion_panel.btn_sad.clicked.connect(self.sad_playlist)\r\n        self.emotion_panel.btn_neutral.clicked.connect(self.happy_playlist)\r\n        self.emotion_panel.btn_happy.clicked.connect(self.neutral_playlist)\r\n\r\n        self.player = QMediaPlayer()\r\n        self.player.mediaStatusChanged.connect(self.on_media_status_changed)\r\n\r\n    def set_theme(self, light_theme):\r\n        self.light_theme = light_theme\r\n        color_hex = \"#AAAAAA\" if self.light_theme else \"#FFFFFF\"\r\n        color = QColor(color_hex)\r\n        c = self.palette()\r\n        c.setColor(QPalette.Window, color)\r\n        self.setPalette(c)\r\n        self.emotion_panel.update_theme(self.light_theme)\r\n\r\n        # sun icon\r\n        self.theme_button = QPushButton(self)\r\n        self.theme_button.setIcon(QIcon(PLAYER_PATH + \"sun.jpg\"))\r\n        self.theme_button.setIconSize(QSize(30, 30))\r\n        self.theme_button.clicked.connect(self.change_theme)\r\n        self.theme_button.setGeometry(QRect(self.width() - 40, 10, 30, 30))\r\n        self.theme_button.move(self.width() - 40, 10)\r\n        self.theme_button.setStyleSheet(\"border-radius: 15px;\")\r\n\r\n    def change_theme(self):\r\n\r\n        self.set_theme(not self.light_theme)\r\n\r\n    def fill(self):\r\n\r\n        profile_dialog = Profile()\r\n        profile_dialog.show()\r\n\r\n    def show_message(self):\r\n        QMessageBox.about(self, \"Message\", \"Added to favorites ❤ \")\r\n\r\n    def set_playlist(self, playlist):  # for connection words and numbers\r\n        if playlist == 1:\r\n            choose_playlist(1)\r\n            self.current_playlist = 1\r\n            self.tracks = choose_em_playlist\r\n        elif playlist == 2:\r\n            choose_playlist(2)\r\n            self.current_playlist = 2\r\n            self.tracks = choose_em_playlist\r\n        elif playlist == 3:\r\n            choose_playlist(3)\r\n            self.current_playlist = 3\r\n            self.tracks = choose_em_playlist\r\n        self.current_index = self.playlist_index.get(self.current_playlist,\r\n                                                     0)  # index of the current track in this playlist\r\n        self.open_file()\r\n\r\n    def sad_playlist(self):\r\n        self.set_playlist(1)\r\n\r\n    def neutral_playlist(self):\r\n        self.set_playlist(2)\r\n\r\n    def happy_playlist(self):\r\n        self.set_playlist(3)\r\n\r\n    def change_volume(self, value):\r\n        self.player.setVolume(value)\r\n\r\n    def initTracks(self):\r\n        choose_playlist(self.current_playlist)\r\n        self.playlist_index = {1: 0, 2: 0, 3: 0}\r\n        self.tracks = choose_em_playlist\r\n\r\n    def volume_pl(self):\r\n        curr = self.player.volume()\r\n        self.player.setVolume(curr + 20)\r\n\r\n    def volume_ms(self):\r\n        curr = self.player.volume()\r\n        self.player.setVolume(curr - 20)\r\n\r\n    def volume_st(self):\r\n        self.player.setMuted(not self.player.isMuted())\r\n\r\n    def open_file(self):\r\n\r\n        directory = os.path.join(CURRENT_EMOTION)\r\n        fourth_track_path = self.get_fourth_track_path(directory)\r\n\r\n        self.update_image()\r\n        self.player.setMedia(QMediaContent(QUrl.fromLocalFile(fourth_track_path)))\r\n        self.player.play()\r\n\r\n        # current_track = list(self.tracks.keys())[self.current_index]\r\n        # track_path = os.path.join(os.getcwd(), current_track)\r\n        # media = QMediaContent(QUrl.fromLocalFile(track_path))\r\n\r\n        # self.player.stop()\r\n        # self.player.setMedia(media)\r\n        # self.player.play()\r\n\r\n    def next_m(self):\r\n        self.current_index = (1 + self.current_index) % len(self.tracks)\r\n        self.playlist_index[self.current_playlist] = self.current_index\r\n        self.open_file()\r\n\r\n    def on_media_status_changed(self, state):  # auto transition to next track\r\n        if state == QMediaPlayer.EndOfMedia:\r\n            self.next_m()\r\n\r\n    def prev_m(self):\r\n        self.current_index = (self.current_index - 1) % len(self.tracks)\r\n        self.playlist_index[self.current_playlist] = self.current_index\r\n        self.open_file()\r\n\r\n    def update_image(self):\r\n\r\n        if CURRENT_EMOTION == 'Calm':\r\n            image_path = PLAYER_PATH + \"neu.jpg\"\r\n            self.emotion_panel.set_neu()\r\n        elif CURRENT_EMOTION == 'Sad':\r\n            image_path = PLAYER_PATH + 'sad.JPG'\r\n            self.emotion_panel.set_sad()\r\n        elif CURRENT_EMOTION == 'Happy':\r\n            image_path = PLAYER_PATH + 'happy.JPG'\r\n            self.emotion_panel.set_happy()\r\n        else:\r\n            image_path = PLAYER_PATH + \"lye.jpg\"\r\n        pixmap = QPixmap(image_path)\r\n        pixmap = pixmap.scaled(400, 300, Qt.AspectRatioMode.KeepAspectRatio)\r\n        self.image_label.setPixmap(pixmap)\r\n        self.image_label.adjustSize()\r\n\r\n    def show_splash(self):\r\n        pixmap = QPixmap(PLAYER_PATH + \"LYEE.jpg\")\r\n        pixmap = pixmap.scaled(600, 450)\r\n\r\n        splash = QSplashScreen(pixmap)\r\n        splash.setFixedSize(pixmap.size())\r\n        splash.setWindowFlags(Qt.WindowStaysOnTopHint | Qt.FramelessWindowHint)\r\n        splash.move(splash.pos().x() + 0, splash.pos().y() - 24)\r\n        splash.show()\r\n\r\n        # add animation for splash-screen\r\n\r\n        animation = QPropertyAnimation(splash, b\"windowOpacity\")\r\n        animation.setDuration(3000)\r\n        animation.setStartValue(0.96)\r\n        animation.setEndValue(0.0)\r\n        animation.setEasingCurve(QEasingCurve.OutCubic)\r\n        animation.start()\r\n\r\n        QTimer.singleShot(1500, splash.close)\r\n        splash.show()\r\n        return splash\r\n\r\n\r\napp = QApplication(sys.argv)\r\n\r\nwidget = QStackedWidget()\r\nwelcome = WelcomeScreen()\r\nwidget.addWidget(welcome)\r\n\r\nlogin = LoginScreen()\r\nwidget.addWidget(login)\r\ncreate = CreateAccScreen()\r\nwidget.addWidget(create)\r\n\r\nmainWindow = MainWindow()\r\nwidget.addWidget(mainWindow)\r\nwidget.setFixedHeight(450)\r\nwidget.setFixedWidth(600)\r\n\r\nif __name__ == \"__main__\":\r\n    model_init()\r\n    app = QApplication(sys.argv)\r\n\r\n    widget = QStackedWidget()\r\n    welcome = WelcomeScreen()\r\n    widget.addWidget(welcome)\r\n\r\n    login = LoginScreen()\r\n    widget.addWidget(login)\r\n    create = CreateAccScreen()\r\n    widget.addWidget(create)\r\n\r\n    mainWindow = MainWindow()\r\n    widget.addWidget(mainWindow)\r\n    widget.setFixedHeight(450)\r\n    widget.setFixedWidth(600)\r\n    widget.show()\r\n\r\n    splash = mainWindow.show_splash()\r\n    splash.show()\r\n\r\n    QTimer.singleShot(1500, lambda: widget.setCurrentIndex(-1))\r\n    try:\r\n        sys.exit(app.exec_())\r\n    except:\r\n        print(\"Exiting\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Listen_Your_Emotion/app_first.py b/Listen_Your_Emotion/app_first.py
--- a/Listen_Your_Emotion/app_first.py	(revision 1f7cf861b51d44cc954127487319f82cd1994473)
+++ b/Listen_Your_Emotion/app_first.py	(date 1717058415221)
@@ -219,13 +219,12 @@
     result_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])
     device = get_default_device()
     device = 'cpu'
-    #model = ResNet(1, len(classes))
-    #model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))
+    # model = ResNet(1, len(classes))
+    # model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))
     model = torch.load('./models/quant_model_scripted.pt', map_location='cpu')
     model = to_device(model, device)
 
 
-
 def emotion_convert(emotion):
     if emotion.lower() in ['angry', 'disgust', 'fear', 'sad']:
         return 'sad'
@@ -383,26 +382,25 @@
         make_photo()
 
         def local_pred_step(batch):
-
             images = batch
             out = model(images)
 
             return out
 
-        def local_predict( pred_loader):
+        def local_predict(pred_loader):
             outputs = [local_pred_step(batch) for batch in pred_loader]
             return [torch.max(el, dim=1)[1] for el in outputs]
+
         data = [result_tfms(PIL.Image.open('./photos/' + os.listdir('./photos/')[-1]).resize((48, 48)))]
-        data_dl = DataLoader(data, 200,  pin_memory=True)
+        data_dl = DataLoader(data, 200, pin_memory=True)
         data_dl = DeviceDataLoader(data_dl, device)
         start_time = time.time()
 
         pred = local_predict(data_dl)[0][0]
-        #pred = predict(model, data_dl)[0][0]
+        # pred = predict(model, data_dl)[0][0]
         print("--- %s seconds to predict ---\n" % (time.time() - start_time))
         print(pred)
 
-
         # save the model and check the model size
         def print_size_of_model(model, label=""):
             torch.save(model.state_dict(), "temp.p")
@@ -410,7 +408,6 @@
             print("model: ", label, ' \t', 'Size (KB):', size / 1e3)
             os.remove('temp.p')
             return size
-        print_size_of_model(model)
 
         return classes[pred]
 
@@ -427,10 +424,9 @@
     def nextTrack(self):
         global CURRENT_EMOTION
 
-        CURRENT_EMOTION = emotion_convert(self.final_predict()).capitalize()  # Перенести в конец, брать из текущей эмоции для ускорения
-
+        CURRENT_EMOTION = emotion_convert(
+            self.final_predict()).capitalize()  # Перенести в конец, брать из текущей эмоции для ускорения
         update_queue(CURRENT_EMOTION)
-
         self.open_file()
 
     # central location
Index: Listen_Your_Emotion/before_starting.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import sys\r\nfrom PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QLabel, QLineEdit, QPushButton, QMessageBox)\r\nfrom PyQt5.QtGui import QPalette, QLinearGradient, QColor, QBrush\r\nfrom PyQt5.QtCore import Qt, QThread, pyqtSignal\r\nimport warnings\r\n\r\n\r\nwarnings.filterwarnings('ignore')\r\n\r\n\r\nclass Worker(QThread):\r\n    finished = pyqtSignal()\r\n    error = pyqtSignal(str)\r\n\r\n    def __init__(self, token, directory):\r\n        super().__init__()\r\n        self.token = token\r\n        self.directory = directory\r\n\r\n    def run(self):\r\n        try:\r\n            create_env_file(self.token, self.directory)\r\n            self.finished.emit()\r\n        except Exception as e:\r\n            self.error.emit(str(e))\r\n\r\ndef create_env_file(token, directory):\r\n    if not token or not directory:\r\n        raise ValueError(\"Все поля должны быть заполнены!\")\r\n\r\n    # Приведение разделителей к одному типу\r\n    directory = directory.replace('/', '\\\\\\\\')\r\n    directory = directory.replace('\\\\', '\\\\\\\\')\r\n\r\n    # Создание .env файла\r\n    with open('.env', 'w') as file:\r\n        file.write(f'TOKEN=\"{token}\"\\\\n')\r\n        file.write(f'DIRECTORY=\"{directory}\"\\\\n')\r\n\r\n    # Вызов функции a() из файла update_music\r\n    try:\r\n        from queue_maker import classify_favoutite_music\r\n        classify_favoutite_music()\r\n    except Exception as e:\r\n        raise RuntimeError(f\"Произошла ошибка при выполнении функции a: {e}\")\r\n\r\nclass App(QWidget):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.initUI()\r\n\r\n    def initUI(self):\r\n        self.setWindowTitle(\"Введите данные и подождите\")\r\n\r\n        # Установка градиентного фона (белый к серому)\r\n        palette = QPalette()\r\n        gradient = QLinearGradient(0, 0, 0, 1)\r\n        gradient.setCoordinateMode(QLinearGradient.ObjectBoundingMode)\r\n        gradient.setColorAt(0.0, QColor(255, 255, 255))\r\n        gradient.setColorAt(1.0, QColor(128, 128, 128))\r\n        palette.setBrush(QPalette.Window, QBrush(gradient))\r\n        self.setPalette(palette)\r\n\r\n        self.token_label = QLabel(\"Введите токен Яндекс музыки\")\r\n        self.token_entry = QLineEdit()\r\n\r\n        self.directory_label = QLabel(\"Введите абсолютный путь к директории, где временно будет храниться музыка\")\r\n        self.directory_entry = QLineEdit()\r\n\r\n        self.submit_button = QPushButton(\"Готово\")\r\n        self.submit_button.clicked.connect(self.on_submit)\r\n\r\n        self.layout = QVBoxLayout()\r\n        self.layout.addWidget(self.token_label)\r\n        self.layout.addWidget(self.token_entry)\r\n        self.layout.addWidget(self.directory_label)\r\n        self.layout.addWidget(self.directory_entry)\r\n        self.layout.addWidget(self.submit_button)\r\n\r\n        self.setLayout(self.layout)\r\n\r\n    def on_submit(self):\r\n        token = self.token_entry.text()\r\n        directory = self.directory_entry.text()\r\n\r\n        self.token_label.hide()\r\n        self.token_entry.hide()\r\n        self.directory_label.hide()\r\n        self.directory_entry.hide()\r\n        self.submit_button.hide()\r\n\r\n        self.processing_label = QLabel(\"Ваша избранная музыка обрабатывается. Пожалуйста, подождите.\")\r\n        self.layout.addWidget(self.processing_label)\r\n\r\n        self.worker = Worker(token, directory)\r\n        self.worker.finished.connect(self.on_finished)\r\n        self.worker.error.connect(self.on_error)\r\n        self.worker.start()\r\n\r\n    def on_finished(self):\r\n        self.layout.removeWidget(self.processing_label)\r\n        self.processing_label.deleteLater()\r\n\r\n        self.finished_label = QLabel(\"Всё готово, приятного прослушивания :)\")\r\n        self.layout.addWidget(self.finished_label)\r\n\r\n    def on_error(self, error_message):\r\n        QMessageBox.critical(None, \"Ошибка\", error_message)\r\n\r\nif __name__ == '__main__':\r\n    app = QApplication(sys.argv)\r\n    ex = App()\r\n    ex.show()\r\n    sys.exit(app.exec_())
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Listen_Your_Emotion/before_starting.py b/Listen_Your_Emotion/before_starting.py
--- a/Listen_Your_Emotion/before_starting.py	(revision 1f7cf861b51d44cc954127487319f82cd1994473)
+++ b/Listen_Your_Emotion/before_starting.py	(date 1717058509643)
@@ -39,7 +39,7 @@
 
     # Вызов функции a() из файла update_music
     try:
-        from queue_maker import classify_favoutite_music
+        from music_storage.queue_maker import classify_favoutite_music
         classify_favoutite_music()
     except Exception as e:
         raise RuntimeError(f"Произошла ошибка при выполнении функции a: {e}")
Index: Listen_Your_Emotion/pyt.py
===================================================================
diff --git a/Listen_Your_Emotion/pyt.py b/Listen_Your_Emotion/pyt.py
deleted file mode 100644
--- a/Listen_Your_Emotion/pyt.py	(revision 1f7cf861b51d44cc954127487319f82cd1994473)
+++ /dev/null	(revision 1f7cf861b51d44cc954127487319f82cd1994473)
@@ -1,3 +0,0 @@
-class a:
-    def __init__(self):
-        self.qwe =1
\ No newline at end of file
