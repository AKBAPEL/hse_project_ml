{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Train Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Validation Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "print(os.listdir(data_dir))\n",
    "classes_train = os.listdir(data_dir + \"/train\")\n",
    "classes_valid = os.listdir(data_dir + \"/validation\")\n",
    "print(f'Train Classes - {classes_train}')\n",
    "print(f'Validation Classes - {classes_valid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tt.Compose([tt.Grayscale(num_output_channels=1),\n",
    "                         tt.RandomHorizontalFlip(),\n",
    "                         tt.RandomRotation(30),\n",
    "                         tt.ToTensor()])\n",
    "\n",
    "valid_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "best_model = 0\n",
    "result_dir = './photos'\n",
    "print(os.listdir(result_dir))\n",
    "result_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds = [result_tfms(PIL.Image.open('./photos/'+path).resize((48, 48)))for path in os.listdir(result_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DataLoader(result_ds, batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(data_dir + '/train', train_tfms)\n",
    "valid_ds = ImageFolder(data_dir + '/validation', valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_res_batch(dl):\n",
    "    for images in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:3], nrow=3).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device\n",
    "print('GPU: ' + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DeviceDataLoader(result_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаги обучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    def pred_step(self, batch):\n",
    "        images = batch \n",
    "        out = self(images)\n",
    "        return out\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        global best_model, new_model\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        new_model = result['val_acc']\n",
    "        if new_model > best_model:\n",
    "            best_model = new_model\n",
    "            torch.save(model.state_dict(), './models/emotion_detection_acc'+str(best_model)+'.pth')\n",
    "            print('save ', './models/emotion_detection_acc'+str(best_model)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ELU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 128)\n",
    "        self.conv2 = conv_block(128, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256)\n",
    "        self.conv4 = conv_block(256, 256, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv5 = conv_block(256, 512)\n",
    "        self.conv6 = conv_block(512, 512, pool=True)\n",
    "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(6), \n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = to_device(xb,device)\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.drop2(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.drop3(out)\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = to_device(ResNet(1, len(classes_train)), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(1, len(classes_train))\n",
    "model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "first_parameter = next(model.parameters())\n",
    "input_shape = first_parameter.size()\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 41.149MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu used 43154432 memory\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpu used {torch.cuda.max_memory_allocated(device=None)} memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def predict(model, pred_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.pred_step(batch) for batch in pred_loader]\n",
    "    return [torch.max(el, dim=1)[1] for el in outputs]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        \n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, max_lr, model, train_loader, val_loader, device,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        print(result)\n",
    "        #model.epoch_end(epoch, result)\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 130\n",
    "max_lr = 0.0008\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Графики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pruning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "def compute_final_pruning_rate(pruning_rate, num_iterations):\n",
    "    \"\"\"A function to compute the final pruning rate for iterative pruning.\n",
    "        Note that this cannot be applied for global pruning rate if the pruning rate is heterogeneous among different layers.\n",
    "\n",
    "    Args:\n",
    "        pruning_rate (float): Pruning rate.\n",
    "        num_iterations (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        float: Final pruning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    final_pruning_rate = 1 - (1 - pruning_rate)**num_iterations\n",
    "\n",
    "    return final_pruning_rate\n",
    "\n",
    "\n",
    "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    if use_mask == True:\n",
    "        for buffer_name, buffer in module.named_buffers():\n",
    "            if \"weight_mask\" in buffer_name and weight == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "            if \"bias_mask\" in buffer_name and bias == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "    else:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name and weight == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "            if \"bias\" in param_name and bias == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n",
    "\n",
    "\n",
    "def measure_global_sparsity(model,\n",
    "                            weight=True,\n",
    "                            bias=False,\n",
    "                            conv2d_use_mask=False,\n",
    "                            linear_use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n",
    "\n",
    "\n",
    "def iterative_pruning_finetuning(model,\n",
    "                                 train_loader,\n",
    "                                 test_loader,\n",
    "                                 device,\n",
    "                                 learning_rate,\n",
    "                                 l1_regularization_strength,\n",
    "                                 l2_regularization_strength,\n",
    "                                 learning_rate_decay=0.1,\n",
    "                                 conv2d_prune_amount=0.4,\n",
    "                                 linear_prune_amount=0.2,\n",
    "                                 num_iterations=10,\n",
    "                                 num_epochs_per_iteration=10,\n",
    "                                 model_filename_prefix=\"pruned_model\",\n",
    "                                 model_dir=\"saved_models\",\n",
    "                                 grouped_pruning=False):\n",
    "    print(\"Accuracy before pruning:\",evaluate(model, valid_dl))\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size before pruning: {:.3f}MB'.format(size_all_mb))\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        print(\"Pruning and Finetuning {}/{}\".format(i + 1, num_iterations))\n",
    "\n",
    "        print(\"Pruning...\")\n",
    "\n",
    "        if grouped_pruning == True:\n",
    "            # Global pruning\n",
    "            # I would rather call it grouped pruning.\n",
    "            parameters_to_prune = []\n",
    "            for module_name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    parameters_to_prune.append((module, \"weight\"))\n",
    "            prune.global_unstructured(\n",
    "                parameters_to_prune,\n",
    "                pruning_method=prune.L1Unstructured,\n",
    "                amount=conv2d_prune_amount,\n",
    "            )\n",
    "        else:\n",
    "            for module_name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    prune.l1_unstructured(module,\n",
    "                                          name=\"weight\",\n",
    "                                          amount=conv2d_prune_amount)\n",
    "                elif isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module,\n",
    "                                          name=\"weight\",\n",
    "                                          amount=linear_prune_amount)\n",
    "        \n",
    "        eval_accuracy = evaluate(model, test_loader)\n",
    "\n",
    "\n",
    "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "            model,\n",
    "            weight=True,\n",
    "            bias=False,\n",
    "            conv2d_use_mask=True,\n",
    "            linear_use_mask=False)\n",
    "\n",
    "        #print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "        print(eval_accuracy)\n",
    "\n",
    "        print(\"Global Sparsity:\")\n",
    "        print(\"{:.2f}\".format(sparsity))\n",
    "\n",
    "        # print(model.conv1._forward_pre_hooks)\n",
    "\n",
    "        print(\"Fine-tuning...\")\n",
    "        train_model(num_epochs_per_iteration, max_lr, model, train_loader, test_loader, device=device,\n",
    "                        grad_clip=grad_clip, weight_decay=l2_regularization_strength, opt_func=opt_func)\n",
    "        \"\"\"train_model(model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    device=device,\n",
    "                    l1_regularization_strength=l1_regularization_strength,\n",
    "                    l2_regularization_strength=l2_regularization_strength,\n",
    "                    learning_rate=learning_rate * (learning_rate_decay**i),\n",
    "                    num_epochs=num_epochs_per_iteration)\"\"\"\n",
    "        \n",
    "        eval_accuracy = evaluate(model, test_loader)\n",
    "        \n",
    "       \n",
    "\n",
    "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "            model,\n",
    "            weight=True,\n",
    "            bias=False,\n",
    "            conv2d_use_mask=True,\n",
    "            linear_use_mask=False)\n",
    "\n",
    "        #print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "        print(eval_accuracy)\n",
    "        print(\"Global Sparsity:\")\n",
    "        print(\"{:.2f}\".format(sparsity))\n",
    "\n",
    "        model_filename = \"{}_{}.pt\".format(model_filename_prefix, i + 1)\n",
    "        model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "        \n",
    "        save_model(model=model,\n",
    "                   model_dir=model_dir,\n",
    "                   model_filename=model_filename)\n",
    "        model = load_model(model=model,\n",
    "                           model_filepath=model_filepath,\n",
    "                           device=device)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def remove_parameters(model):\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            try:\n",
    "                prune.remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                prune.remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            try:\n",
    "                prune.remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                prune.remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "def load_model(model, model_filepath, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 1.2196050882339478, 'val_acc': 0.5452366471290588}\n",
      "Global Sparsity:\n",
      "0.00\n"
     ]
    }
   ],
   "source": [
    "num_classes = 7\n",
    "random_seed = 1\n",
    "l1_regularization_strength = 0\n",
    "l2_regularization_strength = 1e-4\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 1\n",
    "\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "model_filename = \"emotion_detection_acc0.5452366471290588.pth\"\n",
    "model_filename_prefix = \"pruned_model\"\n",
    "pruned_model_filename = \"1.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "pruned_model_filepath = os.path.join(model_dir, pruned_model_filename)\n",
    "\n",
    "\n",
    "# Create an untrained model.\n",
    "\n",
    "\n",
    "# Load a pretrained model.\n",
    "model = ResNet(1, len(classes_train))\n",
    "model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\n",
    "model = to_device(model,device)\n",
    "\n",
    "\n",
    "eval_accuracy = evaluate(model, valid_dl)\n",
    "\n",
    "\n",
    "num_zeros, num_elements, sparsity = measure_global_sparsity(model)\n",
    "print(eval_accuracy)\n",
    "#print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "\n",
    "print(\"Global Sparsity:\")\n",
    "print(\"{:.2f}\".format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative Pruning + Fine-Tuning...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Iterative Pruning + Fine-Tuning...\")\n",
    "\n",
    "pruned_model = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before pruning: {'val_loss': 1.2196050882339478, 'val_acc': 0.5452366471290588}\n",
      "model size before pruning: 41.149MB\n",
      "Pruning and Finetuning 1/1\n",
      "Pruning...\n",
      "{'val_loss': 2.000497579574585, 'val_acc': 0.17319445312023163}\n",
      "Global Sparsity:\n",
      "0.90\n",
      "Fine-tuning...\n",
      "{'val_loss': 1.2871286869049072, 'val_acc': 0.5228894948959351}\n",
      "{'val_loss': 1.247686743736267, 'val_acc': 0.5323184728622437}\n",
      "{'val_loss': 1.1793111562728882, 'val_acc': 0.553891122341156}\n",
      "{'val_loss': 1.1421949863433838, 'val_acc': 0.5806466937065125}\n",
      "{'val_loss': 1.1213805675506592, 'val_acc': 0.5957194566726685}\n",
      "{'val_loss': 1.1213805675506592, 'val_acc': 0.5957194566726685}\n",
      "Global Sparsity:\n",
      "0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (drop3): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_pruning_finetuning(\n",
    "    model=pruned_model,\n",
    "    train_loader=train_dl,\n",
    "    test_loader=valid_dl,\n",
    "    device=cuda_device,\n",
    "    learning_rate=learning_rate,\n",
    "    learning_rate_decay=learning_rate_decay,\n",
    "    l1_regularization_strength=l1_regularization_strength,\n",
    "    l2_regularization_strength=l2_regularization_strength,\n",
    "    conv2d_prune_amount=0.9,\n",
    "    linear_prune_amount=0.05,\n",
    "    num_iterations=1,\n",
    "    num_epochs_per_iteration=5, #Was 200\n",
    "    model_filename_prefix=model_filename_prefix,\n",
    "    model_dir=model_dir,\n",
    "    grouped_pruning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (drop3): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# iterative_pruning_finetuning(\n",
    "#     model=pruned_model,\n",
    "#     train_loader=train_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     device=cuda_device,\n",
    "#     learning_rate=learning_rate,\n",
    "#     learning_rate_decay=learning_rate_decay,\n",
    "#     l1_regularization_strength=l1_regularization_strength,\n",
    "#     l2_regularization_strength=l2_regularization_strength,\n",
    "#     conv2d_prune_amount=0.3,\n",
    "#     linear_prune_amount=0,\n",
    "#     num_iterations=8,\n",
    "#     num_epochs_per_iteration=50,\n",
    "#     model_filename_prefix=model_filename_prefix,\n",
    "#     model_dir=model_dir,\n",
    "#     grouped_pruning=True)\n",
    "\n",
    "\n",
    "# Apply mask to the parameters and remove the mask.\n",
    "remove_parameters(model=pruned_model)\n",
    "\n",
    "\n",
    "\n",
    "#save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 1.1213805675506592, 'val_acc': 0.5957194566726685}\n",
      "Global Sparsity:\n",
      "0.90\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy = evaluate(pruned_model, valid_dl)\n",
    "\n",
    "\n",
    "\n",
    "num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model)\n",
    "print(eval_accuracy)\n",
    "\n",
    "print(\"Global Sparsity:\")\n",
    "print(\"{:.2f}\".format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size after pruning: 41.149MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size after pruning: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
