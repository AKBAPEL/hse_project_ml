{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Train Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Validation Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "print(os.listdir(data_dir))\n",
    "classes_train = os.listdir(data_dir + \"/train\")\n",
    "classes_valid = os.listdir(data_dir + \"/validation\")\n",
    "print(f'Train Classes - {classes_train}')\n",
    "print(f'Validation Classes - {classes_valid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tt.Compose([tt.Grayscale(num_output_channels=1),\n",
    "                         tt.RandomHorizontalFlip(),\n",
    "                         tt.RandomRotation(30),\n",
    "                         tt.ToTensor()])\n",
    "\n",
    "valid_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "best_model = 0\n",
    "result_dir = './photos'\n",
    "print(os.listdir(result_dir))\n",
    "result_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds = [result_tfms(PIL.Image.open('./photos/'+path).resize((48, 48)))for path in os.listdir(result_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DataLoader(result_ds, batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(data_dir + '/train', train_tfms)\n",
    "valid_ds = ImageFolder(data_dir + '/validation', valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_res_batch(dl):\n",
    "    for images in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:3], nrow=3).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device\n",
    "print('GPU: ' + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DeviceDataLoader(result_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаги обучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    def pred_step(self, batch):\n",
    "        images = batch \n",
    "        out = self(images)\n",
    "        return out\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        global best_model, new_model\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        new_model = result['val_acc']\n",
    "        if new_model > best_model:\n",
    "            best_model = new_model\n",
    "            torch.save(model.state_dict(), './models/emotion_detection_acc'+str(best_model)+'.pth')\n",
    "            print('save ', './models/emotion_detection_acc'+str(best_model)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ELU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 128)\n",
    "        self.conv2 = conv_block(128, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256)\n",
    "        self.conv4 = conv_block(256, 256, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv5 = conv_block(256, 512)\n",
    "        self.conv6 = conv_block(512, 512, pool=True)\n",
    "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(6), \n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = to_device(xb,device)\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.drop2(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.drop3(out)\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = to_device(ResNet(1, len(classes_train)), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(1, len(classes_train))\n",
    "model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "first_parameter = next(model.parameters())\n",
    "input_shape = first_parameter.size()\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 41.149MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu used 43154432 memory\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpu used {torch.cuda.max_memory_allocated(device=None)} memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def predict(model, pred_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.pred_step(batch) for batch in pred_loader]\n",
    "    return [torch.max(el, dim=1)[1] for el in outputs]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        \n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, max_lr, model, train_loader, val_loader, device,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        print(result)\n",
    "        #model.epoch_end(epoch, result)\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 130\n",
    "max_lr = 0.0008\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Графики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 1.2196050882339478, 'val_acc': 0.5452366471290588}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model,valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for el in train_dl:\n",
    "    test_batch = el[0]\n",
    "    test_outs  = el[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, 1, 48, 48, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"onnx_model.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"onnx_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.helper.printable_graph(onnx_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession('onnx_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.7093799114227295 seconds with onnx ---\n",
      "--- 3.0181052684783936 seconds without onnx ---\n",
      "\n",
      "--- 2.823071002960205 seconds with onnx ---\n",
      "--- 2.5366690158843994 seconds without onnx ---\n",
      "\n",
      "--- 2.701378345489502 seconds with onnx ---\n",
      "--- 2.6005821228027344 seconds without onnx ---\n",
      "\n",
      "--- 2.675286293029785 seconds with onnx ---\n",
      "--- 2.6121768951416016 seconds without onnx ---\n",
      "\n",
      "--- 2.7328460216522217 seconds with onnx ---\n",
      "--- 2.639616012573242 seconds without onnx ---\n",
      "\n",
      "--- 2.7744600772857666 seconds with onnx ---\n",
      "--- 2.576529026031494 seconds without onnx ---\n",
      "\n",
      "--- 2.769447088241577 seconds with onnx ---\n",
      "--- 2.616605043411255 seconds without onnx ---\n",
      "\n",
      "--- 2.778463363647461 seconds with onnx ---\n",
      "--- 2.5745084285736084 seconds without onnx ---\n",
      "\n",
      "--- 2.664850950241089 seconds with onnx ---\n",
      "--- 2.5759119987487793 seconds without onnx ---\n",
      "\n",
      "--- 2.843546152114868 seconds with onnx ---\n",
      "--- 2.634521961212158 seconds without onnx ---\n",
      "\n",
      "--- 2.732471227645874 seconds with onnx ---\n",
      "--- 2.6131749153137207 seconds without onnx ---\n",
      "\n",
      "--- 2.6944644451141357 seconds with onnx ---\n",
      "--- 2.569204807281494 seconds without onnx ---\n",
      "\n",
      "--- 2.7368974685668945 seconds with onnx ---\n",
      "--- 2.5977320671081543 seconds without onnx ---\n",
      "\n",
      "--- 2.7126357555389404 seconds with onnx ---\n",
      "--- 2.519563674926758 seconds without onnx ---\n",
      "\n",
      "--- 2.746980905532837 seconds with onnx ---\n",
      "--- 2.5713212490081787 seconds without onnx ---\n",
      "\n",
      "--- 2.68554949760437 seconds with onnx ---\n",
      "--- 2.5677237510681152 seconds without onnx ---\n",
      "\n",
      "--- 2.8343088626861572 seconds with onnx ---\n",
      "--- 2.558265447616577 seconds without onnx ---\n",
      "\n",
      "--- 2.5649020671844482 seconds with onnx ---\n",
      "--- 2.630629301071167 seconds without onnx ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_acc = []\n",
    "for el in valid_dl:\n",
    "    test_batch = el[0]\n",
    "    test_outs  = el[1]\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(test_batch)}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    preds = [np.argmax(el) for el in ort_outs[0]]\n",
    "    print(\"--- %s seconds with onnx ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    no_onnx_dl = DataLoader(test_batch, batch_size, num_workers=3, pin_memory=False)\n",
    "    no_onnx_dl = DeviceDataLoader(no_onnx_dl, device)\n",
    "    no_onnx_outs  = predict(model,no_onnx_dl)\n",
    "    print(\"--- %s seconds without onnx ---\\n\" % (time.time() - start_time))\n",
    "    \n",
    "    res_acc.append(accuracy(to_device(torch.tensor(ort_outs[0]),device),to_device(test_outs,device)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with onnx:  tensor(0.5452)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy with onnx: \",sum(res_acc) / len(res_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
