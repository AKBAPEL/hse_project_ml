{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Train Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Validation Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "print(os.listdir(data_dir))\n",
    "classes_train = os.listdir(data_dir + \"/train\")\n",
    "classes_valid = os.listdir(data_dir + \"/validation\")\n",
    "print(f'Train Classes - {classes_train}')\n",
    "print(f'Validation Classes - {classes_valid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tt.Compose([tt.Grayscale(num_output_channels=1),\n",
    "                         tt.RandomHorizontalFlip(),\n",
    "                         tt.RandomRotation(30),\n",
    "                         tt.ToTensor()])\n",
    "\n",
    "valid_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "best_model = 0\n",
    "result_dir = './photos'\n",
    "print(os.listdir(result_dir))\n",
    "result_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds = [result_tfms(PIL.Image.open('./photos/'+path).resize((48, 48)))for path in os.listdir(result_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DataLoader(result_ds, batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(data_dir + '/train', train_tfms)\n",
    "valid_ds = ImageFolder(data_dir + '/validation', valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_res_batch(dl):\n",
    "    for images in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:3], nrow=3).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device\n",
    "print('GPU: ' + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DeviceDataLoader(result_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаги обучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    def pred_step(self, batch):\n",
    "        images = batch \n",
    "        out = self(images)\n",
    "        return out\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        global best_model, new_model\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        new_model = result['val_acc']\n",
    "        if new_model > best_model:\n",
    "            best_model = new_model\n",
    "            torch.save(model.state_dict(), './models/emotion_detection_acc'+str(best_model)+'.pth')\n",
    "            print('save ', './models/emotion_detection_acc'+str(best_model)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ELU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 128)\n",
    "        self.conv2 = conv_block(128, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256)\n",
    "        self.conv4 = conv_block(256, 256, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv5 = conv_block(256, 512)\n",
    "        self.conv6 = conv_block(512, 512, pool=True)\n",
    "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(6), \n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = to_device(xb,device)\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.drop2(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.drop3(out)\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = to_device(ResNet(1, len(classes_train)), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(1, len(classes_train))\n",
    "model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "first_parameter = next(model.parameters())\n",
    "input_shape = first_parameter.size()\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 41.149MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu used 43154432 memory\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpu used {torch.cuda.max_memory_allocated(device=None)} memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def predict(model, pred_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.pred_step(batch) for batch in pred_loader]\n",
    "    return [torch.max(el, dim=1)[1] for el in outputs]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        \n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, max_lr, model, train_loader, val_loader, device,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        print(result)\n",
    "        #model.epoch_end(epoch, result)\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 130\n",
    "max_lr = 0.0008\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig_mapping = get_default_qat_qconfig_mapping(\"fbgemm\")\n",
    "example_inputs = torch.randn(batch_size, 1, 48, 48, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(1, len(classes_train))\n",
    "model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prepared_model_static = prepare_qat_fx(model, qconfig_mapping, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, target) in enumerate(valid_dl):\n",
    "    prepared_model_static(data)\n",
    "    if batch_idx % 10 == 0:\n",
    "        break\n",
    "        print(\"Batch %d/%d complete, continue ...\" %(batch_idx+1, len(valid_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = convert_fx(prepared_model_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "GraphModule(\n",
      "  (activation_post_process_0): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0079]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0)\n",
      "  )\n",
      "  (conv1): Module(\n",
      "    (0): ConvBn2d(\n",
      "      1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0247, 0.0124, 0.0267, 0.0196, 0.0247, 0.0130, 0.0383, 0.0341, 0.0120,\n",
      "                0.0229, 0.0096, 0.0105, 0.0399, 0.0363, 0.0112, 0.0251, 0.0216, 0.0076,\n",
      "                0.0137, 0.0134, 0.0333, 0.0097, 0.0282, 0.0083, 0.0162, 0.0082, 0.0095,\n",
      "                0.0098, 0.0167, 0.0446, 0.0133, 0.0127, 0.0390, 0.0196, 0.0329, 0.0186,\n",
      "                0.0142, 0.0156, 0.0122, 0.0208, 0.0323, 0.0215, 0.0096, 0.0060, 0.0239,\n",
      "                0.0251, 0.0250, 0.0269, 0.0184, 0.0286, 0.0324, 0.0283, 0.0088, 0.0116,\n",
      "                0.0346, 0.0167, 0.0187, 0.0433, 0.0178, 0.0143, 0.0280, 0.0299, 0.0119,\n",
      "                0.0365, 0.0089, 0.0219, 0.0402, 0.0119, 0.0104, 0.0170, 0.0073, 0.0305,\n",
      "                0.0310, 0.0372, 0.0256, 0.0091, 0.0136, 0.0123, 0.0235, 0.0408, 0.0124,\n",
      "                0.0259, 0.0149, 0.0128, 0.0517, 0.0138, 0.0200, 0.0179, 0.0401, 0.0369,\n",
      "                0.0364, 0.0210, 0.0130, 0.0089, 0.0090, 0.0494, 0.0302, 0.0229, 0.0133,\n",
      "                0.0057, 0.0144, 0.0301, 0.0352, 0.0121, 0.0094, 0.0286, 0.0243, 0.0434,\n",
      "                0.0492, 0.0270, 0.0425, 0.0123, 0.0347, 0.0123, 0.0254, 0.0082, 0.0313,\n",
      "                0.0260, 0.0396, 0.0161, 0.0201, 0.0384, 0.0202, 0.0321, 0.0418, 0.0411,\n",
      "                0.0464, 0.0264]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "          min_val=tensor([-3.1566, -1.5846, -2.9813, -2.5102, -3.1567, -1.3603, -4.9018, -4.3678,\n",
      "                  -1.5387, -2.9328, -1.2228, -1.3381, -5.1031, -4.6465, -1.4352, -2.9687,\n",
      "                  -2.7687, -0.9394, -1.4731, -1.7107, -4.2578, -1.2133, -3.6111, -0.1542,\n",
      "                  -2.0726, -1.0495, -1.2141, -1.2524, -2.1423, -5.7095, -1.7048, -1.3266,\n",
      "                  -4.9971, -1.3620, -4.2147, -2.3847, -1.5280, -1.9922, -0.9495, -2.6614,\n",
      "                  -4.1391, -1.6938, -0.6009, -0.1379, -1.9837, -3.2158, -3.2002, -3.3256,\n",
      "                  -2.3591, -3.6662, -3.3825, -3.4468, -1.0847, -1.4902, -4.4295, -2.0942,\n",
      "                  -2.3961, -5.5466, -2.1487, -1.1892, -3.4822, -3.8232, -1.3432, -4.3831,\n",
      "                  -0.5850, -2.7836, -5.1508, -1.5244, -0.5759, -2.1795, -0.9404, -3.8172,\n",
      "                  -3.9657, -4.7615, -3.1263, -1.1456, -1.7398, -0.9033, -2.8708, -4.1838,\n",
      "                  -1.4781, -3.3150, -1.4546, -1.5892, -6.2229, -1.7605, -2.3494, -2.1577,\n",
      "                  -4.3678, -3.9749, -2.4588, -1.8376, -1.6610, -1.1442, -1.1473, -5.6192,\n",
      "                  -3.8713, -2.5974, -1.7047, -0.2536, -1.8373, -3.4888, -4.5052, -1.5490,\n",
      "                  -1.2028, -3.6600, -3.1135, -5.2501, -4.0143, -3.0977, -5.0695, -1.1028,\n",
      "                  -4.4461, -1.5744, -3.2570, -1.0365, -4.0015, -3.3325, -5.0723, -2.0560,\n",
      "                  -1.6036, -4.9200, -2.2794, -2.9505, -3.2032, -4.6152, -5.9373, -3.3778]), max_val=tensor([2.7308, 1.0665, 3.3949, 2.2782, 3.0916, 1.6557, 4.8209, 3.5567, 0.4770,\n",
      "                  2.8417, 0.6049, 1.2526, 4.6606, 2.5819, 1.3398, 3.1866, 2.7200, 0.9594,\n",
      "                  1.7410, 1.5456, 3.7661, 1.2329, 2.1311, 1.0505, 1.6169, 0.4610, 0.8592,\n",
      "                  0.7220, 1.8750, 4.6589, 1.1759, 1.6078, 4.3057, 2.4873, 1.6428, 2.0005,\n",
      "                  1.8069, 0.5096, 1.5477, 2.2264, 4.1047, 2.7357, 1.2216, 0.7629, 3.0404,\n",
      "                  2.5222, 2.2602, 3.4137, 1.2715, 3.4773, 4.1101, 3.5975, 1.1207, 0.8565,\n",
      "                  4.0681, 2.1239, 1.9422, 5.4529, 2.2561, 1.8142, 3.5606, 3.6408, 1.5122,\n",
      "                  4.6390, 1.1306, 2.7783, 4.0418, 0.7856, 1.3148, 1.6972, 0.4448, 3.8738,\n",
      "                  3.2489, 4.5032, 3.2460, 1.1620, 1.0611, 1.5596, 2.9838, 5.1849, 1.5692,\n",
      "                  2.1347, 1.8930, 1.6289, 6.5645, 1.3928, 2.5426, 2.2685, 5.0945, 4.6825,\n",
      "                  4.6266, 2.6668, 1.2767, 0.9585, 1.0055, 6.2778, 3.7164, 2.9081, 0.7909,\n",
      "                  0.7185, 1.6436, 3.8210, 3.7264, 0.8056, 0.6906, 3.1582, 2.7550, 5.5126,\n",
      "                  6.2479, 3.4338, 5.3941, 1.5597, 4.3223, 0.7008, 3.0573, 1.0467, 3.7879,\n",
      "                  3.1262, 4.2070, 1.6521, 2.5466, 4.6953, 2.5637, 4.0824, 5.3105, 5.2259,\n",
      "                  5.8688, 3.1269])\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "  )\n",
      "  (activation_post_process_1): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1788]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.914599418640137, max_val=11.79886245727539)\n",
      "  )\n",
      "  (activation_post_process_2): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1008]), zero_point=tensor([10], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0087257623672485, max_val=11.790238380432129)\n",
      "  )\n",
      "  (conv2): Module(\n",
      "    (0): ConvBn2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0005, 0.0003, 0.0004, 0.0002, 0.0006, 0.0005, 0.0004, 0.0004, 0.0003,\n",
      "                0.0004, 0.0003, 0.0004, 0.0005, 0.0006, 0.0003, 0.0003, 0.0004, 0.0005,\n",
      "                0.0006, 0.0003, 0.0004, 0.0006, 0.0004, 0.0003, 0.0007, 0.0003, 0.0002,\n",
      "                0.0002, 0.0006, 0.0004, 0.0005, 0.0005, 0.0005, 0.0002, 0.0003, 0.0004,\n",
      "                0.0005, 0.0005, 0.0004, 0.0005, 0.0003, 0.0004, 0.0003, 0.0005, 0.0004,\n",
      "                0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005, 0.0003, 0.0002, 0.0006,\n",
      "                0.0004, 0.0004, 0.0002, 0.0004, 0.0003, 0.0004, 0.0004, 0.0005, 0.0004,\n",
      "                0.0003, 0.0005, 0.0005, 0.0003, 0.0006, 0.0006, 0.0006, 0.0003, 0.0004,\n",
      "                0.0005, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0002, 0.0008, 0.0004,\n",
      "                0.0004, 0.0003, 0.0004, 0.0003, 0.0004, 0.0004, 0.0003, 0.0004, 0.0004,\n",
      "                0.0004, 0.0003, 0.0003, 0.0006, 0.0005, 0.0002, 0.0004, 0.0006, 0.0004,\n",
      "                0.0004, 0.0002, 0.0006, 0.0003, 0.0005, 0.0004, 0.0005, 0.0004, 0.0006,\n",
      "                0.0003, 0.0004, 0.0006, 0.0003, 0.0005, 0.0005, 0.0005, 0.0003, 0.0005,\n",
      "                0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005, 0.0006, 0.0003, 0.0005,\n",
      "                0.0004, 0.0005]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "          min_val=tensor([-0.0617, -0.0442, -0.0437, -0.0236, -0.0781, -0.0580, -0.0440, -0.0500,\n",
      "                  -0.0316, -0.0475, -0.0382, -0.0564, -0.0616, -0.0749, -0.0336, -0.0406,\n",
      "                  -0.0549, -0.0659, -0.0707, -0.0429, -0.0519, -0.0727, -0.0524, -0.0411,\n",
      "                  -0.0834, -0.0319, -0.0262, -0.0272, -0.0646, -0.0548, -0.0628, -0.0599,\n",
      "                  -0.0594, -0.0282, -0.0359, -0.0500, -0.0627, -0.0585, -0.0489, -0.0611,\n",
      "                  -0.0417, -0.0471, -0.0409, -0.0659, -0.0470, -0.0467, -0.0672, -0.0542,\n",
      "                  -0.0464, -0.0533, -0.0633, -0.0433, -0.0247, -0.0750, -0.0518, -0.0549,\n",
      "                  -0.0294, -0.0547, -0.0433, -0.0500, -0.0504, -0.0527, -0.0436, -0.0399,\n",
      "                  -0.0651, -0.0575, -0.0403, -0.0737, -0.0780, -0.0690, -0.0384, -0.0544,\n",
      "                  -0.0638, -0.0444, -0.0555, -0.0579, -0.0521, -0.0451, -0.0210, -0.1049,\n",
      "                  -0.0475, -0.0411, -0.0419, -0.0460, -0.0431, -0.0562, -0.0526, -0.0411,\n",
      "                  -0.0493, -0.0572, -0.0455, -0.0280, -0.0312, -0.0722, -0.0635, -0.0306,\n",
      "                  -0.0480, -0.0756, -0.0431, -0.0513, -0.0313, -0.0709, -0.0397, -0.0634,\n",
      "                  -0.0481, -0.0620, -0.0541, -0.0755, -0.0389, -0.0492, -0.0801, -0.0307,\n",
      "                  -0.0658, -0.0589, -0.0636, -0.0403, -0.0698, -0.0556, -0.0430, -0.0560,\n",
      "                  -0.0571, -0.0437, -0.0598, -0.0718, -0.0333, -0.0602, -0.0433, -0.0551]), max_val=tensor([0.0574, 0.0423, 0.0446, 0.0259, 0.0756, 0.0500, 0.0484, 0.0473, 0.0364,\n",
      "                  0.0423, 0.0386, 0.0493, 0.0601, 0.0605, 0.0365, 0.0402, 0.0556, 0.0667,\n",
      "                  0.0659, 0.0443, 0.0501, 0.0727, 0.0527, 0.0409, 0.0738, 0.0364, 0.0278,\n",
      "                  0.0306, 0.0718, 0.0525, 0.0640, 0.0643, 0.0658, 0.0314, 0.0378, 0.0522,\n",
      "                  0.0611, 0.0536, 0.0461, 0.0536, 0.0436, 0.0486, 0.0401, 0.0610, 0.0439,\n",
      "                  0.0443, 0.0678, 0.0664, 0.0482, 0.0505, 0.0548, 0.0410, 0.0235, 0.0670,\n",
      "                  0.0468, 0.0522, 0.0305, 0.0453, 0.0406, 0.0459, 0.0520, 0.0586, 0.0452,\n",
      "                  0.0358, 0.0675, 0.0601, 0.0355, 0.0654, 0.0774, 0.0737, 0.0444, 0.0470,\n",
      "                  0.0577, 0.0464, 0.0494, 0.0571, 0.0443, 0.0460, 0.0211, 0.0900, 0.0435,\n",
      "                  0.0451, 0.0427, 0.0443, 0.0408, 0.0542, 0.0500, 0.0405, 0.0549, 0.0470,\n",
      "                  0.0503, 0.0323, 0.0367, 0.0655, 0.0586, 0.0279, 0.0528, 0.0705, 0.0463,\n",
      "                  0.0501, 0.0274, 0.0565, 0.0417, 0.0641, 0.0481, 0.0615, 0.0565, 0.0653,\n",
      "                  0.0405, 0.0415, 0.0756, 0.0347, 0.0539, 0.0553, 0.0672, 0.0421, 0.0511,\n",
      "                  0.0416, 0.0447, 0.0568, 0.0601, 0.0465, 0.0570, 0.0640, 0.0361, 0.0587,\n",
      "                  0.0494, 0.0599])\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (activation_post_process_3): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1763]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.408576965332031, max_val=11.982356071472168)\n",
      "  )\n",
      "  (activation_post_process_4): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1021]), zero_point=tensor([10], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0232666730880737, max_val=11.94432258605957)\n",
      "  )\n",
      "  (activation_post_process_5): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1021]), zero_point=tensor([10], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0232666730880737, max_val=11.94432258605957)\n",
      "  )\n",
      "  (res1): Module(\n",
      "    (0): Module(\n",
      "      (0): ConvBn2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0006, 0.0005, 0.0004, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0003,\n",
      "                  0.0004, 0.0006, 0.0004, 0.0005, 0.0005, 0.0004, 0.0006, 0.0004, 0.0004,\n",
      "                  0.0004, 0.0003, 0.0005, 0.0004, 0.0005, 0.0003, 0.0005, 0.0004, 0.0005,\n",
      "                  0.0005, 0.0004, 0.0005, 0.0004, 0.0006, 0.0005, 0.0006, 0.0004, 0.0005,\n",
      "                  0.0004, 0.0004, 0.0003, 0.0006, 0.0005, 0.0005, 0.0006, 0.0004, 0.0004,\n",
      "                  0.0005, 0.0004, 0.0003, 0.0004, 0.0005, 0.0004, 0.0003, 0.0004, 0.0005,\n",
      "                  0.0004, 0.0004, 0.0005, 0.0003, 0.0003, 0.0004, 0.0004, 0.0005, 0.0004,\n",
      "                  0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0003, 0.0005, 0.0005, 0.0004,\n",
      "                  0.0004, 0.0004, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004,\n",
      "                  0.0003, 0.0003, 0.0005, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004,\n",
      "                  0.0003, 0.0004, 0.0003, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004,\n",
      "                  0.0004, 0.0005, 0.0005, 0.0004, 0.0003, 0.0004, 0.0005, 0.0005, 0.0004,\n",
      "                  0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0005, 0.0003,\n",
      "                  0.0005, 0.0006, 0.0005, 0.0003, 0.0005, 0.0004, 0.0005, 0.0004, 0.0004,\n",
      "                  0.0006, 0.0004]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "            min_val=tensor([-0.0728, -0.0621, -0.0462, -0.0522, -0.0577, -0.0551, -0.0462, -0.0568,\n",
      "                    -0.0417, -0.0538, -0.0710, -0.0462, -0.0608, -0.0635, -0.0517, -0.0718,\n",
      "                    -0.0537, -0.0508, -0.0534, -0.0399, -0.0678, -0.0516, -0.0619, -0.0413,\n",
      "                    -0.0607, -0.0463, -0.0604, -0.0632, -0.0455, -0.0581, -0.0442, -0.0670,\n",
      "                    -0.0593, -0.0722, -0.0451, -0.0619, -0.0463, -0.0539, -0.0374, -0.0683,\n",
      "                    -0.0620, -0.0543, -0.0708, -0.0512, -0.0510, -0.0578, -0.0524, -0.0402,\n",
      "                    -0.0506, -0.0609, -0.0493, -0.0317, -0.0534, -0.0655, -0.0445, -0.0543,\n",
      "                    -0.0574, -0.0419, -0.0426, -0.0483, -0.0555, -0.0594, -0.0552, -0.0449,\n",
      "                    -0.0661, -0.0576, -0.0506, -0.0543, -0.0388, -0.0689, -0.0698, -0.0532,\n",
      "                    -0.0503, -0.0460, -0.0546, -0.0655, -0.0620, -0.0556, -0.0614, -0.0537,\n",
      "                    -0.0498, -0.0391, -0.0441, -0.0608, -0.0546, -0.0534, -0.0581, -0.0568,\n",
      "                    -0.0546, -0.0520, -0.0361, -0.0547, -0.0365, -0.0551, -0.0697, -0.0498,\n",
      "                    -0.0495, -0.0505, -0.0497, -0.0526, -0.0583, -0.0538, -0.0552, -0.0418,\n",
      "                    -0.0538, -0.0630, -0.0590, -0.0461, -0.0533, -0.0548, -0.0443, -0.0599,\n",
      "                    -0.0500, -0.0451, -0.0502, -0.0602, -0.0412, -0.0604, -0.0777, -0.0578,\n",
      "                    -0.0444, -0.0518, -0.0527, -0.0644, -0.0496, -0.0499, -0.0715, -0.0539]), max_val=tensor([0.0580, 0.0672, 0.0483, 0.0525, 0.0526, 0.0618, 0.0437, 0.0554, 0.0444,\n",
      "                    0.0542, 0.0682, 0.0484, 0.0501, 0.0629, 0.0439, 0.0711, 0.0535, 0.0526,\n",
      "                    0.0495, 0.0408, 0.0601, 0.0552, 0.0520, 0.0391, 0.0661, 0.0444, 0.0624,\n",
      "                    0.0567, 0.0438, 0.0506, 0.0470, 0.0702, 0.0443, 0.0593, 0.0479, 0.0634,\n",
      "                    0.0394, 0.0541, 0.0411, 0.0701, 0.0622, 0.0578, 0.0742, 0.0520, 0.0509,\n",
      "                    0.0557, 0.0529, 0.0398, 0.0452, 0.0591, 0.0415, 0.0385, 0.0548, 0.0592,\n",
      "                    0.0447, 0.0436, 0.0598, 0.0422, 0.0389, 0.0465, 0.0431, 0.0533, 0.0444,\n",
      "                    0.0465, 0.0664, 0.0532, 0.0564, 0.0516, 0.0391, 0.0680, 0.0618, 0.0449,\n",
      "                    0.0544, 0.0516, 0.0534, 0.0637, 0.0599, 0.0547, 0.0487, 0.0582, 0.0494,\n",
      "                    0.0347, 0.0397, 0.0589, 0.0502, 0.0512, 0.0583, 0.0544, 0.0490, 0.0525,\n",
      "                    0.0363, 0.0487, 0.0381, 0.0600, 0.0661, 0.0451, 0.0470, 0.0513, 0.0465,\n",
      "                    0.0494, 0.0558, 0.0611, 0.0542, 0.0401, 0.0510, 0.0610, 0.0570, 0.0452,\n",
      "                    0.0458, 0.0528, 0.0477, 0.0691, 0.0498, 0.0422, 0.0526, 0.0598, 0.0387,\n",
      "                    0.0649, 0.0777, 0.0583, 0.0421, 0.0591, 0.0562, 0.0631, 0.0545, 0.0480,\n",
      "                    0.0684, 0.0524])\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): ConvBn2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004,\n",
      "                  0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005,\n",
      "                  0.0005, 0.0005, 0.0006, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
      "                  0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005, 0.0003, 0.0004,\n",
      "                  0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0006, 0.0004, 0.0005,\n",
      "                  0.0005, 0.0004, 0.0004, 0.0004, 0.0003, 0.0004, 0.0005, 0.0004, 0.0005,\n",
      "                  0.0005, 0.0004, 0.0003, 0.0004, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004,\n",
      "                  0.0004, 0.0005, 0.0004, 0.0004, 0.0005, 0.0005, 0.0006, 0.0004, 0.0004,\n",
      "                  0.0004, 0.0005, 0.0006, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005,\n",
      "                  0.0006, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005,\n",
      "                  0.0003, 0.0004, 0.0005, 0.0003, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005,\n",
      "                  0.0005, 0.0005, 0.0004, 0.0005, 0.0004, 0.0004, 0.0006, 0.0004, 0.0004,\n",
      "                  0.0004, 0.0004, 0.0004, 0.0005, 0.0003, 0.0005, 0.0005, 0.0005, 0.0004,\n",
      "                  0.0005, 0.0006, 0.0005, 0.0004, 0.0004, 0.0004, 0.0006, 0.0005, 0.0005,\n",
      "                  0.0005, 0.0005]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "            min_val=tensor([-0.0532, -0.0574, -0.0573, -0.0627, -0.0599, -0.0571, -0.0476, -0.0445,\n",
      "                    -0.0450, -0.0506, -0.0673, -0.0635, -0.0521, -0.0475, -0.0608, -0.0575,\n",
      "                    -0.0564, -0.0620, -0.0590, -0.0696, -0.0821, -0.0626, -0.0477, -0.0554,\n",
      "                    -0.0539, -0.0491, -0.0529, -0.0575, -0.0581, -0.0454, -0.0626, -0.0582,\n",
      "                    -0.0544, -0.0585, -0.0406, -0.0525, -0.0561, -0.0617, -0.0503, -0.0614,\n",
      "                    -0.0605, -0.0576, -0.0805, -0.0552, -0.0528, -0.0584, -0.0478, -0.0521,\n",
      "                    -0.0480, -0.0426, -0.0522, -0.0581, -0.0528, -0.0592, -0.0569, -0.0506,\n",
      "                    -0.0413, -0.0545, -0.0538, -0.0649, -0.0593, -0.0427, -0.0507, -0.0549,\n",
      "                    -0.0591, -0.0533, -0.0478, -0.0685, -0.0596, -0.0720, -0.0538, -0.0515,\n",
      "                    -0.0466, -0.0651, -0.0724, -0.0670, -0.0542, -0.0603, -0.0546, -0.0572,\n",
      "                    -0.0569, -0.0639, -0.0475, -0.0561, -0.0467, -0.0654, -0.0652, -0.0577,\n",
      "                    -0.0491, -0.0587, -0.0447, -0.0496, -0.0584, -0.0391, -0.0602, -0.0566,\n",
      "                    -0.0521, -0.0500, -0.0581, -0.0593, -0.0590, -0.0433, -0.0651, -0.0485,\n",
      "                    -0.0520, -0.0758, -0.0545, -0.0536, -0.0486, -0.0451, -0.0469, -0.0669,\n",
      "                    -0.0384, -0.0652, -0.0653, -0.0581, -0.0548, -0.0592, -0.0695, -0.0638,\n",
      "                    -0.0537, -0.0501, -0.0455, -0.0682, -0.0648, -0.0574, -0.0564, -0.0650]), max_val=tensor([0.0525, 0.0602, 0.0558, 0.0627, 0.0543, 0.0541, 0.0483, 0.0501, 0.0448,\n",
      "                    0.0522, 0.0645, 0.0619, 0.0513, 0.0494, 0.0549, 0.0591, 0.0533, 0.0581,\n",
      "                    0.0567, 0.0643, 0.0773, 0.0600, 0.0461, 0.0475, 0.0552, 0.0491, 0.0521,\n",
      "                    0.0568, 0.0565, 0.0398, 0.0527, 0.0595, 0.0518, 0.0574, 0.0408, 0.0502,\n",
      "                    0.0517, 0.0587, 0.0473, 0.0534, 0.0592, 0.0569, 0.0819, 0.0515, 0.0607,\n",
      "                    0.0605, 0.0462, 0.0478, 0.0469, 0.0405, 0.0498, 0.0577, 0.0565, 0.0562,\n",
      "                    0.0621, 0.0474, 0.0443, 0.0553, 0.0536, 0.0642, 0.0527, 0.0499, 0.0553,\n",
      "                    0.0566, 0.0492, 0.0542, 0.0451, 0.0646, 0.0598, 0.0699, 0.0555, 0.0506,\n",
      "                    0.0423, 0.0651, 0.0653, 0.0655, 0.0550, 0.0645, 0.0575, 0.0518, 0.0584,\n",
      "                    0.0700, 0.0527, 0.0588, 0.0497, 0.0671, 0.0589, 0.0597, 0.0483, 0.0574,\n",
      "                    0.0429, 0.0510, 0.0577, 0.0369, 0.0631, 0.0593, 0.0470, 0.0519, 0.0548,\n",
      "                    0.0573, 0.0638, 0.0489, 0.0646, 0.0467, 0.0465, 0.0731, 0.0495, 0.0476,\n",
      "                    0.0539, 0.0499, 0.0436, 0.0641, 0.0385, 0.0666, 0.0630, 0.0565, 0.0498,\n",
      "                    0.0610, 0.0723, 0.0534, 0.0496, 0.0504, 0.0444, 0.0746, 0.0569, 0.0572,\n",
      "                    0.0625, 0.0643])\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (activation_post_process_6): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1639]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.500310897827148, max_val=10.316761016845703)\n",
      "  )\n",
      "  (activation_post_process_7): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0891]), zero_point=tensor([11], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9818507432937622, max_val=10.334654808044434)\n",
      "  )\n",
      "  (activation_post_process_8): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1247]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.853060245513916, max_val=7.979063510894775)\n",
      "  )\n",
      "  (activation_post_process_9): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0707]), zero_point=tensor([14], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9904582500457764, max_val=7.988113880157471)\n",
      "  )\n",
      "  (activation_post_process_10): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1434]), zero_point=tensor([14], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0083823204040527, max_val=16.204833984375)\n",
      "  )\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (activation_post_process_11): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2868]), zero_point=tensor([14], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.016754150390625, max_val=32.40677261352539)\n",
      "  )\n",
      "  (conv3): Module(\n",
      "    (0): ConvBn2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
      "                0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
      "                0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
      "                0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "          min_val=tensor([-0.0314, -0.0275, -0.0255, -0.0295, -0.0242, -0.0328, -0.0271, -0.0273,\n",
      "                  -0.0266, -0.0254, -0.0246, -0.0264, -0.0244, -0.0295, -0.0311, -0.0276,\n",
      "                  -0.0255, -0.0299, -0.0293, -0.0285, -0.0245, -0.0257, -0.0261, -0.0289,\n",
      "                  -0.0247, -0.0260, -0.0247, -0.0284, -0.0238, -0.0250, -0.0324, -0.0230,\n",
      "                  -0.0295, -0.0252, -0.0278, -0.0325, -0.0265, -0.0329, -0.0278, -0.0318,\n",
      "                  -0.0257, -0.0289, -0.0280, -0.0232, -0.0290, -0.0239, -0.0244, -0.0254,\n",
      "                  -0.0223, -0.0270, -0.0274, -0.0279, -0.0283, -0.0294, -0.0310, -0.0288,\n",
      "                  -0.0243, -0.0336, -0.0279, -0.0260, -0.0270, -0.0270, -0.0305, -0.0258,\n",
      "                  -0.0279, -0.0310, -0.0245, -0.0286, -0.0265, -0.0277, -0.0286, -0.0315,\n",
      "                  -0.0292, -0.0233, -0.0357, -0.0296, -0.0234, -0.0258, -0.0294, -0.0263,\n",
      "                  -0.0328, -0.0296, -0.0274, -0.0290, -0.0237, -0.0304, -0.0270, -0.0259,\n",
      "                  -0.0250, -0.0232, -0.0306, -0.0277, -0.0234, -0.0280, -0.0284, -0.0271,\n",
      "                  -0.0254, -0.0254, -0.0277, -0.0240, -0.0292, -0.0274, -0.0344, -0.0266,\n",
      "                  -0.0263, -0.0270, -0.0342, -0.0244, -0.0330, -0.0269, -0.0277, -0.0267,\n",
      "                  -0.0260, -0.0245, -0.0229, -0.0266, -0.0318, -0.0248, -0.0294, -0.0304,\n",
      "                  -0.0261, -0.0212, -0.0255, -0.0288, -0.0288, -0.0291, -0.0240, -0.0296,\n",
      "                  -0.0336, -0.0229, -0.0294, -0.0273, -0.0278, -0.0313, -0.0293, -0.0294,\n",
      "                  -0.0275, -0.0293, -0.0270, -0.0303, -0.0291, -0.0268, -0.0202, -0.0278,\n",
      "                  -0.0300, -0.0282, -0.0283, -0.0288, -0.0320, -0.0251, -0.0256, -0.0273,\n",
      "                  -0.0271, -0.0231, -0.0296, -0.0239, -0.0301, -0.0243, -0.0320, -0.0256,\n",
      "                  -0.0246, -0.0245, -0.0280, -0.0244, -0.0319, -0.0243, -0.0228, -0.0253,\n",
      "                  -0.0234, -0.0246, -0.0293, -0.0272, -0.0224, -0.0230, -0.0281, -0.0295,\n",
      "                  -0.0280, -0.0229, -0.0261, -0.0320, -0.0240, -0.0257, -0.0295, -0.0218,\n",
      "                  -0.0278, -0.0248, -0.0309, -0.0270, -0.0230, -0.0259, -0.0285, -0.0249,\n",
      "                  -0.0282, -0.0271, -0.0264, -0.0261, -0.0257, -0.0290, -0.0296, -0.0279,\n",
      "                  -0.0225, -0.0270, -0.0252, -0.0221, -0.0282, -0.0241, -0.0210, -0.0249,\n",
      "                  -0.0280, -0.0269, -0.0266, -0.0279, -0.0221, -0.0257, -0.0226, -0.0305,\n",
      "                  -0.0270, -0.0328, -0.0278, -0.0315, -0.0248, -0.0271, -0.0285, -0.0283,\n",
      "                  -0.0250, -0.0227, -0.0283, -0.0261, -0.0266, -0.0280, -0.0282, -0.0229,\n",
      "                  -0.0298, -0.0264, -0.0299, -0.0238, -0.0182, -0.0270, -0.0292, -0.0288,\n",
      "                  -0.0275, -0.0244, -0.0266, -0.0261, -0.0258, -0.0285, -0.0291, -0.0313,\n",
      "                  -0.0292, -0.0269, -0.0292, -0.0248, -0.0257, -0.0238, -0.0278, -0.0246]), max_val=tensor([0.0316, 0.0243, 0.0256, 0.0303, 0.0236, 0.0328, 0.0258, 0.0280, 0.0266,\n",
      "                  0.0243, 0.0249, 0.0254, 0.0238, 0.0304, 0.0310, 0.0272, 0.0269, 0.0282,\n",
      "                  0.0279, 0.0291, 0.0279, 0.0267, 0.0248, 0.0286, 0.0246, 0.0250, 0.0224,\n",
      "                  0.0325, 0.0237, 0.0256, 0.0299, 0.0238, 0.0286, 0.0241, 0.0275, 0.0338,\n",
      "                  0.0249, 0.0320, 0.0273, 0.0286, 0.0256, 0.0276, 0.0294, 0.0241, 0.0295,\n",
      "                  0.0220, 0.0253, 0.0261, 0.0212, 0.0271, 0.0283, 0.0262, 0.0274, 0.0274,\n",
      "                  0.0321, 0.0301, 0.0206, 0.0332, 0.0288, 0.0284, 0.0267, 0.0256, 0.0305,\n",
      "                  0.0249, 0.0300, 0.0302, 0.0242, 0.0300, 0.0267, 0.0287, 0.0306, 0.0335,\n",
      "                  0.0269, 0.0236, 0.0329, 0.0322, 0.0216, 0.0253, 0.0299, 0.0278, 0.0337,\n",
      "                  0.0304, 0.0265, 0.0301, 0.0242, 0.0274, 0.0294, 0.0260, 0.0247, 0.0248,\n",
      "                  0.0268, 0.0267, 0.0231, 0.0279, 0.0285, 0.0257, 0.0253, 0.0224, 0.0278,\n",
      "                  0.0231, 0.0279, 0.0270, 0.0317, 0.0249, 0.0251, 0.0242, 0.0347, 0.0246,\n",
      "                  0.0335, 0.0278, 0.0284, 0.0284, 0.0254, 0.0246, 0.0221, 0.0270, 0.0276,\n",
      "                  0.0248, 0.0289, 0.0295, 0.0282, 0.0208, 0.0252, 0.0285, 0.0296, 0.0282,\n",
      "                  0.0247, 0.0267, 0.0327, 0.0238, 0.0284, 0.0265, 0.0256, 0.0296, 0.0308,\n",
      "                  0.0275, 0.0303, 0.0283, 0.0259, 0.0327, 0.0304, 0.0251, 0.0193, 0.0289,\n",
      "                  0.0308, 0.0286, 0.0280, 0.0275, 0.0285, 0.0272, 0.0249, 0.0255, 0.0268,\n",
      "                  0.0222, 0.0278, 0.0226, 0.0303, 0.0224, 0.0314, 0.0257, 0.0241, 0.0261,\n",
      "                  0.0269, 0.0212, 0.0331, 0.0254, 0.0230, 0.0250, 0.0215, 0.0274, 0.0285,\n",
      "                  0.0275, 0.0241, 0.0229, 0.0289, 0.0276, 0.0296, 0.0225, 0.0262, 0.0330,\n",
      "                  0.0238, 0.0256, 0.0299, 0.0238, 0.0275, 0.0266, 0.0288, 0.0249, 0.0233,\n",
      "                  0.0260, 0.0289, 0.0229, 0.0282, 0.0271, 0.0242, 0.0237, 0.0251, 0.0274,\n",
      "                  0.0266, 0.0255, 0.0248, 0.0280, 0.0244, 0.0248, 0.0300, 0.0248, 0.0198,\n",
      "                  0.0249, 0.0263, 0.0253, 0.0273, 0.0261, 0.0220, 0.0274, 0.0221, 0.0281,\n",
      "                  0.0249, 0.0341, 0.0298, 0.0282, 0.0257, 0.0276, 0.0273, 0.0281, 0.0257,\n",
      "                  0.0216, 0.0304, 0.0276, 0.0275, 0.0272, 0.0239, 0.0249, 0.0277, 0.0259,\n",
      "                  0.0318, 0.0218, 0.0202, 0.0279, 0.0309, 0.0287, 0.0297, 0.0222, 0.0244,\n",
      "                  0.0248, 0.0274, 0.0266, 0.0292, 0.0305, 0.0277, 0.0269, 0.0265, 0.0241,\n",
      "                  0.0254, 0.0244, 0.0229, 0.0264])\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "  )\n",
      "  (activation_post_process_12): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1552]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.637715339660645, max_val=10.074934959411621)\n",
      "  )\n",
      "  (activation_post_process_13): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0872]), zero_point=tensor([11], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9618643522262573, max_val=10.111359596252441)\n",
      "  )\n",
      "  (conv4): Module(\n",
      "    (0): ConvBn2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,\n",
      "                0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0002, 0.0002,\n",
      "                0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002,\n",
      "                0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0004,\n",
      "                0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,\n",
      "                0.0003, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003,\n",
      "                0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003,\n",
      "                0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003,\n",
      "                0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002,\n",
      "                0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
      "                0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "                0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "                0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002,\n",
      "                0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003,\n",
      "                0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002,\n",
      "                0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003,\n",
      "                0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002,\n",
      "                0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002,\n",
      "                0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003,\n",
      "                0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,\n",
      "                0.0002, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0002,\n",
      "                0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003,\n",
      "                0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                0.0002, 0.0003, 0.0002, 0.0002]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "          min_val=tensor([-0.0279, -0.0266, -0.0315, -0.0310, -0.0309, -0.0299, -0.0308, -0.0302,\n",
      "                  -0.0372, -0.0285, -0.0379, -0.0313, -0.0294, -0.0415, -0.0336, -0.0244,\n",
      "                  -0.0293, -0.0262, -0.0351, -0.0294, -0.0341, -0.0285, -0.0296, -0.0322,\n",
      "                  -0.0308, -0.0326, -0.0267, -0.0295, -0.0361, -0.0318, -0.0337, -0.0335,\n",
      "                  -0.0371, -0.0301, -0.0311, -0.0332, -0.0326, -0.0359, -0.0303, -0.0314,\n",
      "                  -0.0326, -0.0287, -0.0329, -0.0281, -0.0450, -0.0323, -0.0404, -0.0299,\n",
      "                  -0.0260, -0.0255, -0.0318, -0.0373, -0.0341, -0.0339, -0.0310, -0.0317,\n",
      "                  -0.0304, -0.0285, -0.0260, -0.0233, -0.0343, -0.0257, -0.0286, -0.0299,\n",
      "                  -0.0300, -0.0251, -0.0253, -0.0320, -0.0299, -0.0315, -0.0306, -0.0283,\n",
      "                  -0.0386, -0.0314, -0.0275, -0.0266, -0.0311, -0.0315, -0.0284, -0.0394,\n",
      "                  -0.0303, -0.0368, -0.0292, -0.0292, -0.0297, -0.0306, -0.0243, -0.0259,\n",
      "                  -0.0353, -0.0296, -0.0390, -0.0365, -0.0286, -0.0270, -0.0327, -0.0281,\n",
      "                  -0.0357, -0.0318, -0.0350, -0.0319, -0.0316, -0.0285, -0.0266, -0.0247,\n",
      "                  -0.0329, -0.0303, -0.0305, -0.0323, -0.0272, -0.0327, -0.0293, -0.0301,\n",
      "                  -0.0352, -0.0309, -0.0286, -0.0315, -0.0219, -0.0312, -0.0261, -0.0416,\n",
      "                  -0.0257, -0.0283, -0.0291, -0.0309, -0.0284, -0.0319, -0.0363, -0.0309,\n",
      "                  -0.0319, -0.0278, -0.0355, -0.0335, -0.0312, -0.0382, -0.0266, -0.0303,\n",
      "                  -0.0242, -0.0304, -0.0318, -0.0271, -0.0335, -0.0360, -0.0327, -0.0248,\n",
      "                  -0.0382, -0.0268, -0.0311, -0.0338, -0.0267, -0.0310, -0.0345, -0.0298,\n",
      "                  -0.0250, -0.0318, -0.0419, -0.0281, -0.0290, -0.0267, -0.0344, -0.0292,\n",
      "                  -0.0421, -0.0360, -0.0254, -0.0286, -0.0299, -0.0263, -0.0320, -0.0303,\n",
      "                  -0.0277, -0.0444, -0.0338, -0.0406, -0.0350, -0.0302, -0.0322, -0.0377,\n",
      "                  -0.0313, -0.0345, -0.0407, -0.0281, -0.0362, -0.0328, -0.0265, -0.0268,\n",
      "                  -0.0293, -0.0280, -0.0261, -0.0320, -0.0331, -0.0349, -0.0339, -0.0308,\n",
      "                  -0.0368, -0.0258, -0.0274, -0.0317, -0.0246, -0.0276, -0.0291, -0.0295,\n",
      "                  -0.0258, -0.0280, -0.0215, -0.0272, -0.0314, -0.0287, -0.0271, -0.0302,\n",
      "                  -0.0321, -0.0410, -0.0306, -0.0295, -0.0322, -0.0381, -0.0286, -0.0344,\n",
      "                  -0.0278, -0.0285, -0.0302, -0.0273, -0.0342, -0.0305, -0.0345, -0.0235,\n",
      "                  -0.0364, -0.0281, -0.0379, -0.0363, -0.0279, -0.0301, -0.0303, -0.0313,\n",
      "                  -0.0260, -0.0248, -0.0270, -0.0266, -0.0320, -0.0346, -0.0316, -0.0415,\n",
      "                  -0.0360, -0.0276, -0.0349, -0.0388, -0.0341, -0.0322, -0.0396, -0.0282,\n",
      "                  -0.0315, -0.0324, -0.0333, -0.0324, -0.0292, -0.0338, -0.0273, -0.0290]), max_val=tensor([0.0262, 0.0265, 0.0311, 0.0313, 0.0344, 0.0283, 0.0348, 0.0313, 0.0352,\n",
      "                  0.0288, 0.0348, 0.0288, 0.0315, 0.0421, 0.0337, 0.0281, 0.0305, 0.0295,\n",
      "                  0.0341, 0.0282, 0.0340, 0.0279, 0.0279, 0.0331, 0.0304, 0.0341, 0.0293,\n",
      "                  0.0272, 0.0383, 0.0301, 0.0341, 0.0321, 0.0372, 0.0296, 0.0346, 0.0359,\n",
      "                  0.0322, 0.0365, 0.0304, 0.0310, 0.0331, 0.0283, 0.0334, 0.0272, 0.0454,\n",
      "                  0.0340, 0.0387, 0.0281, 0.0251, 0.0274, 0.0301, 0.0358, 0.0339, 0.0379,\n",
      "                  0.0331, 0.0298, 0.0310, 0.0259, 0.0243, 0.0246, 0.0314, 0.0273, 0.0312,\n",
      "                  0.0308, 0.0301, 0.0243, 0.0252, 0.0331, 0.0288, 0.0332, 0.0306, 0.0326,\n",
      "                  0.0435, 0.0308, 0.0294, 0.0253, 0.0331, 0.0302, 0.0285, 0.0373, 0.0318,\n",
      "                  0.0342, 0.0290, 0.0343, 0.0275, 0.0314, 0.0258, 0.0268, 0.0353, 0.0322,\n",
      "                  0.0403, 0.0340, 0.0275, 0.0261, 0.0306, 0.0284, 0.0347, 0.0326, 0.0340,\n",
      "                  0.0328, 0.0305, 0.0279, 0.0260, 0.0241, 0.0340, 0.0307, 0.0313, 0.0328,\n",
      "                  0.0277, 0.0319, 0.0288, 0.0348, 0.0338, 0.0323, 0.0284, 0.0345, 0.0228,\n",
      "                  0.0317, 0.0256, 0.0397, 0.0274, 0.0313, 0.0293, 0.0296, 0.0293, 0.0326,\n",
      "                  0.0358, 0.0319, 0.0324, 0.0303, 0.0376, 0.0330, 0.0329, 0.0362, 0.0275,\n",
      "                  0.0292, 0.0252, 0.0311, 0.0330, 0.0273, 0.0321, 0.0363, 0.0304, 0.0246,\n",
      "                  0.0409, 0.0294, 0.0311, 0.0315, 0.0259, 0.0296, 0.0367, 0.0290, 0.0237,\n",
      "                  0.0336, 0.0424, 0.0293, 0.0292, 0.0253, 0.0393, 0.0275, 0.0400, 0.0365,\n",
      "                  0.0264, 0.0271, 0.0301, 0.0290, 0.0306, 0.0315, 0.0305, 0.0411, 0.0326,\n",
      "                  0.0409, 0.0380, 0.0296, 0.0329, 0.0362, 0.0311, 0.0367, 0.0428, 0.0315,\n",
      "                  0.0388, 0.0346, 0.0258, 0.0272, 0.0275, 0.0273, 0.0268, 0.0314, 0.0316,\n",
      "                  0.0347, 0.0352, 0.0294, 0.0370, 0.0288, 0.0293, 0.0340, 0.0241, 0.0291,\n",
      "                  0.0310, 0.0339, 0.0260, 0.0309, 0.0219, 0.0265, 0.0329, 0.0321, 0.0268,\n",
      "                  0.0300, 0.0350, 0.0395, 0.0329, 0.0298, 0.0317, 0.0387, 0.0274, 0.0374,\n",
      "                  0.0287, 0.0288, 0.0300, 0.0291, 0.0342, 0.0301, 0.0333, 0.0245, 0.0382,\n",
      "                  0.0278, 0.0346, 0.0356, 0.0280, 0.0309, 0.0318, 0.0319, 0.0264, 0.0227,\n",
      "                  0.0293, 0.0263, 0.0306, 0.0346, 0.0285, 0.0374, 0.0338, 0.0279, 0.0350,\n",
      "                  0.0411, 0.0308, 0.0320, 0.0359, 0.0288, 0.0311, 0.0312, 0.0331, 0.0298,\n",
      "                  0.0287, 0.0342, 0.0290, 0.0297])\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (activation_post_process_14): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1436]), zero_point=tensor([57], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.184792518615723, max_val=10.05351448059082)\n",
      "  )\n",
      "  (activation_post_process_15): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0867]), zero_point=tensor([11], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.96070796251297, max_val=10.04925537109375)\n",
      "  )\n",
      "  (activation_post_process_16): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0867]), zero_point=tensor([11], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.96070796251297, max_val=10.04925537109375)\n",
      "  )\n",
      "  (res2): Module(\n",
      "    (0): Module(\n",
      "      (0): ConvBn2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0004, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0002, 0.0003]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "            min_val=tensor([-0.0335, -0.0404, -0.0404, -0.0281, -0.0372, -0.0296, -0.0301, -0.0330,\n",
      "                    -0.0285, -0.0323, -0.0342, -0.0352, -0.0363, -0.0352, -0.0309, -0.0340,\n",
      "                    -0.0347, -0.0337, -0.0334, -0.0285, -0.0324, -0.0390, -0.0353, -0.0245,\n",
      "                    -0.0439, -0.0289, -0.0392, -0.0363, -0.0342, -0.0327, -0.0373, -0.0350,\n",
      "                    -0.0306, -0.0330, -0.0293, -0.0346, -0.0304, -0.0332, -0.0295, -0.0324,\n",
      "                    -0.0422, -0.0345, -0.0333, -0.0288, -0.0315, -0.0327, -0.0305, -0.0389,\n",
      "                    -0.0289, -0.0428, -0.0269, -0.0371, -0.0304, -0.0327, -0.0374, -0.0316,\n",
      "                    -0.0312, -0.0358, -0.0311, -0.0298, -0.0262, -0.0333, -0.0291, -0.0355,\n",
      "                    -0.0336, -0.0326, -0.0335, -0.0464, -0.0335, -0.0370, -0.0404, -0.0307,\n",
      "                    -0.0308, -0.0384, -0.0323, -0.0307, -0.0326, -0.0391, -0.0298, -0.0322,\n",
      "                    -0.0336, -0.0410, -0.0335, -0.0259, -0.0365, -0.0345, -0.0357, -0.0362,\n",
      "                    -0.0416, -0.0320, -0.0356, -0.0411, -0.0365, -0.0368, -0.0322, -0.0342,\n",
      "                    -0.0414, -0.0275, -0.0353, -0.0338, -0.0358, -0.0359, -0.0261, -0.0362,\n",
      "                    -0.0335, -0.0389, -0.0304, -0.0326, -0.0469, -0.0315, -0.0323, -0.0345,\n",
      "                    -0.0299, -0.0336, -0.0380, -0.0412, -0.0274, -0.0314, -0.0333, -0.0290,\n",
      "                    -0.0393, -0.0365, -0.0324, -0.0298, -0.0330, -0.0388, -0.0337, -0.0355,\n",
      "                    -0.0331, -0.0381, -0.0320, -0.0369, -0.0350, -0.0312, -0.0332, -0.0291,\n",
      "                    -0.0302, -0.0355, -0.0426, -0.0329, -0.0372, -0.0362, -0.0351, -0.0357,\n",
      "                    -0.0369, -0.0319, -0.0323, -0.0309, -0.0299, -0.0425, -0.0387, -0.0384,\n",
      "                    -0.0380, -0.0356, -0.0377, -0.0305, -0.0344, -0.0331, -0.0294, -0.0404,\n",
      "                    -0.0364, -0.0403, -0.0386, -0.0292, -0.0362, -0.0336, -0.0362, -0.0308,\n",
      "                    -0.0408, -0.0312, -0.0408, -0.0311, -0.0321, -0.0357, -0.0361, -0.0346,\n",
      "                    -0.0434, -0.0327, -0.0428, -0.0344, -0.0409, -0.0431, -0.0330, -0.0357,\n",
      "                    -0.0311, -0.0347, -0.0341, -0.0411, -0.0300, -0.0376, -0.0423, -0.0383,\n",
      "                    -0.0389, -0.0291, -0.0290, -0.0276, -0.0310, -0.0361, -0.0359, -0.0341,\n",
      "                    -0.0307, -0.0329, -0.0371, -0.0377, -0.0331, -0.0383, -0.0328, -0.0305,\n",
      "                    -0.0354, -0.0299, -0.0300, -0.0401, -0.0343, -0.0307, -0.0340, -0.0295,\n",
      "                    -0.0300, -0.0308, -0.0353, -0.0322, -0.0291, -0.0336, -0.0283, -0.0426,\n",
      "                    -0.0331, -0.0304, -0.0393, -0.0341, -0.0367, -0.0370, -0.0335, -0.0353,\n",
      "                    -0.0377, -0.0359, -0.0305, -0.0328, -0.0385, -0.0306, -0.0366, -0.0291,\n",
      "                    -0.0326, -0.0313, -0.0315, -0.0310, -0.0339, -0.0367, -0.0294, -0.0379,\n",
      "                    -0.0310, -0.0417, -0.0411, -0.0256, -0.0426, -0.0367, -0.0301, -0.0426]), max_val=tensor([0.0360, 0.0401, 0.0415, 0.0284, 0.0378, 0.0310, 0.0280, 0.0350, 0.0307,\n",
      "                    0.0365, 0.0374, 0.0329, 0.0361, 0.0360, 0.0324, 0.0322, 0.0348, 0.0338,\n",
      "                    0.0343, 0.0319, 0.0348, 0.0383, 0.0364, 0.0240, 0.0405, 0.0264, 0.0355,\n",
      "                    0.0353, 0.0351, 0.0342, 0.0373, 0.0334, 0.0317, 0.0359, 0.0312, 0.0351,\n",
      "                    0.0311, 0.0355, 0.0321, 0.0366, 0.0388, 0.0338, 0.0348, 0.0292, 0.0307,\n",
      "                    0.0355, 0.0321, 0.0400, 0.0289, 0.0397, 0.0279, 0.0334, 0.0344, 0.0336,\n",
      "                    0.0364, 0.0348, 0.0327, 0.0411, 0.0318, 0.0278, 0.0302, 0.0297, 0.0304,\n",
      "                    0.0361, 0.0318, 0.0346, 0.0333, 0.0456, 0.0337, 0.0397, 0.0373, 0.0293,\n",
      "                    0.0283, 0.0375, 0.0342, 0.0306, 0.0326, 0.0412, 0.0288, 0.0413, 0.0306,\n",
      "                    0.0395, 0.0327, 0.0247, 0.0401, 0.0380, 0.0356, 0.0339, 0.0361, 0.0323,\n",
      "                    0.0346, 0.0422, 0.0336, 0.0375, 0.0334, 0.0353, 0.0446, 0.0248, 0.0348,\n",
      "                    0.0341, 0.0365, 0.0391, 0.0272, 0.0353, 0.0334, 0.0359, 0.0320, 0.0325,\n",
      "                    0.0511, 0.0303, 0.0332, 0.0371, 0.0314, 0.0331, 0.0351, 0.0393, 0.0283,\n",
      "                    0.0328, 0.0371, 0.0282, 0.0417, 0.0328, 0.0314, 0.0312, 0.0372, 0.0399,\n",
      "                    0.0325, 0.0316, 0.0345, 0.0344, 0.0355, 0.0343, 0.0340, 0.0338, 0.0306,\n",
      "                    0.0295, 0.0338, 0.0317, 0.0500, 0.0317, 0.0359, 0.0315, 0.0347, 0.0360,\n",
      "                    0.0353, 0.0348, 0.0329, 0.0347, 0.0321, 0.0389, 0.0391, 0.0407, 0.0371,\n",
      "                    0.0344, 0.0359, 0.0293, 0.0358, 0.0349, 0.0317, 0.0402, 0.0331, 0.0424,\n",
      "                    0.0387, 0.0300, 0.0369, 0.0338, 0.0332, 0.0319, 0.0391, 0.0299, 0.0389,\n",
      "                    0.0326, 0.0310, 0.0369, 0.0436, 0.0363, 0.0444, 0.0291, 0.0437, 0.0328,\n",
      "                    0.0420, 0.0399, 0.0317, 0.0376, 0.0267, 0.0372, 0.0303, 0.0400, 0.0288,\n",
      "                    0.0407, 0.0439, 0.0358, 0.0366, 0.0286, 0.0278, 0.0289, 0.0299, 0.0374,\n",
      "                    0.0375, 0.0345, 0.0311, 0.0296, 0.0346, 0.0370, 0.0319, 0.0363, 0.0365,\n",
      "                    0.0307, 0.0324, 0.0302, 0.0296, 0.0424, 0.0358, 0.0299, 0.0359, 0.0296,\n",
      "                    0.0310, 0.0318, 0.0343, 0.0344, 0.0324, 0.0347, 0.0286, 0.0403, 0.0352,\n",
      "                    0.0283, 0.0362, 0.0335, 0.0364, 0.0375, 0.0333, 0.0396, 0.0359, 0.0336,\n",
      "                    0.0318, 0.0306, 0.0418, 0.0283, 0.0384, 0.0300, 0.0356, 0.0310, 0.0352,\n",
      "                    0.0332, 0.0365, 0.0351, 0.0287, 0.0360, 0.0320, 0.0399, 0.0405, 0.0247,\n",
      "                    0.0412, 0.0385, 0.0292, 0.0399])\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): ConvBn2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002,\n",
      "                  0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0004, 0.0004, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0004, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003,\n",
      "                  0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "                  0.0002, 0.0003, 0.0003, 0.0003]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "            min_val=tensor([-0.0321, -0.0331, -0.0328, -0.0371, -0.0387, -0.0360, -0.0345, -0.0366,\n",
      "                    -0.0384, -0.0285, -0.0385, -0.0359, -0.0301, -0.0348, -0.0348, -0.0364,\n",
      "                    -0.0300, -0.0315, -0.0308, -0.0342, -0.0317, -0.0390, -0.0348, -0.0371,\n",
      "                    -0.0354, -0.0289, -0.0371, -0.0352, -0.0404, -0.0425, -0.0288, -0.0416,\n",
      "                    -0.0320, -0.0359, -0.0341, -0.0396, -0.0368, -0.0325, -0.0320, -0.0272,\n",
      "                    -0.0348, -0.0281, -0.0275, -0.0329, -0.0386, -0.0378, -0.0349, -0.0331,\n",
      "                    -0.0364, -0.0371, -0.0385, -0.0276, -0.0340, -0.0326, -0.0341, -0.0298,\n",
      "                    -0.0345, -0.0282, -0.0361, -0.0310, -0.0426, -0.0397, -0.0313, -0.0283,\n",
      "                    -0.0333, -0.0337, -0.0282, -0.0348, -0.0340, -0.0337, -0.0354, -0.0433,\n",
      "                    -0.0334, -0.0342, -0.0396, -0.0333, -0.0334, -0.0376, -0.0318, -0.0350,\n",
      "                    -0.0316, -0.0343, -0.0315, -0.0310, -0.0410, -0.0361, -0.0310, -0.0354,\n",
      "                    -0.0318, -0.0344, -0.0433, -0.0370, -0.0350, -0.0342, -0.0382, -0.0373,\n",
      "                    -0.0356, -0.0307, -0.0399, -0.0374, -0.0317, -0.0345, -0.0310, -0.0279,\n",
      "                    -0.0289, -0.0330, -0.0372, -0.0326, -0.0311, -0.0366, -0.0346, -0.0344,\n",
      "                    -0.0320, -0.0356, -0.0267, -0.0387, -0.0291, -0.0302, -0.0393, -0.0337,\n",
      "                    -0.0390, -0.0296, -0.0306, -0.0366, -0.0389, -0.0376, -0.0275, -0.0334,\n",
      "                    -0.0384, -0.0323, -0.0386, -0.0343, -0.0317, -0.0370, -0.0375, -0.0344,\n",
      "                    -0.0366, -0.0353, -0.0298, -0.0343, -0.0350, -0.0385, -0.0376, -0.0351,\n",
      "                    -0.0349, -0.0382, -0.0372, -0.0360, -0.0314, -0.0497, -0.0386, -0.0296,\n",
      "                    -0.0383, -0.0318, -0.0370, -0.0323, -0.0356, -0.0386, -0.0349, -0.0386,\n",
      "                    -0.0347, -0.0391, -0.0288, -0.0278, -0.0322, -0.0349, -0.0290, -0.0276,\n",
      "                    -0.0365, -0.0379, -0.0387, -0.0370, -0.0367, -0.0349, -0.0392, -0.0336,\n",
      "                    -0.0366, -0.0332, -0.0315, -0.0303, -0.0376, -0.0332, -0.0347, -0.0293,\n",
      "                    -0.0299, -0.0316, -0.0336, -0.0318, -0.0370, -0.0378, -0.0386, -0.0334,\n",
      "                    -0.0338, -0.0307, -0.0365, -0.0332, -0.0386, -0.0385, -0.0378, -0.0340,\n",
      "                    -0.0372, -0.0322, -0.0315, -0.0389, -0.0342, -0.0355, -0.0339, -0.0369,\n",
      "                    -0.0295, -0.0345, -0.0374, -0.0358, -0.0326, -0.0401, -0.0338, -0.0328,\n",
      "                    -0.0376, -0.0357, -0.0344, -0.0358, -0.0367, -0.0307, -0.0413, -0.0316,\n",
      "                    -0.0344, -0.0336, -0.0362, -0.0359, -0.0445, -0.0358, -0.0362, -0.0352,\n",
      "                    -0.0298, -0.0311, -0.0323, -0.0321, -0.0342, -0.0325, -0.0363, -0.0405,\n",
      "                    -0.0291, -0.0288, -0.0356, -0.0405, -0.0363, -0.0422, -0.0370, -0.0334,\n",
      "                    -0.0273, -0.0391, -0.0343, -0.0361, -0.0296, -0.0352, -0.0394, -0.0340]), max_val=tensor([0.0311, 0.0322, 0.0312, 0.0341, 0.0371, 0.0322, 0.0328, 0.0365, 0.0365,\n",
      "                    0.0305, 0.0364, 0.0390, 0.0303, 0.0366, 0.0326, 0.0383, 0.0291, 0.0304,\n",
      "                    0.0305, 0.0330, 0.0309, 0.0342, 0.0387, 0.0378, 0.0340, 0.0283, 0.0395,\n",
      "                    0.0333, 0.0453, 0.0455, 0.0289, 0.0373, 0.0319, 0.0334, 0.0345, 0.0364,\n",
      "                    0.0383, 0.0357, 0.0317, 0.0271, 0.0357, 0.0299, 0.0310, 0.0363, 0.0383,\n",
      "                    0.0369, 0.0381, 0.0350, 0.0319, 0.0338, 0.0374, 0.0264, 0.0337, 0.0335,\n",
      "                    0.0396, 0.0295, 0.0367, 0.0305, 0.0339, 0.0313, 0.0384, 0.0351, 0.0307,\n",
      "                    0.0283, 0.0319, 0.0319, 0.0308, 0.0328, 0.0352, 0.0315, 0.0336, 0.0446,\n",
      "                    0.0358, 0.0358, 0.0389, 0.0327, 0.0295, 0.0355, 0.0311, 0.0354, 0.0318,\n",
      "                    0.0342, 0.0342, 0.0319, 0.0355, 0.0370, 0.0302, 0.0380, 0.0312, 0.0386,\n",
      "                    0.0396, 0.0369, 0.0359, 0.0328, 0.0403, 0.0391, 0.0377, 0.0325, 0.0405,\n",
      "                    0.0346, 0.0323, 0.0347, 0.0306, 0.0308, 0.0298, 0.0322, 0.0367, 0.0340,\n",
      "                    0.0307, 0.0359, 0.0358, 0.0373, 0.0347, 0.0360, 0.0288, 0.0353, 0.0287,\n",
      "                    0.0309, 0.0402, 0.0376, 0.0378, 0.0273, 0.0312, 0.0373, 0.0402, 0.0371,\n",
      "                    0.0305, 0.0308, 0.0377, 0.0333, 0.0419, 0.0334, 0.0314, 0.0341, 0.0343,\n",
      "                    0.0362, 0.0325, 0.0321, 0.0319, 0.0371, 0.0337, 0.0335, 0.0358, 0.0350,\n",
      "                    0.0329, 0.0393, 0.0385, 0.0435, 0.0314, 0.0473, 0.0373, 0.0313, 0.0407,\n",
      "                    0.0281, 0.0376, 0.0318, 0.0362, 0.0398, 0.0343, 0.0389, 0.0342, 0.0362,\n",
      "                    0.0305, 0.0307, 0.0322, 0.0360, 0.0327, 0.0279, 0.0379, 0.0350, 0.0360,\n",
      "                    0.0377, 0.0375, 0.0372, 0.0387, 0.0377, 0.0365, 0.0374, 0.0348, 0.0307,\n",
      "                    0.0387, 0.0346, 0.0312, 0.0287, 0.0348, 0.0342, 0.0369, 0.0308, 0.0350,\n",
      "                    0.0365, 0.0369, 0.0329, 0.0367, 0.0310, 0.0347, 0.0349, 0.0380, 0.0410,\n",
      "                    0.0396, 0.0372, 0.0355, 0.0357, 0.0352, 0.0376, 0.0322, 0.0352, 0.0316,\n",
      "                    0.0365, 0.0314, 0.0355, 0.0351, 0.0391, 0.0330, 0.0366, 0.0360, 0.0338,\n",
      "                    0.0359, 0.0383, 0.0380, 0.0357, 0.0367, 0.0321, 0.0421, 0.0303, 0.0348,\n",
      "                    0.0321, 0.0403, 0.0369, 0.0409, 0.0367, 0.0358, 0.0334, 0.0305, 0.0310,\n",
      "                    0.0340, 0.0308, 0.0356, 0.0331, 0.0361, 0.0390, 0.0275, 0.0282, 0.0341,\n",
      "                    0.0375, 0.0410, 0.0379, 0.0350, 0.0330, 0.0265, 0.0356, 0.0343, 0.0386,\n",
      "                    0.0302, 0.0374, 0.0416, 0.0347])\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (activation_post_process_17): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1034]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.309671878814697, max_val=6.828219413757324)\n",
      "  )\n",
      "  (activation_post_process_18): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0616]), zero_point=tensor([16], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9863932132720947, max_val=6.840051651000977)\n",
      "  )\n",
      "  (activation_post_process_19): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1027]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.060937881469727, max_val=6.97986364364624)\n",
      "  )\n",
      "  (activation_post_process_20): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0628]), zero_point=tensor([16], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0061898231506348, max_val=6.9717254638671875)\n",
      "  )\n",
      "  (activation_post_process_21): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1344]), zero_point=tensor([15], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.023242712020874, max_val=15.048189163208008)\n",
      "  )\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (activation_post_process_22): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2103]), zero_point=tensor([19], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.995959758758545, max_val=22.713876724243164)\n",
      "  )\n",
      "  (conv5): Module(\n",
      "    (0): ConvBn2d(\n",
      "      256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0002, 0.0001,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0002,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0002,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0002,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0002,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0002,\n",
      "                0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0002,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0002,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0002, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002,\n",
      "                0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001,\n",
      "                0.0001, 0.0001, 0.0002, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0002,\n",
      "                0.0001, 0.0002, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "                0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0002, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0002, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "                0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "          min_val=tensor([-0.0174, -0.0174, -0.0191, -0.0178, -0.0146, -0.0187, -0.0172, -0.0162,\n",
      "                  -0.0146, -0.0125, -0.0210, -0.0152, -0.0177, -0.0175, -0.0160, -0.0180,\n",
      "                  -0.0148, -0.0184, -0.0176, -0.0159, -0.0164, -0.0169, -0.0189, -0.0148,\n",
      "                  -0.0171, -0.0163, -0.0169, -0.0161, -0.0142, -0.0170, -0.0148, -0.0159,\n",
      "                  -0.0173, -0.0175, -0.0175, -0.0171, -0.0173, -0.0164, -0.0173, -0.0156,\n",
      "                  -0.0146, -0.0167, -0.0139, -0.0158, -0.0174, -0.0161, -0.0170, -0.0202,\n",
      "                  -0.0177, -0.0182, -0.0171, -0.0185, -0.0155, -0.0153, -0.0155, -0.0162,\n",
      "                  -0.0165, -0.0157, -0.0165, -0.0153, -0.0207, -0.0205, -0.0151, -0.0156,\n",
      "                  -0.0197, -0.0164, -0.0185, -0.0151, -0.0182, -0.0142, -0.0158, -0.0164,\n",
      "                  -0.0169, -0.0181, -0.0141, -0.0165, -0.0153, -0.0180, -0.0174, -0.0191,\n",
      "                  -0.0155, -0.0141, -0.0154, -0.0160, -0.0150, -0.0176, -0.0152, -0.0210,\n",
      "                  -0.0171, -0.0162, -0.0168, -0.0158, -0.0164, -0.0168, -0.0165, -0.0187,\n",
      "                  -0.0154, -0.0153, -0.0151, -0.0161, -0.0201, -0.0171, -0.0142, -0.0188,\n",
      "                  -0.0167, -0.0162, -0.0182, -0.0201, -0.0173, -0.0155, -0.0154, -0.0167,\n",
      "                  -0.0143, -0.0172, -0.0168, -0.0148, -0.0168, -0.0168, -0.0163, -0.0189,\n",
      "                  -0.0158, -0.0162, -0.0150, -0.0168, -0.0154, -0.0161, -0.0158, -0.0191,\n",
      "                  -0.0183, -0.0167, -0.0157, -0.0136, -0.0185, -0.0149, -0.0196, -0.0134,\n",
      "                  -0.0164, -0.0140, -0.0151, -0.0155, -0.0189, -0.0155, -0.0186, -0.0158,\n",
      "                  -0.0156, -0.0154, -0.0154, -0.0160, -0.0181, -0.0151, -0.0147, -0.0198,\n",
      "                  -0.0163, -0.0186, -0.0169, -0.0132, -0.0189, -0.0193, -0.0153, -0.0167,\n",
      "                  -0.0187, -0.0183, -0.0163, -0.0193, -0.0183, -0.0150, -0.0163, -0.0141,\n",
      "                  -0.0156, -0.0158, -0.0160, -0.0184, -0.0145, -0.0164, -0.0188, -0.0150,\n",
      "                  -0.0185, -0.0158, -0.0185, -0.0180, -0.0173, -0.0166, -0.0179, -0.0154,\n",
      "                  -0.0155, -0.0152, -0.0186, -0.0180, -0.0156, -0.0165, -0.0187, -0.0142,\n",
      "                  -0.0165, -0.0174, -0.0191, -0.0153, -0.0173, -0.0175, -0.0142, -0.0172,\n",
      "                  -0.0170, -0.0186, -0.0189, -0.0175, -0.0184, -0.0192, -0.0201, -0.0168,\n",
      "                  -0.0181, -0.0167, -0.0168, -0.0195, -0.0161, -0.0157, -0.0171, -0.0162,\n",
      "                  -0.0173, -0.0145, -0.0192, -0.0190, -0.0147, -0.0167, -0.0190, -0.0149,\n",
      "                  -0.0193, -0.0173, -0.0150, -0.0137, -0.0202, -0.0154, -0.0171, -0.0173,\n",
      "                  -0.0154, -0.0171, -0.0174, -0.0151, -0.0189, -0.0162, -0.0176, -0.0162,\n",
      "                  -0.0159, -0.0167, -0.0200, -0.0174, -0.0161, -0.0186, -0.0152, -0.0179,\n",
      "                  -0.0156, -0.0159, -0.0161, -0.0173, -0.0154, -0.0185, -0.0190, -0.0170,\n",
      "                  -0.0183, -0.0173, -0.0181, -0.0156, -0.0159, -0.0197, -0.0167, -0.0164,\n",
      "                  -0.0172, -0.0152, -0.0169, -0.0196, -0.0172, -0.0200, -0.0151, -0.0182,\n",
      "                  -0.0165, -0.0150, -0.0212, -0.0147, -0.0182, -0.0164, -0.0176, -0.0165,\n",
      "                  -0.0163, -0.0170, -0.0189, -0.0171, -0.0176, -0.0211, -0.0155, -0.0190,\n",
      "                  -0.0176, -0.0170, -0.0190, -0.0152, -0.0160, -0.0167, -0.0154, -0.0170,\n",
      "                  -0.0180, -0.0160, -0.0175, -0.0188, -0.0159, -0.0155, -0.0171, -0.0189,\n",
      "                  -0.0155, -0.0179, -0.0190, -0.0196, -0.0166, -0.0154, -0.0145, -0.0173,\n",
      "                  -0.0145, -0.0152, -0.0150, -0.0176, -0.0166, -0.0172, -0.0171, -0.0169,\n",
      "                  -0.0143, -0.0161, -0.0156, -0.0168, -0.0192, -0.0211, -0.0182, -0.0175,\n",
      "                  -0.0152, -0.0180, -0.0160, -0.0137, -0.0170, -0.0163, -0.0174, -0.0155,\n",
      "                  -0.0191, -0.0178, -0.0175, -0.0145, -0.0168, -0.0187, -0.0178, -0.0171,\n",
      "                  -0.0174, -0.0190, -0.0212, -0.0165, -0.0167, -0.0175, -0.0168, -0.0169,\n",
      "                  -0.0148, -0.0153, -0.0174, -0.0149, -0.0167, -0.0162, -0.0165, -0.0201,\n",
      "                  -0.0222, -0.0172, -0.0153, -0.0153, -0.0174, -0.0167, -0.0174, -0.0172,\n",
      "                  -0.0173, -0.0209, -0.0177, -0.0166, -0.0161, -0.0157, -0.0166, -0.0164,\n",
      "                  -0.0205, -0.0160, -0.0150, -0.0171, -0.0190, -0.0176, -0.0170, -0.0182,\n",
      "                  -0.0161, -0.0147, -0.0158, -0.0157, -0.0160, -0.0173, -0.0146, -0.0197,\n",
      "                  -0.0165, -0.0170, -0.0168, -0.0184, -0.0191, -0.0189, -0.0160, -0.0135,\n",
      "                  -0.0192, -0.0173, -0.0156, -0.0159, -0.0179, -0.0164, -0.0168, -0.0190,\n",
      "                  -0.0164, -0.0189, -0.0214, -0.0189, -0.0179, -0.0168, -0.0197, -0.0172,\n",
      "                  -0.0164, -0.0172, -0.0167, -0.0200, -0.0179, -0.0176, -0.0172, -0.0181,\n",
      "                  -0.0186, -0.0207, -0.0172, -0.0158, -0.0159, -0.0185, -0.0141, -0.0157,\n",
      "                  -0.0141, -0.0139, -0.0182, -0.0142, -0.0162, -0.0177, -0.0162, -0.0154,\n",
      "                  -0.0167, -0.0183, -0.0198, -0.0167, -0.0197, -0.0172, -0.0132, -0.0161,\n",
      "                  -0.0189, -0.0178, -0.0141, -0.0175, -0.0167, -0.0174, -0.0147, -0.0221,\n",
      "                  -0.0161, -0.0169, -0.0154, -0.0165, -0.0153, -0.0161, -0.0155, -0.0181,\n",
      "                  -0.0166, -0.0179, -0.0194, -0.0168, -0.0168, -0.0186, -0.0177, -0.0151,\n",
      "                  -0.0150, -0.0174, -0.0169, -0.0205, -0.0181, -0.0173, -0.0160, -0.0163,\n",
      "                  -0.0189, -0.0168, -0.0163, -0.0160, -0.0188, -0.0166, -0.0181, -0.0149,\n",
      "                  -0.0146, -0.0175, -0.0171, -0.0185, -0.0129, -0.0170, -0.0161, -0.0168,\n",
      "                  -0.0188, -0.0165, -0.0159, -0.0173, -0.0179, -0.0173, -0.0163, -0.0182,\n",
      "                  -0.0198, -0.0170, -0.0172, -0.0160, -0.0179, -0.0150, -0.0191, -0.0145]), max_val=tensor([0.0148, 0.0174, 0.0202, 0.0178, 0.0156, 0.0201, 0.0162, 0.0159, 0.0149,\n",
      "                  0.0134, 0.0192, 0.0154, 0.0185, 0.0161, 0.0155, 0.0155, 0.0154, 0.0180,\n",
      "                  0.0166, 0.0156, 0.0169, 0.0161, 0.0191, 0.0157, 0.0171, 0.0151, 0.0146,\n",
      "                  0.0180, 0.0168, 0.0174, 0.0164, 0.0163, 0.0185, 0.0168, 0.0167, 0.0165,\n",
      "                  0.0172, 0.0153, 0.0168, 0.0146, 0.0146, 0.0169, 0.0156, 0.0169, 0.0184,\n",
      "                  0.0180, 0.0173, 0.0185, 0.0176, 0.0186, 0.0174, 0.0189, 0.0172, 0.0160,\n",
      "                  0.0156, 0.0171, 0.0149, 0.0154, 0.0187, 0.0172, 0.0192, 0.0208, 0.0153,\n",
      "                  0.0173, 0.0193, 0.0143, 0.0190, 0.0157, 0.0177, 0.0151, 0.0156, 0.0159,\n",
      "                  0.0170, 0.0163, 0.0169, 0.0165, 0.0162, 0.0177, 0.0199, 0.0175, 0.0137,\n",
      "                  0.0134, 0.0162, 0.0159, 0.0145, 0.0174, 0.0135, 0.0212, 0.0181, 0.0165,\n",
      "                  0.0172, 0.0172, 0.0156, 0.0155, 0.0167, 0.0191, 0.0155, 0.0159, 0.0148,\n",
      "                  0.0162, 0.0182, 0.0169, 0.0135, 0.0177, 0.0173, 0.0167, 0.0193, 0.0200,\n",
      "                  0.0172, 0.0154, 0.0143, 0.0164, 0.0154, 0.0167, 0.0165, 0.0139, 0.0178,\n",
      "                  0.0178, 0.0148, 0.0172, 0.0163, 0.0167, 0.0150, 0.0150, 0.0147, 0.0155,\n",
      "                  0.0157, 0.0188, 0.0170, 0.0164, 0.0143, 0.0150, 0.0193, 0.0148, 0.0186,\n",
      "                  0.0146, 0.0199, 0.0140, 0.0168, 0.0186, 0.0200, 0.0169, 0.0177, 0.0162,\n",
      "                  0.0165, 0.0154, 0.0158, 0.0172, 0.0184, 0.0148, 0.0150, 0.0192, 0.0145,\n",
      "                  0.0185, 0.0165, 0.0142, 0.0188, 0.0205, 0.0142, 0.0168, 0.0190, 0.0179,\n",
      "                  0.0162, 0.0181, 0.0165, 0.0175, 0.0169, 0.0148, 0.0163, 0.0177, 0.0153,\n",
      "                  0.0182, 0.0151, 0.0168, 0.0177, 0.0136, 0.0186, 0.0177, 0.0198, 0.0178,\n",
      "                  0.0173, 0.0174, 0.0169, 0.0173, 0.0167, 0.0162, 0.0172, 0.0198, 0.0156,\n",
      "                  0.0162, 0.0171, 0.0143, 0.0172, 0.0181, 0.0193, 0.0155, 0.0166, 0.0201,\n",
      "                  0.0143, 0.0183, 0.0163, 0.0179, 0.0192, 0.0163, 0.0179, 0.0183, 0.0196,\n",
      "                  0.0183, 0.0160, 0.0167, 0.0174, 0.0182, 0.0143, 0.0142, 0.0166, 0.0190,\n",
      "                  0.0165, 0.0152, 0.0219, 0.0179, 0.0147, 0.0202, 0.0176, 0.0156, 0.0186,\n",
      "                  0.0169, 0.0153, 0.0127, 0.0224, 0.0148, 0.0162, 0.0186, 0.0148, 0.0176,\n",
      "                  0.0184, 0.0161, 0.0172, 0.0180, 0.0187, 0.0180, 0.0167, 0.0177, 0.0213,\n",
      "                  0.0183, 0.0161, 0.0184, 0.0161, 0.0173, 0.0156, 0.0198, 0.0154, 0.0166,\n",
      "                  0.0157, 0.0190, 0.0166, 0.0185, 0.0173, 0.0162, 0.0174, 0.0153, 0.0151,\n",
      "                  0.0207, 0.0171, 0.0173, 0.0168, 0.0149, 0.0176, 0.0178, 0.0166, 0.0198,\n",
      "                  0.0153, 0.0182, 0.0169, 0.0148, 0.0196, 0.0159, 0.0167, 0.0163, 0.0170,\n",
      "                  0.0160, 0.0159, 0.0160, 0.0186, 0.0174, 0.0171, 0.0207, 0.0152, 0.0203,\n",
      "                  0.0159, 0.0166, 0.0188, 0.0146, 0.0149, 0.0173, 0.0145, 0.0174, 0.0166,\n",
      "                  0.0149, 0.0194, 0.0182, 0.0177, 0.0155, 0.0175, 0.0214, 0.0168, 0.0178,\n",
      "                  0.0166, 0.0185, 0.0173, 0.0163, 0.0133, 0.0179, 0.0158, 0.0143, 0.0152,\n",
      "                  0.0178, 0.0188, 0.0155, 0.0163, 0.0182, 0.0155, 0.0158, 0.0162, 0.0184,\n",
      "                  0.0176, 0.0211, 0.0168, 0.0172, 0.0151, 0.0167, 0.0160, 0.0148, 0.0167,\n",
      "                  0.0149, 0.0163, 0.0174, 0.0206, 0.0175, 0.0171, 0.0152, 0.0170, 0.0186,\n",
      "                  0.0192, 0.0178, 0.0187, 0.0174, 0.0196, 0.0185, 0.0175, 0.0148, 0.0172,\n",
      "                  0.0178, 0.0140, 0.0158, 0.0195, 0.0140, 0.0190, 0.0164, 0.0154, 0.0185,\n",
      "                  0.0211, 0.0184, 0.0147, 0.0160, 0.0167, 0.0183, 0.0178, 0.0164, 0.0179,\n",
      "                  0.0193, 0.0213, 0.0166, 0.0137, 0.0177, 0.0151, 0.0156, 0.0208, 0.0164,\n",
      "                  0.0149, 0.0187, 0.0205, 0.0183, 0.0201, 0.0183, 0.0151, 0.0157, 0.0161,\n",
      "                  0.0164, 0.0144, 0.0178, 0.0157, 0.0197, 0.0173, 0.0158, 0.0151, 0.0202,\n",
      "                  0.0181, 0.0191, 0.0166, 0.0136, 0.0186, 0.0163, 0.0146, 0.0175, 0.0164,\n",
      "                  0.0166, 0.0171, 0.0163, 0.0187, 0.0206, 0.0181, 0.0186, 0.0175, 0.0166,\n",
      "                  0.0178, 0.0175, 0.0169, 0.0175, 0.0154, 0.0198, 0.0177, 0.0179, 0.0175,\n",
      "                  0.0168, 0.0170, 0.0181, 0.0158, 0.0135, 0.0168, 0.0199, 0.0157, 0.0166,\n",
      "                  0.0156, 0.0147, 0.0190, 0.0141, 0.0182, 0.0172, 0.0148, 0.0182, 0.0152,\n",
      "                  0.0173, 0.0182, 0.0186, 0.0179, 0.0168, 0.0131, 0.0178, 0.0178, 0.0174,\n",
      "                  0.0162, 0.0162, 0.0152, 0.0167, 0.0154, 0.0204, 0.0161, 0.0152, 0.0158,\n",
      "                  0.0171, 0.0161, 0.0153, 0.0162, 0.0169, 0.0192, 0.0170, 0.0183, 0.0155,\n",
      "                  0.0164, 0.0170, 0.0165, 0.0163, 0.0151, 0.0158, 0.0188, 0.0191, 0.0174,\n",
      "                  0.0189, 0.0174, 0.0178, 0.0184, 0.0165, 0.0182, 0.0148, 0.0181, 0.0161,\n",
      "                  0.0181, 0.0154, 0.0159, 0.0170, 0.0174, 0.0193, 0.0146, 0.0160, 0.0152,\n",
      "                  0.0187, 0.0181, 0.0190, 0.0182, 0.0158, 0.0185, 0.0189, 0.0156, 0.0190,\n",
      "                  0.0188, 0.0162, 0.0168, 0.0143, 0.0174, 0.0149, 0.0195, 0.0132])\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "  )\n",
      "  (activation_post_process_23): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1147]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.887669563293457, max_val=7.678077697753906)\n",
      "  )\n",
      "  (activation_post_process_24): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0683]), zero_point=tensor([15], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0269827842712402, max_val=7.650519847869873)\n",
      "  )\n",
      "  (conv6): Module(\n",
      "    (0): ConvBn2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.0876e-04, 1.3344e-04, 1.1883e-04, 1.1235e-04, 1.2211e-04, 1.7092e-04,\n",
      "                9.8227e-05, 1.3540e-04, 1.2406e-04, 1.3245e-04, 1.4473e-04, 1.3450e-04,\n",
      "                1.5288e-04, 1.0788e-04, 1.1756e-04, 1.0646e-04, 1.2933e-04, 1.0762e-04,\n",
      "                1.2235e-04, 1.4578e-04, 1.3141e-04, 1.1802e-04, 1.3565e-04, 1.3282e-04,\n",
      "                1.3858e-04, 1.2705e-04, 1.1636e-04, 1.5348e-04, 1.3752e-04, 1.0225e-04,\n",
      "                1.1085e-04, 1.4220e-04, 1.3650e-04, 9.1352e-05, 1.5218e-04, 1.5720e-04,\n",
      "                1.1182e-04, 1.2771e-04, 1.3612e-04, 1.4852e-04, 1.2314e-04, 1.0957e-04,\n",
      "                1.1699e-04, 1.2183e-04, 1.0795e-04, 1.1562e-04, 1.3248e-04, 1.3395e-04,\n",
      "                1.1587e-04, 1.2419e-04, 1.6289e-04, 1.8491e-04, 1.1172e-04, 1.1842e-04,\n",
      "                1.2546e-04, 1.0437e-04, 1.3457e-04, 1.0680e-04, 1.2299e-04, 1.0698e-04,\n",
      "                1.4090e-04, 1.5703e-04, 1.1168e-04, 1.2984e-04, 1.8016e-04, 1.4211e-04,\n",
      "                1.3483e-04, 1.2505e-04, 1.3749e-04, 1.3955e-04, 1.5243e-04, 1.3525e-04,\n",
      "                1.2747e-04, 1.0884e-04, 1.1258e-04, 1.0267e-04, 1.1168e-04, 1.2918e-04,\n",
      "                1.2206e-04, 1.1945e-04, 1.3431e-04, 1.1140e-04, 1.2976e-04, 1.4723e-04,\n",
      "                1.3877e-04, 1.5177e-04, 1.4763e-04, 1.2215e-04, 1.5878e-04, 1.2960e-04,\n",
      "                1.1489e-04, 1.3963e-04, 1.1234e-04, 1.2976e-04, 9.7704e-05, 1.1400e-04,\n",
      "                1.3413e-04, 1.2557e-04, 8.4503e-05, 1.5165e-04, 1.2570e-04, 1.3697e-04,\n",
      "                1.2875e-04, 1.1604e-04, 1.3898e-04, 1.1628e-04, 1.4878e-04, 1.0805e-04,\n",
      "                1.1889e-04, 1.0110e-04, 1.3379e-04, 1.1074e-04, 1.6628e-04, 1.5319e-04,\n",
      "                1.2260e-04, 1.1758e-04, 1.1667e-04, 1.3714e-04, 1.3788e-04, 1.0214e-04,\n",
      "                1.3668e-04, 1.0439e-04, 1.3359e-04, 1.1227e-04, 1.2941e-04, 1.2448e-04,\n",
      "                1.2405e-04, 1.1921e-04, 1.2875e-04, 1.3048e-04, 1.2993e-04, 1.5518e-04,\n",
      "                1.2409e-04, 1.1414e-04, 1.1570e-04, 1.1845e-04, 1.4752e-04, 1.5166e-04,\n",
      "                1.0879e-04, 1.0525e-04, 1.2558e-04, 1.1850e-04, 1.2688e-04, 1.1236e-04,\n",
      "                1.5016e-04, 1.0848e-04, 1.2542e-04, 1.2360e-04, 1.3064e-04, 1.3066e-04,\n",
      "                1.2329e-04, 1.3197e-04, 1.2843e-04, 1.3363e-04, 1.1383e-04, 1.3599e-04,\n",
      "                1.1619e-04, 1.1566e-04, 1.2127e-04, 1.4614e-04, 1.0377e-04, 1.0548e-04,\n",
      "                9.8941e-05, 1.1697e-04, 1.2406e-04, 1.3850e-04, 1.1994e-04, 1.4280e-04,\n",
      "                1.7565e-04, 1.2968e-04, 1.3408e-04, 1.2807e-04, 1.0172e-04, 1.3817e-04,\n",
      "                1.1171e-04, 1.1077e-04, 1.1595e-04, 1.2772e-04, 9.8945e-05, 1.3689e-04,\n",
      "                1.4224e-04, 1.1822e-04, 1.1821e-04, 1.2897e-04, 1.1257e-04, 1.2996e-04,\n",
      "                1.1605e-04, 1.2720e-04, 9.8195e-05, 1.2631e-04, 1.2847e-04, 1.6455e-04,\n",
      "                1.2614e-04, 1.2909e-04, 1.3830e-04, 1.6305e-04, 1.0324e-04, 1.3039e-04,\n",
      "                1.2109e-04, 9.5199e-05, 1.2212e-04, 1.5207e-04, 8.8371e-05, 1.2086e-04,\n",
      "                1.3822e-04, 1.5216e-04, 1.1376e-04, 1.2743e-04, 1.3118e-04, 1.4314e-04,\n",
      "                1.3687e-04, 1.3468e-04, 1.0425e-04, 1.3434e-04, 1.3701e-04, 1.1368e-04,\n",
      "                1.3103e-04, 1.3714e-04, 1.1549e-04, 1.0789e-04, 1.3715e-04, 1.2673e-04,\n",
      "                1.5317e-04, 1.0141e-04, 1.4174e-04, 1.2789e-04, 1.3724e-04, 1.4401e-04,\n",
      "                1.7215e-04, 1.2849e-04, 1.2901e-04, 9.6545e-05, 1.1526e-04, 1.1029e-04,\n",
      "                1.2271e-04, 1.4705e-04, 1.4101e-04, 1.2429e-04, 1.3252e-04, 1.3976e-04,\n",
      "                1.4170e-04, 1.5204e-04, 1.3014e-04, 1.3608e-04, 1.5839e-04, 1.1947e-04,\n",
      "                1.7506e-04, 1.3485e-04, 1.1286e-04, 1.1214e-04, 1.3328e-04, 1.2087e-04,\n",
      "                1.0999e-04, 1.3494e-04, 1.3463e-04, 1.1877e-04, 1.4748e-04, 1.3585e-04,\n",
      "                1.4551e-04, 1.1555e-04, 1.2843e-04, 1.0339e-04, 1.4073e-04, 9.9726e-05,\n",
      "                1.3148e-04, 1.3566e-04, 1.1935e-04, 1.3280e-04, 9.6828e-05, 1.4639e-04,\n",
      "                1.3638e-04, 1.5073e-04, 1.4310e-04, 1.0844e-04, 1.2528e-04, 1.4601e-04,\n",
      "                1.1488e-04, 1.3410e-04, 1.3346e-04, 1.3062e-04, 1.1686e-04, 1.0987e-04,\n",
      "                1.1129e-04, 1.1081e-04, 9.8151e-05, 1.4721e-04, 1.3807e-04, 1.2648e-04,\n",
      "                9.9853e-05, 1.2263e-04, 9.6564e-05, 1.0795e-04, 1.0907e-04, 1.3257e-04,\n",
      "                1.1075e-04, 1.2734e-04, 1.3180e-04, 1.3704e-04, 1.6821e-04, 1.1904e-04,\n",
      "                1.0807e-04, 1.2239e-04, 1.3053e-04, 1.0476e-04, 1.2829e-04, 1.2012e-04,\n",
      "                1.3725e-04, 1.1606e-04, 1.0540e-04, 1.1703e-04, 1.1354e-04, 1.4023e-04,\n",
      "                1.3787e-04, 1.4085e-04, 1.8744e-04, 1.6463e-04, 1.4395e-04, 1.2569e-04,\n",
      "                1.3663e-04, 1.2514e-04, 1.6433e-04, 1.3451e-04, 1.2039e-04, 1.3752e-04,\n",
      "                1.0205e-04, 1.2680e-04, 1.3684e-04, 1.2090e-04, 1.1634e-04, 1.4352e-04,\n",
      "                1.5976e-04, 1.2847e-04, 1.4204e-04, 1.0378e-04, 1.2481e-04, 1.2737e-04,\n",
      "                1.2779e-04, 1.2207e-04, 1.0788e-04, 1.0774e-04, 1.1205e-04, 1.0499e-04,\n",
      "                1.3115e-04, 1.2280e-04, 1.1438e-04, 1.0244e-04, 1.3142e-04, 1.2359e-04,\n",
      "                9.1596e-05, 1.4546e-04, 1.0592e-04, 1.1458e-04, 1.5096e-04, 1.2002e-04,\n",
      "                1.1770e-04, 1.2289e-04, 1.4715e-04, 1.3400e-04, 1.2442e-04, 1.2771e-04,\n",
      "                1.1024e-04, 1.1515e-04, 1.0884e-04, 1.6065e-04, 1.1885e-04, 1.4133e-04,\n",
      "                1.3067e-04, 1.1294e-04, 1.3745e-04, 1.5553e-04, 1.0754e-04, 1.1767e-04,\n",
      "                1.2966e-04, 1.4746e-04, 1.4502e-04, 1.1550e-04, 1.3104e-04, 1.2355e-04,\n",
      "                1.1126e-04, 1.5702e-04, 1.3468e-04, 1.1746e-04, 1.1734e-04, 1.5300e-04,\n",
      "                1.2278e-04, 1.1760e-04, 1.4364e-04, 1.4495e-04, 1.2891e-04, 1.5137e-04,\n",
      "                1.3401e-04, 1.0888e-04, 9.4989e-05, 1.2629e-04, 1.1240e-04, 1.1993e-04,\n",
      "                1.1546e-04, 1.1270e-04, 1.2433e-04, 1.1938e-04, 1.2891e-04, 1.4740e-04,\n",
      "                1.2871e-04, 1.2677e-04, 1.6000e-04, 1.3989e-04, 1.0096e-04, 1.4522e-04,\n",
      "                1.1271e-04, 1.0237e-04, 1.3595e-04, 1.2823e-04, 1.2747e-04, 1.2513e-04,\n",
      "                1.1695e-04, 1.0627e-04, 1.3414e-04, 1.0450e-04, 1.4885e-04, 1.3605e-04,\n",
      "                1.3266e-04, 1.1793e-04, 1.0416e-04, 1.3315e-04, 1.4281e-04, 9.9395e-05,\n",
      "                1.4101e-04, 1.3082e-04, 1.1889e-04, 1.2815e-04, 1.4215e-04, 1.4301e-04,\n",
      "                1.4362e-04, 1.4076e-04, 1.4006e-04, 1.2697e-04, 1.1636e-04, 1.3245e-04,\n",
      "                1.2837e-04, 1.4198e-04, 1.1530e-04, 1.1241e-04, 1.4035e-04, 1.3276e-04,\n",
      "                1.5928e-04, 1.3624e-04, 1.6566e-04, 1.1997e-04, 1.1916e-04, 1.4433e-04,\n",
      "                1.1106e-04, 1.3655e-04, 1.4001e-04, 1.2527e-04, 1.1186e-04, 1.3305e-04,\n",
      "                1.0931e-04, 1.2490e-04, 1.4313e-04, 1.4478e-04, 1.2025e-04, 1.3385e-04,\n",
      "                1.3634e-04, 1.1517e-04, 1.2455e-04, 1.2927e-04, 1.0075e-04, 1.0912e-04,\n",
      "                1.5499e-04, 1.2647e-04, 1.1297e-04, 1.4004e-04, 1.2954e-04, 1.6012e-04,\n",
      "                1.0187e-04, 1.4197e-04, 1.1421e-04, 1.1812e-04, 1.2045e-04, 1.1861e-04,\n",
      "                1.3610e-04, 1.1344e-04, 1.2470e-04, 1.4215e-04, 1.3403e-04, 1.0190e-04,\n",
      "                9.2897e-05, 1.3259e-04, 1.1071e-04, 1.4333e-04, 1.4871e-04, 1.1370e-04,\n",
      "                1.3530e-04, 1.2247e-04, 1.2568e-04, 1.3561e-04, 1.5066e-04, 1.3108e-04,\n",
      "                1.2023e-04, 1.3357e-04, 1.2635e-04, 1.3289e-04, 1.4181e-04, 1.5923e-04,\n",
      "                1.3421e-04, 1.0345e-04, 1.4212e-04, 1.0605e-04, 1.3648e-04, 1.2621e-04,\n",
      "                1.1230e-04, 1.6653e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "          min_val=tensor([-0.0139, -0.0148, -0.0152, -0.0143, -0.0151, -0.0219, -0.0125, -0.0168,\n",
      "                  -0.0159, -0.0170, -0.0185, -0.0162, -0.0189, -0.0138, -0.0150, -0.0128,\n",
      "                  -0.0166, -0.0136, -0.0157, -0.0187, -0.0168, -0.0147, -0.0167, -0.0170,\n",
      "                  -0.0169, -0.0163, -0.0148, -0.0189, -0.0176, -0.0130, -0.0142, -0.0182,\n",
      "                  -0.0175, -0.0117, -0.0173, -0.0165, -0.0136, -0.0157, -0.0174, -0.0180,\n",
      "                  -0.0158, -0.0129, -0.0141, -0.0154, -0.0125, -0.0137, -0.0153, -0.0171,\n",
      "                  -0.0148, -0.0159, -0.0195, -0.0205, -0.0143, -0.0152, -0.0158, -0.0134,\n",
      "                  -0.0166, -0.0131, -0.0151, -0.0129, -0.0161, -0.0179, -0.0143, -0.0149,\n",
      "                  -0.0218, -0.0172, -0.0173, -0.0160, -0.0164, -0.0156, -0.0195, -0.0169,\n",
      "                  -0.0163, -0.0132, -0.0144, -0.0131, -0.0140, -0.0165, -0.0155, -0.0124,\n",
      "                  -0.0172, -0.0136, -0.0166, -0.0161, -0.0178, -0.0194, -0.0186, -0.0156,\n",
      "                  -0.0193, -0.0166, -0.0147, -0.0151, -0.0136, -0.0162, -0.0125, -0.0146,\n",
      "                  -0.0167, -0.0149, -0.0104, -0.0194, -0.0161, -0.0175, -0.0158, -0.0149,\n",
      "                  -0.0159, -0.0149, -0.0190, -0.0138, -0.0130, -0.0129, -0.0171, -0.0131,\n",
      "                  -0.0213, -0.0185, -0.0154, -0.0150, -0.0149, -0.0176, -0.0160, -0.0121,\n",
      "                  -0.0164, -0.0134, -0.0155, -0.0120, -0.0146, -0.0144, -0.0152, -0.0153,\n",
      "                  -0.0165, -0.0160, -0.0145, -0.0169, -0.0159, -0.0146, -0.0147, -0.0145,\n",
      "                  -0.0189, -0.0179, -0.0138, -0.0132, -0.0136, -0.0144, -0.0136, -0.0142,\n",
      "                  -0.0177, -0.0139, -0.0152, -0.0138, -0.0167, -0.0165, -0.0158, -0.0168,\n",
      "                  -0.0154, -0.0171, -0.0143, -0.0174, -0.0149, -0.0148, -0.0143, -0.0183,\n",
      "                  -0.0133, -0.0135, -0.0127, -0.0150, -0.0153, -0.0166, -0.0149, -0.0174,\n",
      "                  -0.0178, -0.0166, -0.0172, -0.0142, -0.0126, -0.0177, -0.0143, -0.0133,\n",
      "                  -0.0134, -0.0163, -0.0123, -0.0175, -0.0159, -0.0147, -0.0139, -0.0165,\n",
      "                  -0.0144, -0.0156, -0.0149, -0.0142, -0.0126, -0.0162, -0.0145, -0.0211,\n",
      "                  -0.0156, -0.0143, -0.0166, -0.0185, -0.0130, -0.0150, -0.0155, -0.0121,\n",
      "                  -0.0149, -0.0195, -0.0102, -0.0155, -0.0172, -0.0192, -0.0146, -0.0163,\n",
      "                  -0.0150, -0.0178, -0.0159, -0.0172, -0.0133, -0.0154, -0.0166, -0.0146,\n",
      "                  -0.0168, -0.0158, -0.0148, -0.0138, -0.0166, -0.0162, -0.0148, -0.0123,\n",
      "                  -0.0158, -0.0164, -0.0167, -0.0184, -0.0220, -0.0164, -0.0165, -0.0123,\n",
      "                  -0.0138, -0.0135, -0.0157, -0.0173, -0.0180, -0.0159, -0.0170, -0.0154,\n",
      "                  -0.0158, -0.0188, -0.0157, -0.0173, -0.0190, -0.0153, -0.0224, -0.0155,\n",
      "                  -0.0144, -0.0144, -0.0151, -0.0146, -0.0130, -0.0163, -0.0170, -0.0152,\n",
      "                  -0.0189, -0.0162, -0.0183, -0.0137, -0.0157, -0.0125, -0.0163, -0.0128,\n",
      "                  -0.0159, -0.0153, -0.0148, -0.0146, -0.0124, -0.0162, -0.0169, -0.0191,\n",
      "                  -0.0165, -0.0139, -0.0141, -0.0145, -0.0134, -0.0172, -0.0143, -0.0162,\n",
      "                  -0.0150, -0.0141, -0.0142, -0.0142, -0.0121, -0.0175, -0.0172, -0.0142,\n",
      "                  -0.0119, -0.0157, -0.0121, -0.0136, -0.0138, -0.0170, -0.0132, -0.0155,\n",
      "                  -0.0169, -0.0175, -0.0186, -0.0144, -0.0138, -0.0148, -0.0167, -0.0132,\n",
      "                  -0.0140, -0.0153, -0.0174, -0.0149, -0.0135, -0.0150, -0.0139, -0.0179,\n",
      "                  -0.0163, -0.0166, -0.0214, -0.0205, -0.0164, -0.0151, -0.0175, -0.0153,\n",
      "                  -0.0165, -0.0164, -0.0153, -0.0159, -0.0131, -0.0162, -0.0172, -0.0137,\n",
      "                  -0.0149, -0.0184, -0.0198, -0.0164, -0.0176, -0.0126, -0.0160, -0.0163,\n",
      "                  -0.0162, -0.0156, -0.0138, -0.0130, -0.0143, -0.0134, -0.0153, -0.0157,\n",
      "                  -0.0146, -0.0130, -0.0158, -0.0158, -0.0114, -0.0181, -0.0136, -0.0132,\n",
      "                  -0.0173, -0.0151, -0.0151, -0.0135, -0.0188, -0.0172, -0.0150, -0.0144,\n",
      "                  -0.0141, -0.0147, -0.0139, -0.0199, -0.0138, -0.0181, -0.0156, -0.0143,\n",
      "                  -0.0176, -0.0199, -0.0137, -0.0148, -0.0166, -0.0165, -0.0186, -0.0144,\n",
      "                  -0.0168, -0.0157, -0.0141, -0.0171, -0.0172, -0.0150, -0.0150, -0.0166,\n",
      "                  -0.0157, -0.0151, -0.0158, -0.0167, -0.0165, -0.0194, -0.0172, -0.0139,\n",
      "                  -0.0122, -0.0159, -0.0144, -0.0152, -0.0139, -0.0139, -0.0159, -0.0153,\n",
      "                  -0.0165, -0.0182, -0.0165, -0.0135, -0.0178, -0.0144, -0.0124, -0.0159,\n",
      "                  -0.0132, -0.0118, -0.0169, -0.0162, -0.0155, -0.0160, -0.0150, -0.0122,\n",
      "                  -0.0172, -0.0129, -0.0163, -0.0162, -0.0170, -0.0147, -0.0133, -0.0149,\n",
      "                  -0.0165, -0.0125, -0.0180, -0.0153, -0.0138, -0.0152, -0.0166, -0.0169,\n",
      "                  -0.0184, -0.0161, -0.0169, -0.0163, -0.0131, -0.0146, -0.0156, -0.0170,\n",
      "                  -0.0141, -0.0128, -0.0160, -0.0162, -0.0204, -0.0174, -0.0188, -0.0147,\n",
      "                  -0.0149, -0.0185, -0.0137, -0.0150, -0.0179, -0.0143, -0.0141, -0.0161,\n",
      "                  -0.0124, -0.0152, -0.0179, -0.0175, -0.0150, -0.0171, -0.0175, -0.0147,\n",
      "                  -0.0159, -0.0140, -0.0126, -0.0140, -0.0198, -0.0162, -0.0145, -0.0150,\n",
      "                  -0.0166, -0.0190, -0.0120, -0.0149, -0.0139, -0.0136, -0.0154, -0.0137,\n",
      "                  -0.0168, -0.0137, -0.0145, -0.0182, -0.0152, -0.0128, -0.0119, -0.0151,\n",
      "                  -0.0141, -0.0166, -0.0184, -0.0139, -0.0173, -0.0157, -0.0144, -0.0151,\n",
      "                  -0.0193, -0.0162, -0.0154, -0.0146, -0.0140, -0.0170, -0.0182, -0.0204,\n",
      "                  -0.0172, -0.0126, -0.0151, -0.0136, -0.0175, -0.0162, -0.0136, -0.0213]), max_val=tensor([0.0137, 0.0169, 0.0147, 0.0143, 0.0155, 0.0213, 0.0125, 0.0172, 0.0148,\n",
      "                  0.0163, 0.0158, 0.0171, 0.0194, 0.0120, 0.0142, 0.0135, 0.0150, 0.0137,\n",
      "                  0.0153, 0.0172, 0.0161, 0.0150, 0.0172, 0.0165, 0.0176, 0.0142, 0.0148,\n",
      "                  0.0195, 0.0149, 0.0130, 0.0132, 0.0170, 0.0170, 0.0109, 0.0193, 0.0200,\n",
      "                  0.0142, 0.0162, 0.0165, 0.0189, 0.0148, 0.0139, 0.0149, 0.0155, 0.0137,\n",
      "                  0.0147, 0.0168, 0.0148, 0.0145, 0.0157, 0.0207, 0.0235, 0.0141, 0.0147,\n",
      "                  0.0159, 0.0130, 0.0171, 0.0136, 0.0156, 0.0136, 0.0179, 0.0199, 0.0130,\n",
      "                  0.0165, 0.0229, 0.0180, 0.0167, 0.0154, 0.0175, 0.0177, 0.0179, 0.0172,\n",
      "                  0.0162, 0.0138, 0.0135, 0.0129, 0.0142, 0.0158, 0.0155, 0.0152, 0.0155,\n",
      "                  0.0141, 0.0156, 0.0187, 0.0168, 0.0188, 0.0187, 0.0140, 0.0202, 0.0164,\n",
      "                  0.0145, 0.0177, 0.0143, 0.0165, 0.0123, 0.0143, 0.0170, 0.0159, 0.0107,\n",
      "                  0.0170, 0.0155, 0.0167, 0.0164, 0.0144, 0.0177, 0.0144, 0.0188, 0.0137,\n",
      "                  0.0151, 0.0122, 0.0166, 0.0141, 0.0174, 0.0195, 0.0156, 0.0149, 0.0142,\n",
      "                  0.0168, 0.0175, 0.0130, 0.0174, 0.0131, 0.0170, 0.0143, 0.0164, 0.0158,\n",
      "                  0.0158, 0.0144, 0.0163, 0.0166, 0.0165, 0.0197, 0.0153, 0.0144, 0.0147,\n",
      "                  0.0150, 0.0187, 0.0193, 0.0138, 0.0134, 0.0159, 0.0151, 0.0161, 0.0143,\n",
      "                  0.0191, 0.0134, 0.0159, 0.0157, 0.0162, 0.0166, 0.0155, 0.0168, 0.0163,\n",
      "                  0.0160, 0.0145, 0.0170, 0.0134, 0.0145, 0.0154, 0.0186, 0.0127, 0.0131,\n",
      "                  0.0123, 0.0140, 0.0158, 0.0176, 0.0152, 0.0181, 0.0223, 0.0151, 0.0155,\n",
      "                  0.0163, 0.0129, 0.0172, 0.0137, 0.0141, 0.0147, 0.0137, 0.0126, 0.0168,\n",
      "                  0.0181, 0.0150, 0.0150, 0.0148, 0.0142, 0.0165, 0.0133, 0.0162, 0.0121,\n",
      "                  0.0150, 0.0163, 0.0187, 0.0160, 0.0164, 0.0176, 0.0207, 0.0131, 0.0166,\n",
      "                  0.0146, 0.0121, 0.0155, 0.0177, 0.0112, 0.0153, 0.0176, 0.0193, 0.0143,\n",
      "                  0.0137, 0.0167, 0.0182, 0.0174, 0.0168, 0.0120, 0.0171, 0.0174, 0.0139,\n",
      "                  0.0162, 0.0174, 0.0145, 0.0125, 0.0174, 0.0156, 0.0195, 0.0129, 0.0180,\n",
      "                  0.0154, 0.0174, 0.0156, 0.0207, 0.0146, 0.0157, 0.0123, 0.0146, 0.0140,\n",
      "                  0.0149, 0.0187, 0.0170, 0.0154, 0.0168, 0.0177, 0.0180, 0.0193, 0.0165,\n",
      "                  0.0173, 0.0201, 0.0146, 0.0200, 0.0171, 0.0133, 0.0136, 0.0169, 0.0154,\n",
      "                  0.0140, 0.0171, 0.0171, 0.0146, 0.0166, 0.0173, 0.0185, 0.0147, 0.0163,\n",
      "                  0.0131, 0.0179, 0.0126, 0.0167, 0.0172, 0.0152, 0.0169, 0.0113, 0.0186,\n",
      "                  0.0173, 0.0191, 0.0182, 0.0132, 0.0159, 0.0185, 0.0146, 0.0170, 0.0169,\n",
      "                  0.0166, 0.0140, 0.0139, 0.0135, 0.0133, 0.0125, 0.0187, 0.0175, 0.0161,\n",
      "                  0.0127, 0.0156, 0.0123, 0.0137, 0.0139, 0.0160, 0.0141, 0.0162, 0.0151,\n",
      "                  0.0169, 0.0214, 0.0151, 0.0137, 0.0155, 0.0161, 0.0133, 0.0163, 0.0153,\n",
      "                  0.0174, 0.0134, 0.0122, 0.0149, 0.0144, 0.0177, 0.0175, 0.0179, 0.0238,\n",
      "                  0.0209, 0.0183, 0.0160, 0.0161, 0.0159, 0.0209, 0.0171, 0.0153, 0.0175,\n",
      "                  0.0127, 0.0160, 0.0174, 0.0154, 0.0147, 0.0171, 0.0203, 0.0146, 0.0180,\n",
      "                  0.0132, 0.0155, 0.0151, 0.0162, 0.0155, 0.0137, 0.0137, 0.0138, 0.0133,\n",
      "                  0.0167, 0.0143, 0.0142, 0.0130, 0.0167, 0.0157, 0.0116, 0.0185, 0.0135,\n",
      "                  0.0146, 0.0192, 0.0152, 0.0139, 0.0156, 0.0163, 0.0169, 0.0158, 0.0162,\n",
      "                  0.0133, 0.0141, 0.0137, 0.0204, 0.0151, 0.0175, 0.0166, 0.0143, 0.0167,\n",
      "                  0.0192, 0.0137, 0.0149, 0.0145, 0.0187, 0.0184, 0.0147, 0.0159, 0.0157,\n",
      "                  0.0141, 0.0199, 0.0164, 0.0139, 0.0136, 0.0194, 0.0140, 0.0139, 0.0182,\n",
      "                  0.0184, 0.0152, 0.0180, 0.0166, 0.0130, 0.0117, 0.0160, 0.0139, 0.0152,\n",
      "                  0.0147, 0.0143, 0.0156, 0.0147, 0.0163, 0.0187, 0.0158, 0.0161, 0.0203,\n",
      "                  0.0178, 0.0128, 0.0184, 0.0143, 0.0130, 0.0173, 0.0163, 0.0162, 0.0142,\n",
      "                  0.0143, 0.0135, 0.0166, 0.0133, 0.0189, 0.0173, 0.0157, 0.0150, 0.0125,\n",
      "                  0.0169, 0.0181, 0.0126, 0.0156, 0.0166, 0.0151, 0.0163, 0.0181, 0.0182,\n",
      "                  0.0174, 0.0179, 0.0178, 0.0148, 0.0148, 0.0168, 0.0163, 0.0180, 0.0146,\n",
      "                  0.0143, 0.0178, 0.0169, 0.0194, 0.0172, 0.0210, 0.0152, 0.0151, 0.0180,\n",
      "                  0.0141, 0.0173, 0.0163, 0.0159, 0.0142, 0.0169, 0.0139, 0.0159, 0.0182,\n",
      "                  0.0184, 0.0153, 0.0164, 0.0165, 0.0146, 0.0153, 0.0164, 0.0128, 0.0117,\n",
      "                  0.0185, 0.0158, 0.0140, 0.0178, 0.0164, 0.0203, 0.0129, 0.0180, 0.0145,\n",
      "                  0.0150, 0.0153, 0.0151, 0.0173, 0.0144, 0.0158, 0.0148, 0.0170, 0.0129,\n",
      "                  0.0118, 0.0168, 0.0141, 0.0182, 0.0189, 0.0144, 0.0158, 0.0142, 0.0160,\n",
      "                  0.0172, 0.0179, 0.0166, 0.0152, 0.0170, 0.0160, 0.0147, 0.0177, 0.0180,\n",
      "                  0.0165, 0.0131, 0.0180, 0.0131, 0.0160, 0.0157, 0.0143, 0.0163])\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (activation_post_process_25): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1032]), zero_point=tensor([58], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.982511043548584, max_val=7.122013092041016)\n",
      "  )\n",
      "  (activation_post_process_26): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0639]), zero_point=tensor([16], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0204319953918457, max_val=7.096462249755859)\n",
      "  )\n",
      "  (activation_post_process_27): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0639]), zero_point=tensor([16], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0204319953918457, max_val=7.096462249755859)\n",
      "  )\n",
      "  (res3): Module(\n",
      "    (0): Module(\n",
      "      (0): ConvBn2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.7605e-04, 1.4268e-04, 1.5655e-04, 1.2589e-04, 1.4066e-04, 1.1591e-04,\n",
      "                  1.4197e-04, 1.3828e-04, 1.3286e-04, 1.3690e-04, 1.4115e-04, 1.7977e-04,\n",
      "                  1.1682e-04, 1.3406e-04, 1.1768e-04, 1.5350e-04, 1.6507e-04, 1.3106e-04,\n",
      "                  1.5037e-04, 1.3680e-04, 1.5359e-04, 1.1636e-04, 1.5187e-04, 1.4154e-04,\n",
      "                  1.1449e-04, 1.4813e-04, 1.3658e-04, 1.6228e-04, 1.4573e-04, 1.5184e-04,\n",
      "                  1.8708e-04, 1.2878e-04, 1.0961e-04, 1.5213e-04, 1.1885e-04, 1.4027e-04,\n",
      "                  1.9514e-04, 1.5368e-04, 1.2250e-04, 1.3715e-04, 1.6968e-04, 1.1022e-04,\n",
      "                  9.0101e-05, 1.1436e-04, 1.4037e-04, 1.3876e-04, 1.7433e-04, 1.2224e-04,\n",
      "                  1.5377e-04, 1.4783e-04, 1.2644e-04, 1.4645e-04, 1.4405e-04, 1.6198e-04,\n",
      "                  1.2924e-04, 1.6962e-04, 1.6150e-04, 1.4646e-04, 1.4676e-04, 1.2518e-04,\n",
      "                  1.6942e-04, 1.2923e-04, 1.4091e-04, 1.1363e-04, 1.4305e-04, 1.7198e-04,\n",
      "                  1.1662e-04, 1.0689e-04, 1.4329e-04, 1.3743e-04, 1.3643e-04, 1.5567e-04,\n",
      "                  1.1583e-04, 1.3393e-04, 1.2097e-04, 1.2403e-04, 1.2488e-04, 1.2221e-04,\n",
      "                  1.7090e-04, 1.5681e-04, 1.2876e-04, 1.3866e-04, 1.1009e-04, 1.3452e-04,\n",
      "                  1.3618e-04, 1.5459e-04, 1.2879e-04, 1.3001e-04, 1.3809e-04, 1.1873e-04,\n",
      "                  1.4698e-04, 1.4939e-04, 1.2450e-04, 1.2528e-04, 1.1023e-04, 1.5928e-04,\n",
      "                  1.3449e-04, 2.1385e-04, 1.2882e-04, 1.0557e-04, 1.8161e-04, 1.4426e-04,\n",
      "                  1.3649e-04, 1.1182e-04, 1.4556e-04, 1.6627e-04, 1.3428e-04, 1.4674e-04,\n",
      "                  1.3330e-04, 1.8099e-04, 1.1656e-04, 1.2096e-04, 1.4391e-04, 1.2000e-04,\n",
      "                  1.2860e-04, 1.6114e-04, 1.1419e-04, 1.2593e-04, 1.5369e-04, 9.7522e-05,\n",
      "                  1.3492e-04, 1.5526e-04, 2.0645e-04, 1.1122e-04, 1.4342e-04, 1.3492e-04,\n",
      "                  1.1031e-04, 1.2008e-04, 1.3749e-04, 1.5327e-04, 2.3481e-04, 1.4576e-04,\n",
      "                  1.1811e-04, 1.1460e-04, 1.3287e-04, 1.2590e-04, 1.6714e-04, 2.3047e-04,\n",
      "                  1.4575e-04, 1.4707e-04, 1.2270e-04, 1.3642e-04, 1.3183e-04, 1.6124e-04,\n",
      "                  1.5765e-04, 1.2451e-04, 1.7646e-04, 1.7392e-04, 1.3951e-04, 9.8598e-05,\n",
      "                  1.4183e-04, 1.3149e-04, 1.1502e-04, 1.6405e-04, 1.3148e-04, 1.4788e-04,\n",
      "                  1.5061e-04, 1.5210e-04, 1.1636e-04, 1.1754e-04, 1.6342e-04, 1.3575e-04,\n",
      "                  1.1389e-04, 1.2058e-04, 1.3931e-04, 1.3730e-04, 1.6712e-04, 1.1791e-04,\n",
      "                  1.4897e-04, 1.3330e-04, 1.2351e-04, 1.0022e-04, 1.1128e-04, 1.3864e-04,\n",
      "                  1.5838e-04, 1.3069e-04, 1.2356e-04, 1.3209e-04, 1.6472e-04, 1.3356e-04,\n",
      "                  1.9332e-04, 1.4515e-04, 1.4108e-04, 1.3920e-04, 1.6863e-04, 1.2397e-04,\n",
      "                  1.4206e-04, 1.4362e-04, 1.3006e-04, 1.5844e-04, 1.5698e-04, 1.6112e-04,\n",
      "                  1.3849e-04, 8.9678e-05, 1.4264e-04, 1.4789e-04, 1.2102e-04, 1.6164e-04,\n",
      "                  1.3115e-04, 1.2423e-04, 1.6073e-04, 1.1016e-04, 1.3962e-04, 1.3132e-04,\n",
      "                  1.7611e-04, 1.1880e-04, 1.3676e-04, 1.7730e-04, 1.0262e-04, 1.5048e-04,\n",
      "                  1.1285e-04, 9.8705e-05, 1.1985e-04, 1.1414e-04, 1.1997e-04, 1.0826e-04,\n",
      "                  1.2262e-04, 1.8303e-04, 1.1283e-04, 1.1347e-04, 1.1257e-04, 1.3492e-04,\n",
      "                  1.5367e-04, 1.3398e-04, 1.2472e-04, 1.5182e-04, 1.6910e-04, 1.4515e-04,\n",
      "                  1.4248e-04, 1.6379e-04, 1.3359e-04, 1.5461e-04, 1.4936e-04, 1.2664e-04,\n",
      "                  1.6564e-04, 1.0788e-04, 1.4218e-04, 1.1201e-04, 1.6132e-04, 1.2426e-04,\n",
      "                  1.2633e-04, 1.2948e-04, 1.7352e-04, 1.3174e-04, 1.5306e-04, 1.6686e-04,\n",
      "                  1.0788e-04, 1.5896e-04, 1.6369e-04, 1.4827e-04, 1.7616e-04, 1.3202e-04,\n",
      "                  1.5220e-04, 1.4606e-04, 1.8445e-04, 1.5005e-04, 1.5725e-04, 1.3968e-04,\n",
      "                  1.0213e-04, 1.2992e-04, 9.5745e-05, 1.1953e-04, 1.0710e-04, 1.4446e-04,\n",
      "                  1.4771e-04, 1.1695e-04, 1.0327e-04, 1.4591e-04, 1.1274e-04, 1.3013e-04,\n",
      "                  1.4993e-04, 1.4827e-04, 1.2641e-04, 1.6081e-04, 1.3361e-04, 1.1620e-04,\n",
      "                  1.0357e-04, 1.4429e-04, 1.4363e-04, 1.5418e-04, 1.2248e-04, 1.2013e-04,\n",
      "                  1.5918e-04, 1.3031e-04, 1.4518e-04, 1.3435e-04, 1.2827e-04, 1.5761e-04,\n",
      "                  1.5439e-04, 1.6376e-04, 1.1823e-04, 1.2573e-04, 1.5770e-04, 1.5048e-04,\n",
      "                  1.2735e-04, 1.4383e-04, 1.5201e-04, 1.2438e-04, 1.5337e-04, 1.4786e-04,\n",
      "                  1.1700e-04, 1.7658e-04, 1.2086e-04, 1.2482e-04, 1.3083e-04, 1.4275e-04,\n",
      "                  1.2165e-04, 1.3215e-04, 1.5477e-04, 1.5236e-04, 1.0268e-04, 1.3247e-04,\n",
      "                  1.4574e-04, 1.6296e-04, 1.3575e-04, 1.6351e-04, 1.2183e-04, 1.3407e-04,\n",
      "                  1.3607e-04, 1.5280e-04, 1.3723e-04, 1.1860e-04, 1.4923e-04, 1.2074e-04,\n",
      "                  1.3695e-04, 1.2305e-04, 1.5269e-04, 1.6052e-04, 1.2407e-04, 1.4120e-04,\n",
      "                  1.2520e-04, 1.8851e-04, 1.2691e-04, 1.0866e-04, 1.3707e-04, 1.1238e-04,\n",
      "                  1.6749e-04, 1.0896e-04, 1.4660e-04, 1.2934e-04, 1.6515e-04, 1.2680e-04,\n",
      "                  1.6754e-04, 1.2506e-04, 1.3966e-04, 1.2354e-04, 1.3970e-04, 1.1221e-04,\n",
      "                  1.1387e-04, 1.1506e-04, 1.3745e-04, 1.2180e-04, 1.3434e-04, 1.2069e-04,\n",
      "                  1.1634e-04, 1.1808e-04, 1.4019e-04, 1.5872e-04, 1.2608e-04, 1.3472e-04,\n",
      "                  1.2634e-04, 1.6629e-04, 1.5039e-04, 1.2247e-04, 1.4925e-04, 1.2684e-04,\n",
      "                  1.3037e-04, 1.3585e-04, 1.2422e-04, 1.3136e-04, 1.2660e-04, 1.3614e-04,\n",
      "                  1.4957e-04, 1.9011e-04, 1.9602e-04, 1.6711e-04, 1.2275e-04, 1.3741e-04,\n",
      "                  1.2415e-04, 1.4440e-04, 1.1836e-04, 1.7563e-04, 1.8569e-04, 1.3266e-04,\n",
      "                  1.3567e-04, 1.4295e-04, 1.4371e-04, 1.3787e-04, 1.4768e-04, 2.0644e-04,\n",
      "                  1.1631e-04, 1.3064e-04, 1.3773e-04, 1.5136e-04, 1.7885e-04, 1.4654e-04,\n",
      "                  1.2870e-04, 1.3958e-04, 1.3466e-04, 1.5415e-04, 1.0870e-04, 1.4508e-04,\n",
      "                  1.6542e-04, 8.7126e-05, 1.2836e-04, 1.7886e-04, 1.3428e-04, 1.4578e-04,\n",
      "                  1.3317e-04, 1.0490e-04, 1.1610e-04, 1.1672e-04, 1.4265e-04, 1.6305e-04,\n",
      "                  1.3410e-04, 1.3860e-04, 1.1007e-04, 1.2574e-04, 1.0415e-04, 1.3548e-04,\n",
      "                  1.5665e-04, 1.4042e-04, 1.4082e-04, 1.2601e-04, 1.5368e-04, 1.9291e-04,\n",
      "                  1.2959e-04, 1.4794e-04, 1.4519e-04, 1.2265e-04, 1.4590e-04, 1.2263e-04,\n",
      "                  1.3975e-04, 1.5194e-04, 1.1395e-04, 1.7754e-04, 1.1831e-04, 1.2458e-04,\n",
      "                  1.1295e-04, 1.0971e-04, 1.3916e-04, 1.3810e-04, 1.2522e-04, 2.0734e-04,\n",
      "                  1.2978e-04, 1.5554e-04, 1.7855e-04, 1.5125e-04, 1.4901e-04, 1.3795e-04,\n",
      "                  1.1251e-04, 1.0991e-04, 1.8208e-04, 1.7453e-04, 1.6796e-04, 1.3447e-04,\n",
      "                  1.5093e-04, 2.0362e-04, 1.2092e-04, 1.1503e-04, 1.6901e-04, 1.2263e-04,\n",
      "                  1.5131e-04, 1.3236e-04, 1.1516e-04, 1.4426e-04, 1.4079e-04, 1.3077e-04,\n",
      "                  1.5629e-04, 1.2985e-04, 1.1975e-04, 1.5618e-04, 1.5821e-04, 1.7768e-04,\n",
      "                  1.4586e-04, 1.3102e-04, 1.2728e-04, 1.2455e-04, 9.7476e-05, 1.4658e-04,\n",
      "                  1.7277e-04, 1.4941e-04, 1.2893e-04, 1.6069e-04, 1.5090e-04, 1.5771e-04,\n",
      "                  1.4012e-04, 1.4005e-04, 1.5067e-04, 1.4849e-04, 1.2820e-04, 1.2495e-04,\n",
      "                  1.4279e-04, 9.8624e-05, 1.1134e-04, 1.4691e-04, 1.5414e-04, 1.2595e-04,\n",
      "                  1.9724e-04, 1.2266e-04, 1.1006e-04, 1.0135e-04, 1.4925e-04, 1.1786e-04,\n",
      "                  1.0668e-04, 1.3892e-04, 1.3490e-04, 1.7291e-04, 1.4072e-04, 1.5754e-04,\n",
      "                  1.8043e-04, 1.2496e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "            min_val=tensor([-0.0225, -0.0158, -0.0197, -0.0155, -0.0170, -0.0128, -0.0157, -0.0177,\n",
      "                    -0.0147, -0.0175, -0.0181, -0.0230, -0.0143, -0.0141, -0.0146, -0.0192,\n",
      "                    -0.0192, -0.0168, -0.0192, -0.0175, -0.0197, -0.0134, -0.0188, -0.0178,\n",
      "                    -0.0139, -0.0190, -0.0173, -0.0200, -0.0184, -0.0194, -0.0239, -0.0165,\n",
      "                    -0.0120, -0.0195, -0.0141, -0.0172, -0.0233, -0.0197, -0.0157, -0.0176,\n",
      "                    -0.0216, -0.0141, -0.0115, -0.0146, -0.0180, -0.0178, -0.0216, -0.0156,\n",
      "                    -0.0180, -0.0159, -0.0158, -0.0185, -0.0174, -0.0203, -0.0158, -0.0215,\n",
      "                    -0.0207, -0.0187, -0.0188, -0.0160, -0.0208, -0.0142, -0.0180, -0.0145,\n",
      "                    -0.0137, -0.0214, -0.0149, -0.0132, -0.0168, -0.0163, -0.0162, -0.0198,\n",
      "                    -0.0148, -0.0158, -0.0155, -0.0159, -0.0152, -0.0143, -0.0209, -0.0200,\n",
      "                    -0.0165, -0.0173, -0.0140, -0.0172, -0.0169, -0.0184, -0.0165, -0.0166,\n",
      "                    -0.0160, -0.0152, -0.0188, -0.0179, -0.0147, -0.0156, -0.0139, -0.0188,\n",
      "                    -0.0172, -0.0248, -0.0156, -0.0127, -0.0199, -0.0175, -0.0175, -0.0132,\n",
      "                    -0.0161, -0.0203, -0.0165, -0.0183, -0.0143, -0.0230, -0.0149, -0.0149,\n",
      "                    -0.0159, -0.0154, -0.0160, -0.0197, -0.0136, -0.0161, -0.0188, -0.0107,\n",
      "                    -0.0173, -0.0185, -0.0264, -0.0137, -0.0182, -0.0173, -0.0128, -0.0153,\n",
      "                    -0.0160, -0.0189, -0.0257, -0.0172, -0.0151, -0.0137, -0.0169, -0.0161,\n",
      "                    -0.0184, -0.0295, -0.0174, -0.0188, -0.0137, -0.0175, -0.0157, -0.0171,\n",
      "                    -0.0189, -0.0159, -0.0183, -0.0200, -0.0155, -0.0126, -0.0164, -0.0168,\n",
      "                    -0.0147, -0.0206, -0.0168, -0.0176, -0.0178, -0.0195, -0.0149, -0.0150,\n",
      "                    -0.0209, -0.0164, -0.0146, -0.0154, -0.0155, -0.0176, -0.0201, -0.0151,\n",
      "                    -0.0185, -0.0165, -0.0150, -0.0117, -0.0128, -0.0169, -0.0195, -0.0158,\n",
      "                    -0.0158, -0.0159, -0.0211, -0.0171, -0.0229, -0.0182, -0.0167, -0.0178,\n",
      "                    -0.0216, -0.0146, -0.0163, -0.0184, -0.0147, -0.0203, -0.0201, -0.0206,\n",
      "                    -0.0177, -0.0115, -0.0166, -0.0189, -0.0148, -0.0207, -0.0168, -0.0159,\n",
      "                    -0.0206, -0.0141, -0.0179, -0.0158, -0.0215, -0.0152, -0.0175, -0.0227,\n",
      "                    -0.0127, -0.0193, -0.0141, -0.0117, -0.0153, -0.0146, -0.0154, -0.0139,\n",
      "                    -0.0157, -0.0234, -0.0144, -0.0139, -0.0134, -0.0173, -0.0174, -0.0171,\n",
      "                    -0.0147, -0.0194, -0.0201, -0.0184, -0.0182, -0.0192, -0.0171, -0.0198,\n",
      "                    -0.0191, -0.0162, -0.0212, -0.0138, -0.0182, -0.0143, -0.0206, -0.0144,\n",
      "                    -0.0162, -0.0143, -0.0222, -0.0160, -0.0168, -0.0205, -0.0125, -0.0182,\n",
      "                    -0.0210, -0.0179, -0.0211, -0.0169, -0.0188, -0.0171, -0.0223, -0.0178,\n",
      "                    -0.0201, -0.0179, -0.0123, -0.0166, -0.0123, -0.0132, -0.0126, -0.0166,\n",
      "                    -0.0189, -0.0143, -0.0126, -0.0187, -0.0135, -0.0167, -0.0192, -0.0183,\n",
      "                    -0.0162, -0.0183, -0.0171, -0.0145, -0.0133, -0.0183, -0.0175, -0.0190,\n",
      "                    -0.0153, -0.0154, -0.0195, -0.0155, -0.0186, -0.0163, -0.0164, -0.0187,\n",
      "                    -0.0178, -0.0210, -0.0145, -0.0161, -0.0202, -0.0180, -0.0161, -0.0184,\n",
      "                    -0.0194, -0.0159, -0.0196, -0.0189, -0.0150, -0.0226, -0.0155, -0.0148,\n",
      "                    -0.0167, -0.0175, -0.0156, -0.0169, -0.0189, -0.0195, -0.0131, -0.0170,\n",
      "                    -0.0187, -0.0195, -0.0174, -0.0195, -0.0145, -0.0157, -0.0173, -0.0193,\n",
      "                    -0.0164, -0.0149, -0.0171, -0.0155, -0.0175, -0.0158, -0.0190, -0.0205,\n",
      "                    -0.0159, -0.0164, -0.0160, -0.0230, -0.0162, -0.0137, -0.0167, -0.0140,\n",
      "                    -0.0196, -0.0139, -0.0178, -0.0166, -0.0197, -0.0154, -0.0198, -0.0157,\n",
      "                    -0.0179, -0.0148, -0.0179, -0.0144, -0.0127, -0.0141, -0.0160, -0.0156,\n",
      "                    -0.0172, -0.0154, -0.0138, -0.0138, -0.0179, -0.0203, -0.0149, -0.0166,\n",
      "                    -0.0161, -0.0212, -0.0186, -0.0157, -0.0191, -0.0147, -0.0167, -0.0174,\n",
      "                    -0.0143, -0.0165, -0.0155, -0.0174, -0.0180, -0.0243, -0.0242, -0.0186,\n",
      "                    -0.0156, -0.0176, -0.0135, -0.0183, -0.0145, -0.0190, -0.0186, -0.0170,\n",
      "                    -0.0166, -0.0183, -0.0184, -0.0171, -0.0182, -0.0201, -0.0149, -0.0167,\n",
      "                    -0.0176, -0.0194, -0.0229, -0.0188, -0.0165, -0.0179, -0.0172, -0.0177,\n",
      "                    -0.0137, -0.0178, -0.0175, -0.0109, -0.0164, -0.0229, -0.0151, -0.0187,\n",
      "                    -0.0170, -0.0134, -0.0143, -0.0149, -0.0170, -0.0209, -0.0150, -0.0173,\n",
      "                    -0.0138, -0.0161, -0.0124, -0.0164, -0.0201, -0.0168, -0.0180, -0.0143,\n",
      "                    -0.0181, -0.0209, -0.0144, -0.0189, -0.0171, -0.0150, -0.0187, -0.0145,\n",
      "                    -0.0161, -0.0190, -0.0146, -0.0227, -0.0151, -0.0152, -0.0145, -0.0140,\n",
      "                    -0.0168, -0.0163, -0.0158, -0.0264, -0.0156, -0.0199, -0.0213, -0.0178,\n",
      "                    -0.0178, -0.0160, -0.0144, -0.0141, -0.0204, -0.0195, -0.0215, -0.0163,\n",
      "                    -0.0193, -0.0242, -0.0150, -0.0147, -0.0216, -0.0157, -0.0186, -0.0169,\n",
      "                    -0.0141, -0.0168, -0.0180, -0.0140, -0.0200, -0.0154, -0.0127, -0.0184,\n",
      "                    -0.0203, -0.0227, -0.0179, -0.0162, -0.0156, -0.0154, -0.0124, -0.0179,\n",
      "                    -0.0221, -0.0187, -0.0163, -0.0183, -0.0188, -0.0189, -0.0141, -0.0172,\n",
      "                    -0.0178, -0.0163, -0.0164, -0.0160, -0.0180, -0.0123, -0.0140, -0.0188,\n",
      "                    -0.0169, -0.0161, -0.0225, -0.0149, -0.0141, -0.0130, -0.0180, -0.0142,\n",
      "                    -0.0137, -0.0161, -0.0173, -0.0221, -0.0176, -0.0202, -0.0226, -0.0160]), max_val=tensor([0.0224, 0.0181, 0.0199, 0.0160, 0.0179, 0.0147, 0.0180, 0.0168, 0.0169,\n",
      "                    0.0172, 0.0177, 0.0211, 0.0148, 0.0170, 0.0149, 0.0195, 0.0210, 0.0162,\n",
      "                    0.0171, 0.0156, 0.0195, 0.0148, 0.0193, 0.0180, 0.0145, 0.0188, 0.0173,\n",
      "                    0.0206, 0.0185, 0.0186, 0.0216, 0.0149, 0.0139, 0.0177, 0.0151, 0.0178,\n",
      "                    0.0248, 0.0189, 0.0151, 0.0161, 0.0215, 0.0140, 0.0102, 0.0133, 0.0178,\n",
      "                    0.0169, 0.0221, 0.0146, 0.0195, 0.0188, 0.0161, 0.0186, 0.0183, 0.0206,\n",
      "                    0.0164, 0.0215, 0.0199, 0.0175, 0.0176, 0.0154, 0.0215, 0.0164, 0.0170,\n",
      "                    0.0139, 0.0182, 0.0218, 0.0148, 0.0136, 0.0182, 0.0175, 0.0173, 0.0198,\n",
      "                    0.0138, 0.0170, 0.0146, 0.0129, 0.0159, 0.0155, 0.0217, 0.0199, 0.0159,\n",
      "                    0.0176, 0.0140, 0.0170, 0.0173, 0.0196, 0.0149, 0.0159, 0.0175, 0.0131,\n",
      "                    0.0185, 0.0190, 0.0158, 0.0159, 0.0140, 0.0202, 0.0161, 0.0272, 0.0164,\n",
      "                    0.0134, 0.0231, 0.0183, 0.0143, 0.0142, 0.0185, 0.0211, 0.0171, 0.0186,\n",
      "                    0.0169, 0.0230, 0.0144, 0.0154, 0.0183, 0.0151, 0.0163, 0.0205, 0.0145,\n",
      "                    0.0153, 0.0195, 0.0124, 0.0171, 0.0197, 0.0247, 0.0141, 0.0182, 0.0169,\n",
      "                    0.0140, 0.0153, 0.0175, 0.0195, 0.0298, 0.0185, 0.0142, 0.0146, 0.0169,\n",
      "                    0.0153, 0.0212, 0.0247, 0.0185, 0.0177, 0.0156, 0.0165, 0.0167, 0.0205,\n",
      "                    0.0200, 0.0152, 0.0224, 0.0221, 0.0177, 0.0109, 0.0180, 0.0163, 0.0120,\n",
      "                    0.0208, 0.0166, 0.0188, 0.0191, 0.0185, 0.0145, 0.0140, 0.0200, 0.0172,\n",
      "                    0.0128, 0.0152, 0.0177, 0.0170, 0.0212, 0.0128, 0.0189, 0.0169, 0.0157,\n",
      "                    0.0127, 0.0141, 0.0176, 0.0201, 0.0166, 0.0156, 0.0168, 0.0207, 0.0155,\n",
      "                    0.0246, 0.0184, 0.0179, 0.0155, 0.0199, 0.0157, 0.0180, 0.0166, 0.0165,\n",
      "                    0.0182, 0.0194, 0.0167, 0.0163, 0.0113, 0.0181, 0.0173, 0.0154, 0.0199,\n",
      "                    0.0151, 0.0145, 0.0197, 0.0130, 0.0170, 0.0167, 0.0224, 0.0146, 0.0161,\n",
      "                    0.0213, 0.0130, 0.0159, 0.0143, 0.0125, 0.0147, 0.0126, 0.0133, 0.0136,\n",
      "                    0.0142, 0.0206, 0.0143, 0.0144, 0.0143, 0.0158, 0.0195, 0.0151, 0.0158,\n",
      "                    0.0186, 0.0215, 0.0184, 0.0157, 0.0208, 0.0169, 0.0161, 0.0169, 0.0157,\n",
      "                    0.0193, 0.0137, 0.0161, 0.0132, 0.0202, 0.0158, 0.0154, 0.0164, 0.0178,\n",
      "                    0.0167, 0.0194, 0.0212, 0.0137, 0.0202, 0.0188, 0.0188, 0.0224, 0.0158,\n",
      "                    0.0193, 0.0185, 0.0234, 0.0191, 0.0175, 0.0160, 0.0130, 0.0159, 0.0112,\n",
      "                    0.0152, 0.0136, 0.0183, 0.0184, 0.0149, 0.0131, 0.0170, 0.0143, 0.0152,\n",
      "                    0.0190, 0.0188, 0.0150, 0.0204, 0.0165, 0.0148, 0.0130, 0.0183, 0.0182,\n",
      "                    0.0196, 0.0156, 0.0150, 0.0202, 0.0165, 0.0184, 0.0171, 0.0159, 0.0200,\n",
      "                    0.0196, 0.0193, 0.0150, 0.0145, 0.0193, 0.0191, 0.0162, 0.0176, 0.0193,\n",
      "                    0.0140, 0.0173, 0.0180, 0.0136, 0.0213, 0.0151, 0.0159, 0.0141, 0.0181,\n",
      "                    0.0154, 0.0158, 0.0197, 0.0165, 0.0122, 0.0167, 0.0175, 0.0207, 0.0169,\n",
      "                    0.0208, 0.0155, 0.0170, 0.0173, 0.0194, 0.0174, 0.0151, 0.0190, 0.0136,\n",
      "                    0.0168, 0.0144, 0.0194, 0.0181, 0.0149, 0.0179, 0.0155, 0.0239, 0.0158,\n",
      "                    0.0138, 0.0174, 0.0143, 0.0213, 0.0130, 0.0186, 0.0162, 0.0210, 0.0161,\n",
      "                    0.0213, 0.0159, 0.0160, 0.0157, 0.0161, 0.0141, 0.0145, 0.0146, 0.0175,\n",
      "                    0.0150, 0.0166, 0.0153, 0.0148, 0.0150, 0.0160, 0.0182, 0.0160, 0.0171,\n",
      "                    0.0160, 0.0211, 0.0191, 0.0139, 0.0188, 0.0161, 0.0164, 0.0167, 0.0158,\n",
      "                    0.0167, 0.0161, 0.0170, 0.0190, 0.0239, 0.0249, 0.0212, 0.0156, 0.0170,\n",
      "                    0.0158, 0.0183, 0.0150, 0.0223, 0.0236, 0.0155, 0.0172, 0.0175, 0.0160,\n",
      "                    0.0175, 0.0188, 0.0262, 0.0144, 0.0154, 0.0160, 0.0188, 0.0210, 0.0164,\n",
      "                    0.0161, 0.0162, 0.0157, 0.0196, 0.0138, 0.0184, 0.0210, 0.0111, 0.0154,\n",
      "                    0.0187, 0.0171, 0.0182, 0.0168, 0.0131, 0.0147, 0.0147, 0.0181, 0.0177,\n",
      "                    0.0170, 0.0176, 0.0140, 0.0139, 0.0132, 0.0172, 0.0193, 0.0178, 0.0175,\n",
      "                    0.0160, 0.0195, 0.0245, 0.0165, 0.0174, 0.0184, 0.0156, 0.0175, 0.0156,\n",
      "                    0.0177, 0.0193, 0.0142, 0.0221, 0.0124, 0.0158, 0.0128, 0.0138, 0.0177,\n",
      "                    0.0175, 0.0159, 0.0263, 0.0165, 0.0194, 0.0227, 0.0192, 0.0189, 0.0175,\n",
      "                    0.0125, 0.0130, 0.0231, 0.0222, 0.0211, 0.0171, 0.0184, 0.0259, 0.0154,\n",
      "                    0.0141, 0.0201, 0.0153, 0.0192, 0.0153, 0.0146, 0.0183, 0.0176, 0.0166,\n",
      "                    0.0178, 0.0165, 0.0152, 0.0198, 0.0186, 0.0223, 0.0185, 0.0166, 0.0162,\n",
      "                    0.0158, 0.0124, 0.0186, 0.0199, 0.0190, 0.0164, 0.0204, 0.0192, 0.0200,\n",
      "                    0.0178, 0.0178, 0.0191, 0.0189, 0.0161, 0.0154, 0.0181, 0.0125, 0.0141,\n",
      "                    0.0181, 0.0196, 0.0150, 0.0250, 0.0156, 0.0129, 0.0128, 0.0190, 0.0150,\n",
      "                    0.0130, 0.0176, 0.0165, 0.0219, 0.0179, 0.0199, 0.0229, 0.0151])\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): ConvBn2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([9.4642e-05, 1.2192e-04, 1.0800e-04, 7.5714e-05, 6.6750e-05, 9.3638e-05,\n",
      "                  7.3549e-05, 7.7489e-05, 7.1541e-05, 7.7810e-05, 8.3721e-05, 7.6785e-05,\n",
      "                  1.0677e-04, 8.0673e-05, 7.4933e-05, 1.0770e-04, 7.0169e-05, 1.0737e-04,\n",
      "                  9.2590e-05, 1.0221e-04, 7.7124e-05, 8.7977e-05, 8.4885e-05, 9.1170e-05,\n",
      "                  9.2806e-05, 6.5607e-05, 9.7790e-05, 7.4432e-05, 6.8859e-05, 6.1000e-05,\n",
      "                  7.2984e-05, 9.2024e-05, 1.2433e-04, 7.3921e-05, 8.7823e-05, 7.9948e-05,\n",
      "                  9.5358e-05, 9.3174e-05, 6.8856e-05, 7.1023e-05, 7.4986e-05, 7.7464e-05,\n",
      "                  1.3917e-04, 9.7649e-05, 9.6696e-05, 1.0030e-04, 6.9468e-05, 9.5946e-05,\n",
      "                  1.0117e-04, 6.8326e-05, 1.5484e-04, 1.3473e-04, 8.2997e-05, 1.2485e-04,\n",
      "                  6.6122e-05, 7.9801e-05, 1.0406e-04, 8.2655e-05, 8.4713e-05, 9.9632e-05,\n",
      "                  1.1039e-04, 9.9145e-05, 6.9989e-05, 6.8786e-05, 8.3828e-05, 1.0246e-04,\n",
      "                  9.9959e-05, 7.7963e-05, 9.9707e-05, 1.1094e-04, 7.0638e-05, 1.1020e-04,\n",
      "                  1.1649e-04, 6.1000e-05, 8.9666e-05, 7.9741e-05, 7.3648e-05, 6.7454e-05,\n",
      "                  9.8263e-05, 1.0787e-04, 1.2094e-04, 8.8814e-05, 8.3659e-05, 8.0787e-05,\n",
      "                  7.3637e-05, 8.4756e-05, 8.9674e-05, 8.2414e-05, 1.3245e-04, 9.1291e-05,\n",
      "                  8.5910e-05, 6.2118e-05, 8.7314e-05, 1.0196e-04, 7.6280e-05, 9.5553e-05,\n",
      "                  1.0623e-04, 9.3214e-05, 9.0894e-05, 1.2077e-04, 7.8141e-05, 9.5297e-05,\n",
      "                  1.1004e-04, 7.1762e-05, 9.7544e-05, 6.8578e-05, 7.0099e-05, 8.0706e-05,\n",
      "                  7.5768e-05, 7.8150e-05, 7.5940e-05, 9.7483e-05, 7.0884e-05, 7.6351e-05,\n",
      "                  1.0886e-04, 6.9222e-05, 1.0164e-04, 7.5869e-05, 8.8962e-05, 6.1000e-05,\n",
      "                  8.0161e-05, 7.1813e-05, 8.7602e-05, 7.8361e-05, 9.0779e-05, 6.3038e-05,\n",
      "                  1.0376e-04, 8.6273e-05, 8.0998e-05, 8.4260e-05, 1.0048e-04, 7.3915e-05,\n",
      "                  8.6343e-05, 6.8652e-05, 9.5351e-05, 8.6087e-05, 8.2613e-05, 1.1211e-04,\n",
      "                  8.2297e-05, 7.6461e-05, 8.6580e-05, 7.0427e-05, 7.6709e-05, 7.8135e-05,\n",
      "                  1.0066e-04, 8.9774e-05, 6.2648e-05, 9.0735e-05, 7.7553e-05, 7.2747e-05,\n",
      "                  9.4558e-05, 7.3807e-05, 8.3109e-05, 1.0038e-04, 7.2098e-05, 8.1586e-05,\n",
      "                  8.8169e-05, 1.3235e-04, 9.5498e-05, 1.3280e-04, 9.6183e-05, 1.0538e-04,\n",
      "                  1.0749e-04, 9.0829e-05, 1.1418e-04, 6.2934e-05, 7.4040e-05, 7.6171e-05,\n",
      "                  8.7481e-05, 1.2107e-04, 9.1299e-05, 7.8362e-05, 7.8333e-05, 9.4258e-05,\n",
      "                  8.6370e-05, 7.0420e-05, 7.4002e-05, 9.1647e-05, 6.4476e-05, 9.1384e-05,\n",
      "                  1.1551e-04, 8.7844e-05, 7.2002e-05, 6.5197e-05, 9.2210e-05, 8.8862e-05,\n",
      "                  1.1902e-04, 9.4113e-05, 7.1239e-05, 8.5275e-05, 1.2114e-04, 8.6969e-05,\n",
      "                  8.4465e-05, 8.5563e-05, 9.2392e-05, 7.5673e-05, 8.8619e-05, 8.5640e-05,\n",
      "                  6.4779e-05, 6.1000e-05, 9.7740e-05, 1.0635e-04, 9.5872e-05, 1.0833e-04,\n",
      "                  8.4872e-05, 1.1183e-04, 8.2494e-05, 6.1000e-05, 7.7868e-05, 7.7461e-05,\n",
      "                  1.0129e-04, 7.7681e-05, 9.8383e-05, 7.2827e-05, 1.1246e-04, 7.2885e-05,\n",
      "                  1.0175e-04, 1.0153e-04, 1.2102e-04, 8.3937e-05, 9.0733e-05, 8.2485e-05,\n",
      "                  7.9639e-05, 7.2351e-05, 1.0256e-04, 7.6972e-05, 7.0300e-05, 1.0580e-04,\n",
      "                  1.2766e-04, 7.5409e-05, 1.0760e-04, 7.3833e-05, 6.9660e-05, 8.1631e-05,\n",
      "                  7.0349e-05, 9.8446e-05, 6.2894e-05, 9.8221e-05, 8.6192e-05, 8.5603e-05,\n",
      "                  9.5665e-05, 6.9321e-05, 1.0906e-04, 1.0274e-04, 7.7843e-05, 7.6785e-05,\n",
      "                  7.9327e-05, 7.2631e-05, 8.2750e-05, 7.7201e-05, 8.2839e-05, 7.5790e-05,\n",
      "                  7.5136e-05, 8.7052e-05, 8.7379e-05, 7.7537e-05, 1.0646e-04, 6.5800e-05,\n",
      "                  7.4898e-05, 9.8482e-05, 7.0827e-05, 6.7007e-05, 7.1706e-05, 9.8434e-05,\n",
      "                  6.3290e-05, 1.1334e-04, 6.3569e-05, 9.0677e-05, 7.2901e-05, 8.7073e-05,\n",
      "                  1.0925e-04, 8.3550e-05, 1.0863e-04, 1.0018e-04, 9.4389e-05, 7.9628e-05,\n",
      "                  7.2401e-05, 9.8543e-05, 1.2474e-04, 8.1433e-05, 8.5051e-05, 7.5095e-05,\n",
      "                  8.8290e-05, 7.2909e-05, 8.7367e-05, 8.7696e-05, 9.7713e-05, 9.2512e-05,\n",
      "                  9.1960e-05, 9.8298e-05, 8.2766e-05, 7.8420e-05, 7.6277e-05, 1.1026e-04,\n",
      "                  6.9053e-05, 6.6839e-05, 7.0355e-05, 8.9956e-05, 9.9478e-05, 1.0954e-04,\n",
      "                  9.0810e-05, 1.1573e-04, 6.3854e-05, 8.8862e-05, 8.6094e-05, 8.1086e-05,\n",
      "                  8.9748e-05, 8.8743e-05, 7.7322e-05, 8.6755e-05, 8.5622e-05, 7.3927e-05,\n",
      "                  1.0491e-04, 7.5250e-05, 1.2888e-04, 9.6043e-05, 8.7867e-05, 9.9335e-05,\n",
      "                  8.2706e-05, 9.2922e-05, 6.7849e-05, 9.5820e-05, 8.1622e-05, 8.2077e-05,\n",
      "                  9.0613e-05, 7.5999e-05, 8.7175e-05, 7.0023e-05, 9.4033e-05, 1.0480e-04,\n",
      "                  8.6991e-05, 6.2747e-05, 7.2478e-05, 8.3133e-05, 9.4975e-05, 8.6238e-05,\n",
      "                  9.1601e-05, 8.9012e-05, 1.3310e-04, 7.1412e-05, 7.0044e-05, 9.1204e-05,\n",
      "                  9.1667e-05, 7.8616e-05, 6.9046e-05, 8.0934e-05, 1.1060e-04, 1.3556e-04,\n",
      "                  7.7666e-05, 8.1061e-05, 9.0144e-05, 9.2352e-05, 8.0748e-05, 7.9127e-05,\n",
      "                  8.5041e-05, 1.0443e-04, 1.1330e-04, 9.9205e-05, 9.6421e-05, 7.4828e-05,\n",
      "                  7.3091e-05, 8.0227e-05, 8.1513e-05, 1.0985e-04, 7.6806e-05, 8.4984e-05,\n",
      "                  9.0254e-05, 7.8183e-05, 8.6863e-05, 8.6844e-05, 6.9008e-05, 9.8430e-05,\n",
      "                  6.6173e-05, 1.0181e-04, 8.0560e-05, 9.4930e-05, 9.8638e-05, 6.4816e-05,\n",
      "                  6.7289e-05, 7.5567e-05, 6.4655e-05, 6.8372e-05, 7.5144e-05, 9.6701e-05,\n",
      "                  1.0610e-04, 9.2384e-05, 1.2930e-04, 7.1935e-05, 1.0457e-04, 7.4418e-05,\n",
      "                  7.7384e-05, 6.4918e-05, 6.3931e-05, 9.1632e-05, 9.6039e-05, 9.3490e-05,\n",
      "                  1.0486e-04, 8.3864e-05, 7.1547e-05, 7.1809e-05, 7.1922e-05, 6.7782e-05,\n",
      "                  6.2791e-05, 9.0041e-05, 8.9988e-05, 8.1794e-05, 1.0613e-04, 9.7688e-05,\n",
      "                  9.8097e-05, 9.3747e-05, 8.4103e-05, 1.0983e-04, 1.2254e-04, 8.7164e-05,\n",
      "                  7.4656e-05, 8.9536e-05, 1.3548e-04, 6.8263e-05, 7.0104e-05, 1.3093e-04,\n",
      "                  8.1385e-05, 8.6298e-05, 8.8358e-05, 6.1603e-05, 9.4679e-05, 6.4232e-05,\n",
      "                  9.3085e-05, 8.1125e-05, 9.0428e-05, 9.6606e-05, 1.0383e-04, 7.6492e-05,\n",
      "                  7.9653e-05, 7.1304e-05, 1.0428e-04, 8.3213e-05, 7.1247e-05, 6.1000e-05,\n",
      "                  8.4236e-05, 1.2715e-04, 9.6286e-05, 7.8231e-05, 9.2335e-05, 1.1599e-04,\n",
      "                  9.5272e-05, 7.9021e-05, 8.4733e-05, 7.9383e-05, 1.0138e-04, 7.7662e-05,\n",
      "                  7.7992e-05, 7.4452e-05, 1.0308e-04, 8.8241e-05, 8.5536e-05, 7.3795e-05,\n",
      "                  8.1521e-05, 8.1839e-05, 7.7707e-05, 1.0798e-04, 1.1894e-04, 1.0035e-04,\n",
      "                  1.1515e-04, 7.1537e-05, 8.4773e-05, 6.7499e-05, 6.7563e-05, 9.4251e-05,\n",
      "                  9.6121e-05, 8.9294e-05, 8.2399e-05, 9.4704e-05, 7.2477e-05, 8.4833e-05,\n",
      "                  8.3729e-05, 6.8607e-05, 8.5790e-05, 8.6071e-05, 7.2925e-05, 7.6403e-05,\n",
      "                  8.5284e-05, 9.8571e-05, 8.8055e-05, 7.1968e-05, 9.8092e-05, 8.4320e-05,\n",
      "                  7.5081e-05, 8.4642e-05, 1.0720e-04, 7.8411e-05, 8.5759e-05, 9.6954e-05,\n",
      "                  1.3117e-04, 9.5671e-05, 8.4907e-05, 7.7168e-05, 7.9634e-05, 9.9483e-05,\n",
      "                  7.5793e-05, 9.9219e-05, 7.3068e-05, 8.7783e-05, 6.9653e-05, 1.2337e-04,\n",
      "                  8.8732e-05, 6.9074e-05, 9.2542e-05, 8.7078e-05, 7.8775e-05, 6.8206e-05,\n",
      "                  8.9391e-05, 1.0396e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
      "            min_val=tensor([-0.0121, -0.0147, -0.0130, -0.0097, -0.0085, -0.0118, -0.0094, -0.0084,\n",
      "                    -0.0092, -0.0094, -0.0107, -0.0097, -0.0137, -0.0103, -0.0096, -0.0126,\n",
      "                    -0.0090, -0.0133, -0.0119, -0.0110, -0.0099, -0.0102, -0.0109, -0.0117,\n",
      "                    -0.0119, -0.0084, -0.0125, -0.0089, -0.0088, -0.0072, -0.0093, -0.0118,\n",
      "                    -0.0151, -0.0095, -0.0111, -0.0102, -0.0122, -0.0119, -0.0088, -0.0091,\n",
      "                    -0.0093, -0.0088, -0.0165, -0.0125, -0.0124, -0.0126, -0.0082, -0.0123,\n",
      "                    -0.0130, -0.0087, -0.0198, -0.0172, -0.0106, -0.0160, -0.0085, -0.0102,\n",
      "                    -0.0133, -0.0106, -0.0108, -0.0128, -0.0141, -0.0121, -0.0084, -0.0087,\n",
      "                    -0.0107, -0.0131, -0.0125, -0.0098, -0.0127, -0.0122, -0.0090, -0.0140,\n",
      "                    -0.0149, -0.0075, -0.0105, -0.0097, -0.0094, -0.0086, -0.0116, -0.0127,\n",
      "                    -0.0155, -0.0114, -0.0103, -0.0103, -0.0094, -0.0108, -0.0115, -0.0093,\n",
      "                    -0.0151, -0.0115, -0.0110, -0.0079, -0.0112, -0.0112, -0.0096, -0.0111,\n",
      "                    -0.0136, -0.0119, -0.0116, -0.0149, -0.0100, -0.0106, -0.0141, -0.0088,\n",
      "                    -0.0125, -0.0085, -0.0090, -0.0102, -0.0092, -0.0100, -0.0097, -0.0125,\n",
      "                    -0.0091, -0.0094, -0.0129, -0.0085, -0.0115, -0.0097, -0.0096, -0.0075,\n",
      "                    -0.0102, -0.0092, -0.0107, -0.0100, -0.0116, -0.0081, -0.0133, -0.0110,\n",
      "                    -0.0104, -0.0104, -0.0116, -0.0095, -0.0109, -0.0088, -0.0122, -0.0110,\n",
      "                    -0.0104, -0.0144, -0.0105, -0.0096, -0.0111, -0.0077, -0.0095, -0.0093,\n",
      "                    -0.0129, -0.0102, -0.0080, -0.0116, -0.0099, -0.0091, -0.0121, -0.0094,\n",
      "                    -0.0103, -0.0128, -0.0091, -0.0092, -0.0113, -0.0158, -0.0122, -0.0162,\n",
      "                    -0.0105, -0.0128, -0.0138, -0.0108, -0.0146, -0.0081, -0.0089, -0.0097,\n",
      "                    -0.0110, -0.0154, -0.0115, -0.0099, -0.0100, -0.0121, -0.0111, -0.0090,\n",
      "                    -0.0094, -0.0117, -0.0083, -0.0114, -0.0126, -0.0104, -0.0092, -0.0083,\n",
      "                    -0.0094, -0.0099, -0.0142, -0.0120, -0.0085, -0.0108, -0.0155, -0.0111,\n",
      "                    -0.0100, -0.0107, -0.0118, -0.0097, -0.0113, -0.0109, -0.0080, -0.0075,\n",
      "                    -0.0125, -0.0117, -0.0123, -0.0139, -0.0108, -0.0125, -0.0100, -0.0076,\n",
      "                    -0.0100, -0.0098, -0.0124, -0.0099, -0.0126, -0.0093, -0.0129, -0.0086,\n",
      "                    -0.0107, -0.0130, -0.0121, -0.0104, -0.0107, -0.0102, -0.0095, -0.0090,\n",
      "                    -0.0117, -0.0087, -0.0089, -0.0135, -0.0126, -0.0096, -0.0138, -0.0095,\n",
      "                    -0.0086, -0.0104, -0.0090, -0.0120, -0.0081, -0.0126, -0.0110, -0.0107,\n",
      "                    -0.0122, -0.0089, -0.0140, -0.0132, -0.0093, -0.0098, -0.0102, -0.0091,\n",
      "                    -0.0104, -0.0099, -0.0106, -0.0097, -0.0096, -0.0105, -0.0112, -0.0099,\n",
      "                    -0.0123, -0.0082, -0.0084, -0.0126, -0.0083, -0.0086, -0.0092, -0.0126,\n",
      "                    -0.0081, -0.0135, -0.0081, -0.0116, -0.0091, -0.0108, -0.0125, -0.0107,\n",
      "                    -0.0139, -0.0123, -0.0121, -0.0102, -0.0080, -0.0126, -0.0151, -0.0102,\n",
      "                    -0.0101, -0.0096, -0.0113, -0.0092, -0.0112, -0.0102, -0.0125, -0.0110,\n",
      "                    -0.0118, -0.0113, -0.0106, -0.0100, -0.0098, -0.0132, -0.0088, -0.0080,\n",
      "                    -0.0090, -0.0115, -0.0127, -0.0140, -0.0112, -0.0148, -0.0082, -0.0107,\n",
      "                    -0.0110, -0.0098, -0.0114, -0.0113, -0.0099, -0.0105, -0.0110, -0.0095,\n",
      "                    -0.0134, -0.0096, -0.0165, -0.0121, -0.0112, -0.0127, -0.0096, -0.0119,\n",
      "                    -0.0087, -0.0123, -0.0104, -0.0105, -0.0116, -0.0097, -0.0104, -0.0090,\n",
      "                    -0.0120, -0.0126, -0.0111, -0.0080, -0.0088, -0.0106, -0.0122, -0.0100,\n",
      "                    -0.0117, -0.0111, -0.0170, -0.0086, -0.0090, -0.0117, -0.0117, -0.0100,\n",
      "                    -0.0088, -0.0101, -0.0142, -0.0174, -0.0099, -0.0096, -0.0115, -0.0118,\n",
      "                    -0.0103, -0.0090, -0.0109, -0.0132, -0.0145, -0.0127, -0.0123, -0.0091,\n",
      "                    -0.0094, -0.0098, -0.0104, -0.0126, -0.0098, -0.0109, -0.0116, -0.0100,\n",
      "                    -0.0111, -0.0107, -0.0088, -0.0122, -0.0085, -0.0130, -0.0103, -0.0113,\n",
      "                    -0.0117, -0.0083, -0.0082, -0.0097, -0.0083, -0.0083, -0.0096, -0.0124,\n",
      "                    -0.0136, -0.0115, -0.0166, -0.0092, -0.0131, -0.0095, -0.0095, -0.0082,\n",
      "                    -0.0079, -0.0113, -0.0123, -0.0115, -0.0134, -0.0107, -0.0090, -0.0092,\n",
      "                    -0.0089, -0.0087, -0.0080, -0.0115, -0.0115, -0.0105, -0.0136, -0.0125,\n",
      "                    -0.0125, -0.0113, -0.0108, -0.0134, -0.0157, -0.0107, -0.0096, -0.0115,\n",
      "                    -0.0151, -0.0087, -0.0090, -0.0152, -0.0104, -0.0107, -0.0112, -0.0079,\n",
      "                    -0.0115, -0.0082, -0.0114, -0.0091, -0.0112, -0.0103, -0.0122, -0.0098,\n",
      "                    -0.0102, -0.0081, -0.0130, -0.0107, -0.0081, -0.0071, -0.0108, -0.0141,\n",
      "                    -0.0110, -0.0100, -0.0108, -0.0138, -0.0110, -0.0100, -0.0107, -0.0094,\n",
      "                    -0.0130, -0.0099, -0.0100, -0.0095, -0.0127, -0.0111, -0.0109, -0.0094,\n",
      "                    -0.0104, -0.0105, -0.0099, -0.0138, -0.0152, -0.0111, -0.0142, -0.0084,\n",
      "                    -0.0103, -0.0086, -0.0085, -0.0110, -0.0120, -0.0114, -0.0099, -0.0121,\n",
      "                    -0.0093, -0.0109, -0.0107, -0.0082, -0.0104, -0.0110, -0.0093, -0.0090,\n",
      "                    -0.0105, -0.0126, -0.0113, -0.0090, -0.0126, -0.0108, -0.0096, -0.0108,\n",
      "                    -0.0118, -0.0097, -0.0110, -0.0124, -0.0151, -0.0108, -0.0109, -0.0093,\n",
      "                    -0.0093, -0.0127, -0.0097, -0.0115, -0.0094, -0.0107, -0.0089, -0.0149,\n",
      "                    -0.0112, -0.0088, -0.0118, -0.0111, -0.0101, -0.0083, -0.0114, -0.0133]), max_val=tensor([0.0117, 0.0155, 0.0137, 0.0093, 0.0081, 0.0119, 0.0081, 0.0098, 0.0086,\n",
      "                    0.0099, 0.0097, 0.0098, 0.0119, 0.0090, 0.0094, 0.0137, 0.0087, 0.0136,\n",
      "                    0.0113, 0.0130, 0.0092, 0.0112, 0.0101, 0.0111, 0.0111, 0.0079, 0.0114,\n",
      "                    0.0095, 0.0087, 0.0075, 0.0087, 0.0114, 0.0158, 0.0093, 0.0112, 0.0085,\n",
      "                    0.0108, 0.0111, 0.0086, 0.0082, 0.0095, 0.0098, 0.0177, 0.0107, 0.0119,\n",
      "                    0.0127, 0.0088, 0.0108, 0.0123, 0.0082, 0.0180, 0.0164, 0.0104, 0.0154,\n",
      "                    0.0083, 0.0096, 0.0123, 0.0098, 0.0096, 0.0121, 0.0137, 0.0126, 0.0089,\n",
      "                    0.0087, 0.0106, 0.0130, 0.0127, 0.0099, 0.0127, 0.0141, 0.0087, 0.0140,\n",
      "                    0.0148, 0.0075, 0.0114, 0.0101, 0.0089, 0.0080, 0.0125, 0.0137, 0.0142,\n",
      "                    0.0110, 0.0106, 0.0095, 0.0087, 0.0108, 0.0102, 0.0105, 0.0168, 0.0116,\n",
      "                    0.0090, 0.0079, 0.0097, 0.0129, 0.0097, 0.0121, 0.0126, 0.0115, 0.0111,\n",
      "                    0.0153, 0.0090, 0.0121, 0.0138, 0.0091, 0.0120, 0.0087, 0.0085, 0.0102,\n",
      "                    0.0096, 0.0095, 0.0090, 0.0106, 0.0090, 0.0097, 0.0138, 0.0088, 0.0129,\n",
      "                    0.0088, 0.0113, 0.0074, 0.0102, 0.0089, 0.0111, 0.0088, 0.0101, 0.0073,\n",
      "                    0.0127, 0.0105, 0.0095, 0.0107, 0.0128, 0.0089, 0.0110, 0.0080, 0.0106,\n",
      "                    0.0108, 0.0105, 0.0128, 0.0105, 0.0097, 0.0099, 0.0089, 0.0097, 0.0099,\n",
      "                    0.0123, 0.0114, 0.0074, 0.0115, 0.0088, 0.0092, 0.0117, 0.0094, 0.0106,\n",
      "                    0.0127, 0.0092, 0.0104, 0.0110, 0.0168, 0.0108, 0.0169, 0.0122, 0.0134,\n",
      "                    0.0128, 0.0115, 0.0124, 0.0075, 0.0094, 0.0080, 0.0111, 0.0154, 0.0116,\n",
      "                    0.0100, 0.0099, 0.0118, 0.0105, 0.0084, 0.0094, 0.0114, 0.0082, 0.0116,\n",
      "                    0.0147, 0.0112, 0.0089, 0.0078, 0.0117, 0.0113, 0.0151, 0.0118, 0.0090,\n",
      "                    0.0108, 0.0151, 0.0097, 0.0107, 0.0109, 0.0104, 0.0095, 0.0104, 0.0109,\n",
      "                    0.0082, 0.0073, 0.0113, 0.0135, 0.0104, 0.0134, 0.0108, 0.0142, 0.0105,\n",
      "                    0.0071, 0.0096, 0.0098, 0.0129, 0.0092, 0.0119, 0.0092, 0.0143, 0.0093,\n",
      "                    0.0129, 0.0121, 0.0154, 0.0107, 0.0115, 0.0105, 0.0101, 0.0092, 0.0130,\n",
      "                    0.0098, 0.0089, 0.0128, 0.0162, 0.0096, 0.0121, 0.0090, 0.0088, 0.0101,\n",
      "                    0.0079, 0.0125, 0.0077, 0.0115, 0.0108, 0.0109, 0.0117, 0.0083, 0.0132,\n",
      "                    0.0125, 0.0099, 0.0095, 0.0096, 0.0092, 0.0105, 0.0084, 0.0102, 0.0083,\n",
      "                    0.0095, 0.0111, 0.0105, 0.0097, 0.0135, 0.0084, 0.0095, 0.0123, 0.0090,\n",
      "                    0.0085, 0.0087, 0.0125, 0.0074, 0.0144, 0.0074, 0.0115, 0.0093, 0.0111,\n",
      "                    0.0139, 0.0095, 0.0133, 0.0127, 0.0112, 0.0099, 0.0092, 0.0117, 0.0158,\n",
      "                    0.0103, 0.0108, 0.0092, 0.0111, 0.0093, 0.0110, 0.0111, 0.0112, 0.0117,\n",
      "                    0.0111, 0.0125, 0.0105, 0.0100, 0.0096, 0.0140, 0.0074, 0.0085, 0.0085,\n",
      "                    0.0114, 0.0115, 0.0128, 0.0115, 0.0134, 0.0079, 0.0113, 0.0102, 0.0103,\n",
      "                    0.0114, 0.0113, 0.0094, 0.0110, 0.0103, 0.0076, 0.0127, 0.0093, 0.0146,\n",
      "                    0.0122, 0.0109, 0.0124, 0.0105, 0.0113, 0.0085, 0.0112, 0.0100, 0.0096,\n",
      "                    0.0106, 0.0093, 0.0111, 0.0083, 0.0104, 0.0133, 0.0102, 0.0077, 0.0092,\n",
      "                    0.0100, 0.0107, 0.0110, 0.0097, 0.0113, 0.0149, 0.0091, 0.0087, 0.0104,\n",
      "                    0.0110, 0.0100, 0.0087, 0.0103, 0.0126, 0.0166, 0.0086, 0.0103, 0.0102,\n",
      "                    0.0100, 0.0096, 0.0100, 0.0095, 0.0133, 0.0122, 0.0117, 0.0122, 0.0095,\n",
      "                    0.0087, 0.0102, 0.0091, 0.0140, 0.0090, 0.0106, 0.0105, 0.0094, 0.0107,\n",
      "                    0.0110, 0.0088, 0.0125, 0.0077, 0.0113, 0.0083, 0.0121, 0.0125, 0.0079,\n",
      "                    0.0085, 0.0091, 0.0075, 0.0087, 0.0092, 0.0117, 0.0125, 0.0117, 0.0145,\n",
      "                    0.0091, 0.0133, 0.0093, 0.0098, 0.0082, 0.0081, 0.0116, 0.0110, 0.0119,\n",
      "                    0.0130, 0.0100, 0.0091, 0.0088, 0.0091, 0.0082, 0.0074, 0.0112, 0.0105,\n",
      "                    0.0102, 0.0121, 0.0120, 0.0125, 0.0119, 0.0105, 0.0139, 0.0145, 0.0111,\n",
      "                    0.0089, 0.0101, 0.0172, 0.0083, 0.0088, 0.0166, 0.0103, 0.0110, 0.0112,\n",
      "                    0.0078, 0.0120, 0.0082, 0.0118, 0.0103, 0.0115, 0.0123, 0.0132, 0.0097,\n",
      "                    0.0081, 0.0091, 0.0132, 0.0100, 0.0090, 0.0074, 0.0104, 0.0161, 0.0122,\n",
      "                    0.0089, 0.0117, 0.0147, 0.0121, 0.0100, 0.0108, 0.0101, 0.0123, 0.0099,\n",
      "                    0.0091, 0.0094, 0.0131, 0.0112, 0.0108, 0.0090, 0.0101, 0.0101, 0.0099,\n",
      "                    0.0125, 0.0140, 0.0127, 0.0146, 0.0091, 0.0108, 0.0084, 0.0086, 0.0120,\n",
      "                    0.0122, 0.0113, 0.0105, 0.0112, 0.0088, 0.0092, 0.0099, 0.0087, 0.0109,\n",
      "                    0.0109, 0.0090, 0.0097, 0.0108, 0.0119, 0.0107, 0.0091, 0.0120, 0.0097,\n",
      "                    0.0088, 0.0100, 0.0136, 0.0100, 0.0109, 0.0119, 0.0167, 0.0122, 0.0100,\n",
      "                    0.0098, 0.0101, 0.0118, 0.0089, 0.0126, 0.0089, 0.0111, 0.0083, 0.0157,\n",
      "                    0.0113, 0.0086, 0.0118, 0.0107, 0.0100, 0.0087, 0.0107, 0.0121])\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (activation_post_process_28): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0786]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.792599678039551, max_val=5.186406135559082)\n",
      "  )\n",
      "  (activation_post_process_29): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0486]), zero_point=tensor([20], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9731674790382385, max_val=5.204519748687744)\n",
      "  )\n",
      "  (activation_post_process_30): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0765]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.665796756744385, max_val=5.046833038330078)\n",
      "  )\n",
      "  (activation_post_process_31): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0475]), zero_point=tensor([21], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9988173842430115, max_val=5.038806438446045)\n",
      "  )\n",
      "  (activation_post_process_32): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0909]), zero_point=tensor([21], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.90863835811615, max_val=9.631388664245605)\n",
      "  )\n",
      "  (drop3): Dropout(p=0.5, inplace=False)\n",
      "  (activation_post_process_33): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1805]), zero_point=tensor([20], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6690683364868164, max_val=19.250625610351562)\n",
      "  )\n",
      "  (classifier): Module(\n",
      "    (0): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(\n",
      "      in_features=512, out_features=7, bias=True\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.0376, -0.0456, -0.0388, -0.0426, -0.0380, -0.0410, -0.0419]), max_val=tensor([0.0471, 0.0477, 0.0458, 0.0472, 0.0462, 0.0471, 0.0461]))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (activation_post_process_34): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1805]), zero_point=tensor([20], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6690683364868164, max_val=19.250625610351562)\n",
      "  )\n",
      "  (activation_post_process_35): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1514]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=19.226797103881836)\n",
      "  )\n",
      "  (activation_post_process_36): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1216]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=3.1785521507263184, max_val=15.437615394592285)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, xb):\n",
      "    to = xb.to('cpu', non_blocking = True);  xb = None\n",
      "    activation_post_process_0 = self.activation_post_process_0(to);  to = None\n",
      "    conv1_0 = getattr(self.conv1, \"0\")(activation_post_process_0);  activation_post_process_0 = None\n",
      "    activation_post_process_1 = self.activation_post_process_1(conv1_0);  conv1_0 = None\n",
      "    conv1_2 = getattr(self.conv1, \"2\")(activation_post_process_1);  activation_post_process_1 = None\n",
      "    activation_post_process_2 = self.activation_post_process_2(conv1_2);  conv1_2 = None\n",
      "    conv2_0 = getattr(self.conv2, \"0\")(activation_post_process_2);  activation_post_process_2 = None\n",
      "    activation_post_process_3 = self.activation_post_process_3(conv2_0);  conv2_0 = None\n",
      "    conv2_2 = getattr(self.conv2, \"2\")(activation_post_process_3);  activation_post_process_3 = None\n",
      "    activation_post_process_4 = self.activation_post_process_4(conv2_2);  conv2_2 = None\n",
      "    conv2_3 = getattr(self.conv2, \"3\")(activation_post_process_4);  activation_post_process_4 = None\n",
      "    activation_post_process_5 = self.activation_post_process_5(conv2_3);  conv2_3 = None\n",
      "    res1_0_0 = getattr(getattr(self.res1, \"0\"), \"0\")(activation_post_process_5)\n",
      "    activation_post_process_6 = self.activation_post_process_6(res1_0_0);  res1_0_0 = None\n",
      "    res1_0_2 = getattr(getattr(self.res1, \"0\"), \"2\")(activation_post_process_6);  activation_post_process_6 = None\n",
      "    activation_post_process_7 = self.activation_post_process_7(res1_0_2);  res1_0_2 = None\n",
      "    res1_1_0 = getattr(getattr(self.res1, \"1\"), \"0\")(activation_post_process_7);  activation_post_process_7 = None\n",
      "    activation_post_process_8 = self.activation_post_process_8(res1_1_0);  res1_1_0 = None\n",
      "    res1_1_2 = getattr(getattr(self.res1, \"1\"), \"2\")(activation_post_process_8);  activation_post_process_8 = None\n",
      "    activation_post_process_9 = self.activation_post_process_9(res1_1_2);  res1_1_2 = None\n",
      "    add = activation_post_process_9 + activation_post_process_5;  activation_post_process_9 = activation_post_process_5 = None\n",
      "    activation_post_process_10 = self.activation_post_process_10(add);  add = None\n",
      "    drop1 = self.drop1(activation_post_process_10);  activation_post_process_10 = None\n",
      "    activation_post_process_11 = self.activation_post_process_11(drop1);  drop1 = None\n",
      "    conv3_0 = getattr(self.conv3, \"0\")(activation_post_process_11);  activation_post_process_11 = None\n",
      "    activation_post_process_12 = self.activation_post_process_12(conv3_0);  conv3_0 = None\n",
      "    conv3_2 = getattr(self.conv3, \"2\")(activation_post_process_12);  activation_post_process_12 = None\n",
      "    activation_post_process_13 = self.activation_post_process_13(conv3_2);  conv3_2 = None\n",
      "    conv4_0 = getattr(self.conv4, \"0\")(activation_post_process_13);  activation_post_process_13 = None\n",
      "    activation_post_process_14 = self.activation_post_process_14(conv4_0);  conv4_0 = None\n",
      "    conv4_2 = getattr(self.conv4, \"2\")(activation_post_process_14);  activation_post_process_14 = None\n",
      "    activation_post_process_15 = self.activation_post_process_15(conv4_2);  conv4_2 = None\n",
      "    conv4_3 = getattr(self.conv4, \"3\")(activation_post_process_15);  activation_post_process_15 = None\n",
      "    activation_post_process_16 = self.activation_post_process_16(conv4_3);  conv4_3 = None\n",
      "    res2_0_0 = getattr(getattr(self.res2, \"0\"), \"0\")(activation_post_process_16)\n",
      "    activation_post_process_17 = self.activation_post_process_17(res2_0_0);  res2_0_0 = None\n",
      "    res2_0_2 = getattr(getattr(self.res2, \"0\"), \"2\")(activation_post_process_17);  activation_post_process_17 = None\n",
      "    activation_post_process_18 = self.activation_post_process_18(res2_0_2);  res2_0_2 = None\n",
      "    res2_1_0 = getattr(getattr(self.res2, \"1\"), \"0\")(activation_post_process_18);  activation_post_process_18 = None\n",
      "    activation_post_process_19 = self.activation_post_process_19(res2_1_0);  res2_1_0 = None\n",
      "    res2_1_2 = getattr(getattr(self.res2, \"1\"), \"2\")(activation_post_process_19);  activation_post_process_19 = None\n",
      "    activation_post_process_20 = self.activation_post_process_20(res2_1_2);  res2_1_2 = None\n",
      "    add_1 = activation_post_process_20 + activation_post_process_16;  activation_post_process_20 = activation_post_process_16 = None\n",
      "    activation_post_process_21 = self.activation_post_process_21(add_1);  add_1 = None\n",
      "    drop2 = self.drop2(activation_post_process_21);  activation_post_process_21 = None\n",
      "    activation_post_process_22 = self.activation_post_process_22(drop2);  drop2 = None\n",
      "    conv5_0 = getattr(self.conv5, \"0\")(activation_post_process_22);  activation_post_process_22 = None\n",
      "    activation_post_process_23 = self.activation_post_process_23(conv5_0);  conv5_0 = None\n",
      "    conv5_2 = getattr(self.conv5, \"2\")(activation_post_process_23);  activation_post_process_23 = None\n",
      "    activation_post_process_24 = self.activation_post_process_24(conv5_2);  conv5_2 = None\n",
      "    conv6_0 = getattr(self.conv6, \"0\")(activation_post_process_24);  activation_post_process_24 = None\n",
      "    activation_post_process_25 = self.activation_post_process_25(conv6_0);  conv6_0 = None\n",
      "    conv6_2 = getattr(self.conv6, \"2\")(activation_post_process_25);  activation_post_process_25 = None\n",
      "    activation_post_process_26 = self.activation_post_process_26(conv6_2);  conv6_2 = None\n",
      "    conv6_3 = getattr(self.conv6, \"3\")(activation_post_process_26);  activation_post_process_26 = None\n",
      "    activation_post_process_27 = self.activation_post_process_27(conv6_3);  conv6_3 = None\n",
      "    res3_0_0 = getattr(getattr(self.res3, \"0\"), \"0\")(activation_post_process_27)\n",
      "    activation_post_process_28 = self.activation_post_process_28(res3_0_0);  res3_0_0 = None\n",
      "    res3_0_2 = getattr(getattr(self.res3, \"0\"), \"2\")(activation_post_process_28);  activation_post_process_28 = None\n",
      "    activation_post_process_29 = self.activation_post_process_29(res3_0_2);  res3_0_2 = None\n",
      "    res3_1_0 = getattr(getattr(self.res3, \"1\"), \"0\")(activation_post_process_29);  activation_post_process_29 = None\n",
      "    activation_post_process_30 = self.activation_post_process_30(res3_1_0);  res3_1_0 = None\n",
      "    res3_1_2 = getattr(getattr(self.res3, \"1\"), \"2\")(activation_post_process_30);  activation_post_process_30 = None\n",
      "    activation_post_process_31 = self.activation_post_process_31(res3_1_2);  res3_1_2 = None\n",
      "    add_2 = activation_post_process_31 + activation_post_process_27;  activation_post_process_31 = activation_post_process_27 = None\n",
      "    activation_post_process_32 = self.activation_post_process_32(add_2);  add_2 = None\n",
      "    drop3 = self.drop3(activation_post_process_32);  activation_post_process_32 = None\n",
      "    activation_post_process_33 = self.activation_post_process_33(drop3);  drop3 = None\n",
      "    classifier_0 = getattr(self.classifier, \"0\")(activation_post_process_33);  activation_post_process_33 = None\n",
      "    activation_post_process_34 = self.activation_post_process_34(classifier_0);  classifier_0 = None\n",
      "    classifier_1 = getattr(self.classifier, \"1\")(activation_post_process_34);  activation_post_process_34 = None\n",
      "    activation_post_process_35 = self.activation_post_process_35(classifier_1);  classifier_1 = None\n",
      "    classifier_2 = getattr(self.classifier, \"2\")(activation_post_process_35);  activation_post_process_35 = None\n",
      "    activation_post_process_36 = self.activation_post_process_36(classifier_2);  classifier_2 = None\n",
      "    return activation_post_process_36\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n",
      "Quantized model:\n",
      "GraphModule(\n",
      "  (activation_post_process_0): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (conv1): Module(\n",
      "    (0): QuantizedConv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.1789742112159729, zero_point=61, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "  )\n",
      "  (activation_post_process_1): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_2): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (conv2): Module(\n",
      "    (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.17638634145259857, zero_point=59, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (activation_post_process_3): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_4): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (res1): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.1640031933784485, zero_point=64, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.1246543824672699, zero_point=63, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (activation_post_process_6): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_7): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_8): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_9): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_10): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (drop1): QuantizedDropout(p=0.5, inplace=False)\n",
      "  (activation_post_process_11): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (conv3): Module(\n",
      "    (0): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.15532495081424713, zero_point=62, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "  )\n",
      "  (activation_post_process_12): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_13): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (conv4): Module(\n",
      "    (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.1435985416173935, zero_point=57, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (activation_post_process_14): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_15): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (res2): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.10345306992530823, zero_point=61, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.10269077867269516, zero_point=59, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (activation_post_process_17): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_18): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_19): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_20): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_21): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (drop2): QuantizedDropout(p=0.5, inplace=False)\n",
      "  (activation_post_process_22): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (conv5): Module(\n",
      "    (0): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.11475169658660889, zero_point=60, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "  )\n",
      "  (activation_post_process_23): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_24): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (conv6): Module(\n",
      "    (0): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.10323867946863174, zero_point=58, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (activation_post_process_25): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_26): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (res3): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07864926755428314, zero_point=61, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07648919522762299, zero_point=61, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (activation_post_process_28): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_29): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_30): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_31): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_32): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (drop3): QuantizedDropout(p=0.5, inplace=False)\n",
      "  (activation_post_process_33): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (classifier): Module(\n",
      "    (0): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): QuantizedLinear(in_features=512, out_features=7, scale=0.12156301736831665, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      "  (activation_post_process_35): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      "  (activation_post_process_36): Module(\n",
      "    (activation_post_process): Module()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, xb):\n",
      "    to = xb.to('cpu', non_blocking = True);  xb = None\n",
      "    activation_post_process_0_observer_enabled = self.activation_post_process_0.observer_enabled\n",
      "    activation_post_process_0_fake_quant_enabled = self.activation_post_process_0.fake_quant_enabled\n",
      "    activation_post_process_0_activation_post_process_min_val = self.activation_post_process_0.activation_post_process.min_val\n",
      "    activation_post_process_0_activation_post_process_max_val = self.activation_post_process_0.activation_post_process.max_val\n",
      "    activation_post_process_0_scale = self.activation_post_process_0.scale\n",
      "    activation_post_process_0_zero_point = self.activation_post_process_0.zero_point\n",
      "    fused_moving_avg_obs_fake_quant = torch.fused_moving_avg_obs_fake_quant(to, activation_post_process_0_observer_enabled, activation_post_process_0_fake_quant_enabled, activation_post_process_0_activation_post_process_min_val, activation_post_process_0_activation_post_process_max_val, activation_post_process_0_scale, activation_post_process_0_zero_point, 0.01, 0, 127, -1, False, False);  to = activation_post_process_0_observer_enabled = activation_post_process_0_fake_quant_enabled = activation_post_process_0_activation_post_process_min_val = activation_post_process_0_activation_post_process_max_val = activation_post_process_0_scale = activation_post_process_0_zero_point = None\n",
      "    activation_post_process_0_scale_0 = self.activation_post_process_0_scale_0\n",
      "    activation_post_process_0_zero_point_0 = self.activation_post_process_0_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant, activation_post_process_0_scale_0, activation_post_process_0_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant = activation_post_process_0_scale_0 = activation_post_process_0_zero_point_0 = None\n",
      "    conv1_0 = getattr(self.conv1, \"0\")(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    dequantize_1 = conv1_0.dequantize();  conv1_0 = None\n",
      "    activation_post_process_1_observer_enabled = self.activation_post_process_1.observer_enabled\n",
      "    activation_post_process_1_fake_quant_enabled = self.activation_post_process_1.fake_quant_enabled\n",
      "    activation_post_process_1_activation_post_process_min_val = self.activation_post_process_1.activation_post_process.min_val\n",
      "    activation_post_process_1_activation_post_process_max_val = self.activation_post_process_1.activation_post_process.max_val\n",
      "    activation_post_process_1_scale = self.activation_post_process_1.scale\n",
      "    activation_post_process_1_zero_point = self.activation_post_process_1.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_1 = torch.fused_moving_avg_obs_fake_quant(dequantize_1, activation_post_process_1_observer_enabled, activation_post_process_1_fake_quant_enabled, activation_post_process_1_activation_post_process_min_val, activation_post_process_1_activation_post_process_max_val, activation_post_process_1_scale, activation_post_process_1_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_1 = activation_post_process_1_observer_enabled = activation_post_process_1_fake_quant_enabled = activation_post_process_1_activation_post_process_min_val = activation_post_process_1_activation_post_process_max_val = activation_post_process_1_scale = activation_post_process_1_zero_point = None\n",
      "    activation_post_process_1_scale_0 = self.activation_post_process_1_scale_0\n",
      "    activation_post_process_1_zero_point_0 = self.activation_post_process_1_zero_point_0\n",
      "    quantize_per_tensor_2 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_1, activation_post_process_1_scale_0, activation_post_process_1_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_1 = activation_post_process_1_scale_0 = activation_post_process_1_zero_point_0 = None\n",
      "    conv1_2 = getattr(self.conv1, \"2\")(quantize_per_tensor_2);  quantize_per_tensor_2 = None\n",
      "    dequantize_3 = conv1_2.dequantize();  conv1_2 = None\n",
      "    activation_post_process_2_observer_enabled = self.activation_post_process_2.observer_enabled\n",
      "    activation_post_process_2_fake_quant_enabled = self.activation_post_process_2.fake_quant_enabled\n",
      "    activation_post_process_2_activation_post_process_min_val = self.activation_post_process_2.activation_post_process.min_val\n",
      "    activation_post_process_2_activation_post_process_max_val = self.activation_post_process_2.activation_post_process.max_val\n",
      "    activation_post_process_2_scale = self.activation_post_process_2.scale\n",
      "    activation_post_process_2_zero_point = self.activation_post_process_2.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_2 = torch.fused_moving_avg_obs_fake_quant(dequantize_3, activation_post_process_2_observer_enabled, activation_post_process_2_fake_quant_enabled, activation_post_process_2_activation_post_process_min_val, activation_post_process_2_activation_post_process_max_val, activation_post_process_2_scale, activation_post_process_2_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_3 = activation_post_process_2_observer_enabled = activation_post_process_2_fake_quant_enabled = activation_post_process_2_activation_post_process_min_val = activation_post_process_2_activation_post_process_max_val = activation_post_process_2_scale = activation_post_process_2_zero_point = None\n",
      "    activation_post_process_2_scale_0 = self.activation_post_process_2_scale_0\n",
      "    activation_post_process_2_zero_point_0 = self.activation_post_process_2_zero_point_0\n",
      "    quantize_per_tensor_4 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_2, activation_post_process_2_scale_0, activation_post_process_2_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_2 = activation_post_process_2_scale_0 = activation_post_process_2_zero_point_0 = None\n",
      "    conv2_0 = getattr(self.conv2, \"0\")(quantize_per_tensor_4);  quantize_per_tensor_4 = None\n",
      "    dequantize_5 = conv2_0.dequantize();  conv2_0 = None\n",
      "    activation_post_process_3_observer_enabled = self.activation_post_process_3.observer_enabled\n",
      "    activation_post_process_3_fake_quant_enabled = self.activation_post_process_3.fake_quant_enabled\n",
      "    activation_post_process_3_activation_post_process_min_val = self.activation_post_process_3.activation_post_process.min_val\n",
      "    activation_post_process_3_activation_post_process_max_val = self.activation_post_process_3.activation_post_process.max_val\n",
      "    activation_post_process_3_scale = self.activation_post_process_3.scale\n",
      "    activation_post_process_3_zero_point = self.activation_post_process_3.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_3 = torch.fused_moving_avg_obs_fake_quant(dequantize_5, activation_post_process_3_observer_enabled, activation_post_process_3_fake_quant_enabled, activation_post_process_3_activation_post_process_min_val, activation_post_process_3_activation_post_process_max_val, activation_post_process_3_scale, activation_post_process_3_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_5 = activation_post_process_3_observer_enabled = activation_post_process_3_fake_quant_enabled = activation_post_process_3_activation_post_process_min_val = activation_post_process_3_activation_post_process_max_val = activation_post_process_3_scale = activation_post_process_3_zero_point = None\n",
      "    activation_post_process_3_scale_0 = self.activation_post_process_3_scale_0\n",
      "    activation_post_process_3_zero_point_0 = self.activation_post_process_3_zero_point_0\n",
      "    quantize_per_tensor_6 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_3, activation_post_process_3_scale_0, activation_post_process_3_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_3 = activation_post_process_3_scale_0 = activation_post_process_3_zero_point_0 = None\n",
      "    conv2_2 = getattr(self.conv2, \"2\")(quantize_per_tensor_6);  quantize_per_tensor_6 = None\n",
      "    dequantize_7 = conv2_2.dequantize();  conv2_2 = None\n",
      "    activation_post_process_4_observer_enabled = self.activation_post_process_4.observer_enabled\n",
      "    activation_post_process_4_fake_quant_enabled = self.activation_post_process_4.fake_quant_enabled\n",
      "    activation_post_process_4_activation_post_process_min_val = self.activation_post_process_4.activation_post_process.min_val\n",
      "    activation_post_process_4_activation_post_process_max_val = self.activation_post_process_4.activation_post_process.max_val\n",
      "    activation_post_process_4_scale = self.activation_post_process_4.scale\n",
      "    activation_post_process_4_zero_point = self.activation_post_process_4.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_4 = torch.fused_moving_avg_obs_fake_quant(dequantize_7, activation_post_process_4_observer_enabled, activation_post_process_4_fake_quant_enabled, activation_post_process_4_activation_post_process_min_val, activation_post_process_4_activation_post_process_max_val, activation_post_process_4_scale, activation_post_process_4_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_7 = activation_post_process_4_observer_enabled = activation_post_process_4_fake_quant_enabled = activation_post_process_4_activation_post_process_min_val = activation_post_process_4_activation_post_process_max_val = activation_post_process_4_scale = activation_post_process_4_zero_point = None\n",
      "    activation_post_process_4_scale_0 = self.activation_post_process_4_scale_0\n",
      "    activation_post_process_4_zero_point_0 = self.activation_post_process_4_zero_point_0\n",
      "    quantize_per_tensor_8 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_4, activation_post_process_4_scale_0, activation_post_process_4_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_4 = activation_post_process_4_scale_0 = activation_post_process_4_zero_point_0 = None\n",
      "    conv2_3 = getattr(self.conv2, \"3\")(quantize_per_tensor_8);  quantize_per_tensor_8 = None\n",
      "    dequantize_9 = conv2_3.dequantize();  conv2_3 = None\n",
      "    activation_post_process_4_observer_enabled_1 = self.activation_post_process_4.observer_enabled\n",
      "    activation_post_process_4_fake_quant_enabled_1 = self.activation_post_process_4.fake_quant_enabled\n",
      "    activation_post_process_4_activation_post_process_min_val_1 = self.activation_post_process_4.activation_post_process.min_val\n",
      "    activation_post_process_4_activation_post_process_max_val_1 = self.activation_post_process_4.activation_post_process.max_val\n",
      "    activation_post_process_4_scale_1 = self.activation_post_process_4.scale\n",
      "    activation_post_process_4_zero_point_1 = self.activation_post_process_4.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_5 = torch.fused_moving_avg_obs_fake_quant(dequantize_9, activation_post_process_4_observer_enabled_1, activation_post_process_4_fake_quant_enabled_1, activation_post_process_4_activation_post_process_min_val_1, activation_post_process_4_activation_post_process_max_val_1, activation_post_process_4_scale_1, activation_post_process_4_zero_point_1, 0.01, 0, 127, -1, False, False);  dequantize_9 = activation_post_process_4_observer_enabled_1 = activation_post_process_4_fake_quant_enabled_1 = activation_post_process_4_activation_post_process_min_val_1 = activation_post_process_4_activation_post_process_max_val_1 = activation_post_process_4_scale_1 = activation_post_process_4_zero_point_1 = None\n",
      "    activation_post_process_4_scale_2 = self.activation_post_process_4_scale_1\n",
      "    activation_post_process_4_zero_point_2 = self.activation_post_process_4_zero_point_1\n",
      "    quantize_per_tensor_10 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_5, activation_post_process_4_scale_2, activation_post_process_4_zero_point_2, torch.quint8);  fused_moving_avg_obs_fake_quant_5 = activation_post_process_4_scale_2 = activation_post_process_4_zero_point_2 = None\n",
      "    res1_0_0 = getattr(getattr(self.res1, \"0\"), \"0\")(quantize_per_tensor_10)\n",
      "    dequantize_11 = res1_0_0.dequantize();  res1_0_0 = None\n",
      "    activation_post_process_6_observer_enabled = self.activation_post_process_6.observer_enabled\n",
      "    activation_post_process_6_fake_quant_enabled = self.activation_post_process_6.fake_quant_enabled\n",
      "    activation_post_process_6_activation_post_process_min_val = self.activation_post_process_6.activation_post_process.min_val\n",
      "    activation_post_process_6_activation_post_process_max_val = self.activation_post_process_6.activation_post_process.max_val\n",
      "    activation_post_process_6_scale = self.activation_post_process_6.scale\n",
      "    activation_post_process_6_zero_point = self.activation_post_process_6.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_6 = torch.fused_moving_avg_obs_fake_quant(dequantize_11, activation_post_process_6_observer_enabled, activation_post_process_6_fake_quant_enabled, activation_post_process_6_activation_post_process_min_val, activation_post_process_6_activation_post_process_max_val, activation_post_process_6_scale, activation_post_process_6_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_11 = activation_post_process_6_observer_enabled = activation_post_process_6_fake_quant_enabled = activation_post_process_6_activation_post_process_min_val = activation_post_process_6_activation_post_process_max_val = activation_post_process_6_scale = activation_post_process_6_zero_point = None\n",
      "    activation_post_process_6_scale_0 = self.activation_post_process_6_scale_0\n",
      "    activation_post_process_6_zero_point_0 = self.activation_post_process_6_zero_point_0\n",
      "    quantize_per_tensor_12 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_6, activation_post_process_6_scale_0, activation_post_process_6_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_6 = activation_post_process_6_scale_0 = activation_post_process_6_zero_point_0 = None\n",
      "    res1_0_2 = getattr(getattr(self.res1, \"0\"), \"2\")(quantize_per_tensor_12);  quantize_per_tensor_12 = None\n",
      "    dequantize_13 = res1_0_2.dequantize();  res1_0_2 = None\n",
      "    activation_post_process_7_observer_enabled = self.activation_post_process_7.observer_enabled\n",
      "    activation_post_process_7_fake_quant_enabled = self.activation_post_process_7.fake_quant_enabled\n",
      "    activation_post_process_7_activation_post_process_min_val = self.activation_post_process_7.activation_post_process.min_val\n",
      "    activation_post_process_7_activation_post_process_max_val = self.activation_post_process_7.activation_post_process.max_val\n",
      "    activation_post_process_7_scale = self.activation_post_process_7.scale\n",
      "    activation_post_process_7_zero_point = self.activation_post_process_7.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_7 = torch.fused_moving_avg_obs_fake_quant(dequantize_13, activation_post_process_7_observer_enabled, activation_post_process_7_fake_quant_enabled, activation_post_process_7_activation_post_process_min_val, activation_post_process_7_activation_post_process_max_val, activation_post_process_7_scale, activation_post_process_7_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_13 = activation_post_process_7_observer_enabled = activation_post_process_7_fake_quant_enabled = activation_post_process_7_activation_post_process_min_val = activation_post_process_7_activation_post_process_max_val = activation_post_process_7_scale = activation_post_process_7_zero_point = None\n",
      "    activation_post_process_7_scale_0 = self.activation_post_process_7_scale_0\n",
      "    activation_post_process_7_zero_point_0 = self.activation_post_process_7_zero_point_0\n",
      "    quantize_per_tensor_14 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_7, activation_post_process_7_scale_0, activation_post_process_7_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_7 = activation_post_process_7_scale_0 = activation_post_process_7_zero_point_0 = None\n",
      "    res1_1_0 = getattr(getattr(self.res1, \"1\"), \"0\")(quantize_per_tensor_14);  quantize_per_tensor_14 = None\n",
      "    dequantize_15 = res1_1_0.dequantize();  res1_1_0 = None\n",
      "    activation_post_process_8_observer_enabled = self.activation_post_process_8.observer_enabled\n",
      "    activation_post_process_8_fake_quant_enabled = self.activation_post_process_8.fake_quant_enabled\n",
      "    activation_post_process_8_activation_post_process_min_val = self.activation_post_process_8.activation_post_process.min_val\n",
      "    activation_post_process_8_activation_post_process_max_val = self.activation_post_process_8.activation_post_process.max_val\n",
      "    activation_post_process_8_scale = self.activation_post_process_8.scale\n",
      "    activation_post_process_8_zero_point = self.activation_post_process_8.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_8 = torch.fused_moving_avg_obs_fake_quant(dequantize_15, activation_post_process_8_observer_enabled, activation_post_process_8_fake_quant_enabled, activation_post_process_8_activation_post_process_min_val, activation_post_process_8_activation_post_process_max_val, activation_post_process_8_scale, activation_post_process_8_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_15 = activation_post_process_8_observer_enabled = activation_post_process_8_fake_quant_enabled = activation_post_process_8_activation_post_process_min_val = activation_post_process_8_activation_post_process_max_val = activation_post_process_8_scale = activation_post_process_8_zero_point = None\n",
      "    activation_post_process_8_scale_0 = self.activation_post_process_8_scale_0\n",
      "    activation_post_process_8_zero_point_0 = self.activation_post_process_8_zero_point_0\n",
      "    quantize_per_tensor_16 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_8, activation_post_process_8_scale_0, activation_post_process_8_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_8 = activation_post_process_8_scale_0 = activation_post_process_8_zero_point_0 = None\n",
      "    res1_1_2 = getattr(getattr(self.res1, \"1\"), \"2\")(quantize_per_tensor_16);  quantize_per_tensor_16 = None\n",
      "    dequantize_17 = res1_1_2.dequantize();  res1_1_2 = None\n",
      "    activation_post_process_9_observer_enabled = self.activation_post_process_9.observer_enabled\n",
      "    activation_post_process_9_fake_quant_enabled = self.activation_post_process_9.fake_quant_enabled\n",
      "    activation_post_process_9_activation_post_process_min_val = self.activation_post_process_9.activation_post_process.min_val\n",
      "    activation_post_process_9_activation_post_process_max_val = self.activation_post_process_9.activation_post_process.max_val\n",
      "    activation_post_process_9_scale = self.activation_post_process_9.scale\n",
      "    activation_post_process_9_zero_point = self.activation_post_process_9.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_9 = torch.fused_moving_avg_obs_fake_quant(dequantize_17, activation_post_process_9_observer_enabled, activation_post_process_9_fake_quant_enabled, activation_post_process_9_activation_post_process_min_val, activation_post_process_9_activation_post_process_max_val, activation_post_process_9_scale, activation_post_process_9_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_17 = activation_post_process_9_observer_enabled = activation_post_process_9_fake_quant_enabled = activation_post_process_9_activation_post_process_min_val = activation_post_process_9_activation_post_process_max_val = activation_post_process_9_scale = activation_post_process_9_zero_point = None\n",
      "    activation_post_process_9_scale_0 = self.activation_post_process_9_scale_0\n",
      "    activation_post_process_9_zero_point_0 = self.activation_post_process_9_zero_point_0\n",
      "    quantize_per_tensor_18 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_9, activation_post_process_9_scale_0, activation_post_process_9_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_9 = activation_post_process_9_scale_0 = activation_post_process_9_zero_point_0 = None\n",
      "    _scale_0 = self._scale_0\n",
      "    _zero_point_0 = self._zero_point_0\n",
      "    add_3 = torch.ops.quantized.add(quantize_per_tensor_18, quantize_per_tensor_10, _scale_0, _zero_point_0);  quantize_per_tensor_18 = quantize_per_tensor_10 = _scale_0 = _zero_point_0 = None\n",
      "    dequantize_19 = add_3.dequantize();  add_3 = None\n",
      "    activation_post_process_10_observer_enabled = self.activation_post_process_10.observer_enabled\n",
      "    activation_post_process_10_fake_quant_enabled = self.activation_post_process_10.fake_quant_enabled\n",
      "    activation_post_process_10_activation_post_process_min_val = self.activation_post_process_10.activation_post_process.min_val\n",
      "    activation_post_process_10_activation_post_process_max_val = self.activation_post_process_10.activation_post_process.max_val\n",
      "    activation_post_process_10_scale = self.activation_post_process_10.scale\n",
      "    activation_post_process_10_zero_point = self.activation_post_process_10.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_10 = torch.fused_moving_avg_obs_fake_quant(dequantize_19, activation_post_process_10_observer_enabled, activation_post_process_10_fake_quant_enabled, activation_post_process_10_activation_post_process_min_val, activation_post_process_10_activation_post_process_max_val, activation_post_process_10_scale, activation_post_process_10_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_19 = activation_post_process_10_observer_enabled = activation_post_process_10_fake_quant_enabled = activation_post_process_10_activation_post_process_min_val = activation_post_process_10_activation_post_process_max_val = activation_post_process_10_scale = activation_post_process_10_zero_point = None\n",
      "    activation_post_process_10_scale_0 = self.activation_post_process_10_scale_0\n",
      "    activation_post_process_10_zero_point_0 = self.activation_post_process_10_zero_point_0\n",
      "    quantize_per_tensor_20 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_10, activation_post_process_10_scale_0, activation_post_process_10_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_10 = activation_post_process_10_scale_0 = activation_post_process_10_zero_point_0 = None\n",
      "    drop1 = self.drop1(quantize_per_tensor_20);  quantize_per_tensor_20 = None\n",
      "    dequantize_21 = drop1.dequantize();  drop1 = None\n",
      "    activation_post_process_11_observer_enabled = self.activation_post_process_11.observer_enabled\n",
      "    activation_post_process_11_fake_quant_enabled = self.activation_post_process_11.fake_quant_enabled\n",
      "    activation_post_process_11_activation_post_process_min_val = self.activation_post_process_11.activation_post_process.min_val\n",
      "    activation_post_process_11_activation_post_process_max_val = self.activation_post_process_11.activation_post_process.max_val\n",
      "    activation_post_process_11_scale = self.activation_post_process_11.scale\n",
      "    activation_post_process_11_zero_point = self.activation_post_process_11.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_11 = torch.fused_moving_avg_obs_fake_quant(dequantize_21, activation_post_process_11_observer_enabled, activation_post_process_11_fake_quant_enabled, activation_post_process_11_activation_post_process_min_val, activation_post_process_11_activation_post_process_max_val, activation_post_process_11_scale, activation_post_process_11_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_21 = activation_post_process_11_observer_enabled = activation_post_process_11_fake_quant_enabled = activation_post_process_11_activation_post_process_min_val = activation_post_process_11_activation_post_process_max_val = activation_post_process_11_scale = activation_post_process_11_zero_point = None\n",
      "    activation_post_process_11_scale_0 = self.activation_post_process_11_scale_0\n",
      "    activation_post_process_11_zero_point_0 = self.activation_post_process_11_zero_point_0\n",
      "    quantize_per_tensor_22 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_11, activation_post_process_11_scale_0, activation_post_process_11_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_11 = activation_post_process_11_scale_0 = activation_post_process_11_zero_point_0 = None\n",
      "    conv3_0 = getattr(self.conv3, \"0\")(quantize_per_tensor_22);  quantize_per_tensor_22 = None\n",
      "    dequantize_23 = conv3_0.dequantize();  conv3_0 = None\n",
      "    activation_post_process_12_observer_enabled = self.activation_post_process_12.observer_enabled\n",
      "    activation_post_process_12_fake_quant_enabled = self.activation_post_process_12.fake_quant_enabled\n",
      "    activation_post_process_12_activation_post_process_min_val = self.activation_post_process_12.activation_post_process.min_val\n",
      "    activation_post_process_12_activation_post_process_max_val = self.activation_post_process_12.activation_post_process.max_val\n",
      "    activation_post_process_12_scale = self.activation_post_process_12.scale\n",
      "    activation_post_process_12_zero_point = self.activation_post_process_12.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_12 = torch.fused_moving_avg_obs_fake_quant(dequantize_23, activation_post_process_12_observer_enabled, activation_post_process_12_fake_quant_enabled, activation_post_process_12_activation_post_process_min_val, activation_post_process_12_activation_post_process_max_val, activation_post_process_12_scale, activation_post_process_12_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_23 = activation_post_process_12_observer_enabled = activation_post_process_12_fake_quant_enabled = activation_post_process_12_activation_post_process_min_val = activation_post_process_12_activation_post_process_max_val = activation_post_process_12_scale = activation_post_process_12_zero_point = None\n",
      "    activation_post_process_12_scale_0 = self.activation_post_process_12_scale_0\n",
      "    activation_post_process_12_zero_point_0 = self.activation_post_process_12_zero_point_0\n",
      "    quantize_per_tensor_24 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_12, activation_post_process_12_scale_0, activation_post_process_12_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_12 = activation_post_process_12_scale_0 = activation_post_process_12_zero_point_0 = None\n",
      "    conv3_2 = getattr(self.conv3, \"2\")(quantize_per_tensor_24);  quantize_per_tensor_24 = None\n",
      "    dequantize_25 = conv3_2.dequantize();  conv3_2 = None\n",
      "    activation_post_process_13_observer_enabled = self.activation_post_process_13.observer_enabled\n",
      "    activation_post_process_13_fake_quant_enabled = self.activation_post_process_13.fake_quant_enabled\n",
      "    activation_post_process_13_activation_post_process_min_val = self.activation_post_process_13.activation_post_process.min_val\n",
      "    activation_post_process_13_activation_post_process_max_val = self.activation_post_process_13.activation_post_process.max_val\n",
      "    activation_post_process_13_scale = self.activation_post_process_13.scale\n",
      "    activation_post_process_13_zero_point = self.activation_post_process_13.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_13 = torch.fused_moving_avg_obs_fake_quant(dequantize_25, activation_post_process_13_observer_enabled, activation_post_process_13_fake_quant_enabled, activation_post_process_13_activation_post_process_min_val, activation_post_process_13_activation_post_process_max_val, activation_post_process_13_scale, activation_post_process_13_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_25 = activation_post_process_13_observer_enabled = activation_post_process_13_fake_quant_enabled = activation_post_process_13_activation_post_process_min_val = activation_post_process_13_activation_post_process_max_val = activation_post_process_13_scale = activation_post_process_13_zero_point = None\n",
      "    activation_post_process_13_scale_0 = self.activation_post_process_13_scale_0\n",
      "    activation_post_process_13_zero_point_0 = self.activation_post_process_13_zero_point_0\n",
      "    quantize_per_tensor_26 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_13, activation_post_process_13_scale_0, activation_post_process_13_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_13 = activation_post_process_13_scale_0 = activation_post_process_13_zero_point_0 = None\n",
      "    conv4_0 = getattr(self.conv4, \"0\")(quantize_per_tensor_26);  quantize_per_tensor_26 = None\n",
      "    dequantize_27 = conv4_0.dequantize();  conv4_0 = None\n",
      "    activation_post_process_14_observer_enabled = self.activation_post_process_14.observer_enabled\n",
      "    activation_post_process_14_fake_quant_enabled = self.activation_post_process_14.fake_quant_enabled\n",
      "    activation_post_process_14_activation_post_process_min_val = self.activation_post_process_14.activation_post_process.min_val\n",
      "    activation_post_process_14_activation_post_process_max_val = self.activation_post_process_14.activation_post_process.max_val\n",
      "    activation_post_process_14_scale = self.activation_post_process_14.scale\n",
      "    activation_post_process_14_zero_point = self.activation_post_process_14.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_14 = torch.fused_moving_avg_obs_fake_quant(dequantize_27, activation_post_process_14_observer_enabled, activation_post_process_14_fake_quant_enabled, activation_post_process_14_activation_post_process_min_val, activation_post_process_14_activation_post_process_max_val, activation_post_process_14_scale, activation_post_process_14_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_27 = activation_post_process_14_observer_enabled = activation_post_process_14_fake_quant_enabled = activation_post_process_14_activation_post_process_min_val = activation_post_process_14_activation_post_process_max_val = activation_post_process_14_scale = activation_post_process_14_zero_point = None\n",
      "    activation_post_process_14_scale_0 = self.activation_post_process_14_scale_0\n",
      "    activation_post_process_14_zero_point_0 = self.activation_post_process_14_zero_point_0\n",
      "    quantize_per_tensor_28 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_14, activation_post_process_14_scale_0, activation_post_process_14_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_14 = activation_post_process_14_scale_0 = activation_post_process_14_zero_point_0 = None\n",
      "    conv4_2 = getattr(self.conv4, \"2\")(quantize_per_tensor_28);  quantize_per_tensor_28 = None\n",
      "    dequantize_29 = conv4_2.dequantize();  conv4_2 = None\n",
      "    activation_post_process_15_observer_enabled = self.activation_post_process_15.observer_enabled\n",
      "    activation_post_process_15_fake_quant_enabled = self.activation_post_process_15.fake_quant_enabled\n",
      "    activation_post_process_15_activation_post_process_min_val = self.activation_post_process_15.activation_post_process.min_val\n",
      "    activation_post_process_15_activation_post_process_max_val = self.activation_post_process_15.activation_post_process.max_val\n",
      "    activation_post_process_15_scale = self.activation_post_process_15.scale\n",
      "    activation_post_process_15_zero_point = self.activation_post_process_15.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_15 = torch.fused_moving_avg_obs_fake_quant(dequantize_29, activation_post_process_15_observer_enabled, activation_post_process_15_fake_quant_enabled, activation_post_process_15_activation_post_process_min_val, activation_post_process_15_activation_post_process_max_val, activation_post_process_15_scale, activation_post_process_15_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_29 = activation_post_process_15_observer_enabled = activation_post_process_15_fake_quant_enabled = activation_post_process_15_activation_post_process_min_val = activation_post_process_15_activation_post_process_max_val = activation_post_process_15_scale = activation_post_process_15_zero_point = None\n",
      "    activation_post_process_15_scale_0 = self.activation_post_process_15_scale_0\n",
      "    activation_post_process_15_zero_point_0 = self.activation_post_process_15_zero_point_0\n",
      "    quantize_per_tensor_30 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_15, activation_post_process_15_scale_0, activation_post_process_15_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_15 = activation_post_process_15_scale_0 = activation_post_process_15_zero_point_0 = None\n",
      "    conv4_3 = getattr(self.conv4, \"3\")(quantize_per_tensor_30);  quantize_per_tensor_30 = None\n",
      "    dequantize_31 = conv4_3.dequantize();  conv4_3 = None\n",
      "    activation_post_process_15_observer_enabled_1 = self.activation_post_process_15.observer_enabled\n",
      "    activation_post_process_15_fake_quant_enabled_1 = self.activation_post_process_15.fake_quant_enabled\n",
      "    activation_post_process_15_activation_post_process_min_val_1 = self.activation_post_process_15.activation_post_process.min_val\n",
      "    activation_post_process_15_activation_post_process_max_val_1 = self.activation_post_process_15.activation_post_process.max_val\n",
      "    activation_post_process_15_scale_1 = self.activation_post_process_15.scale\n",
      "    activation_post_process_15_zero_point_1 = self.activation_post_process_15.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_16 = torch.fused_moving_avg_obs_fake_quant(dequantize_31, activation_post_process_15_observer_enabled_1, activation_post_process_15_fake_quant_enabled_1, activation_post_process_15_activation_post_process_min_val_1, activation_post_process_15_activation_post_process_max_val_1, activation_post_process_15_scale_1, activation_post_process_15_zero_point_1, 0.01, 0, 127, -1, False, False);  dequantize_31 = activation_post_process_15_observer_enabled_1 = activation_post_process_15_fake_quant_enabled_1 = activation_post_process_15_activation_post_process_min_val_1 = activation_post_process_15_activation_post_process_max_val_1 = activation_post_process_15_scale_1 = activation_post_process_15_zero_point_1 = None\n",
      "    activation_post_process_15_scale_2 = self.activation_post_process_15_scale_1\n",
      "    activation_post_process_15_zero_point_2 = self.activation_post_process_15_zero_point_1\n",
      "    quantize_per_tensor_32 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_16, activation_post_process_15_scale_2, activation_post_process_15_zero_point_2, torch.quint8);  fused_moving_avg_obs_fake_quant_16 = activation_post_process_15_scale_2 = activation_post_process_15_zero_point_2 = None\n",
      "    res2_0_0 = getattr(getattr(self.res2, \"0\"), \"0\")(quantize_per_tensor_32)\n",
      "    dequantize_33 = res2_0_0.dequantize();  res2_0_0 = None\n",
      "    activation_post_process_17_observer_enabled = self.activation_post_process_17.observer_enabled\n",
      "    activation_post_process_17_fake_quant_enabled = self.activation_post_process_17.fake_quant_enabled\n",
      "    activation_post_process_17_activation_post_process_min_val = self.activation_post_process_17.activation_post_process.min_val\n",
      "    activation_post_process_17_activation_post_process_max_val = self.activation_post_process_17.activation_post_process.max_val\n",
      "    activation_post_process_17_scale = self.activation_post_process_17.scale\n",
      "    activation_post_process_17_zero_point = self.activation_post_process_17.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_17 = torch.fused_moving_avg_obs_fake_quant(dequantize_33, activation_post_process_17_observer_enabled, activation_post_process_17_fake_quant_enabled, activation_post_process_17_activation_post_process_min_val, activation_post_process_17_activation_post_process_max_val, activation_post_process_17_scale, activation_post_process_17_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_33 = activation_post_process_17_observer_enabled = activation_post_process_17_fake_quant_enabled = activation_post_process_17_activation_post_process_min_val = activation_post_process_17_activation_post_process_max_val = activation_post_process_17_scale = activation_post_process_17_zero_point = None\n",
      "    activation_post_process_17_scale_0 = self.activation_post_process_17_scale_0\n",
      "    activation_post_process_17_zero_point_0 = self.activation_post_process_17_zero_point_0\n",
      "    quantize_per_tensor_34 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_17, activation_post_process_17_scale_0, activation_post_process_17_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_17 = activation_post_process_17_scale_0 = activation_post_process_17_zero_point_0 = None\n",
      "    res2_0_2 = getattr(getattr(self.res2, \"0\"), \"2\")(quantize_per_tensor_34);  quantize_per_tensor_34 = None\n",
      "    dequantize_35 = res2_0_2.dequantize();  res2_0_2 = None\n",
      "    activation_post_process_18_observer_enabled = self.activation_post_process_18.observer_enabled\n",
      "    activation_post_process_18_fake_quant_enabled = self.activation_post_process_18.fake_quant_enabled\n",
      "    activation_post_process_18_activation_post_process_min_val = self.activation_post_process_18.activation_post_process.min_val\n",
      "    activation_post_process_18_activation_post_process_max_val = self.activation_post_process_18.activation_post_process.max_val\n",
      "    activation_post_process_18_scale = self.activation_post_process_18.scale\n",
      "    activation_post_process_18_zero_point = self.activation_post_process_18.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_18 = torch.fused_moving_avg_obs_fake_quant(dequantize_35, activation_post_process_18_observer_enabled, activation_post_process_18_fake_quant_enabled, activation_post_process_18_activation_post_process_min_val, activation_post_process_18_activation_post_process_max_val, activation_post_process_18_scale, activation_post_process_18_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_35 = activation_post_process_18_observer_enabled = activation_post_process_18_fake_quant_enabled = activation_post_process_18_activation_post_process_min_val = activation_post_process_18_activation_post_process_max_val = activation_post_process_18_scale = activation_post_process_18_zero_point = None\n",
      "    activation_post_process_18_scale_0 = self.activation_post_process_18_scale_0\n",
      "    activation_post_process_18_zero_point_0 = self.activation_post_process_18_zero_point_0\n",
      "    quantize_per_tensor_36 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_18, activation_post_process_18_scale_0, activation_post_process_18_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_18 = activation_post_process_18_scale_0 = activation_post_process_18_zero_point_0 = None\n",
      "    res2_1_0 = getattr(getattr(self.res2, \"1\"), \"0\")(quantize_per_tensor_36);  quantize_per_tensor_36 = None\n",
      "    dequantize_37 = res2_1_0.dequantize();  res2_1_0 = None\n",
      "    activation_post_process_19_observer_enabled = self.activation_post_process_19.observer_enabled\n",
      "    activation_post_process_19_fake_quant_enabled = self.activation_post_process_19.fake_quant_enabled\n",
      "    activation_post_process_19_activation_post_process_min_val = self.activation_post_process_19.activation_post_process.min_val\n",
      "    activation_post_process_19_activation_post_process_max_val = self.activation_post_process_19.activation_post_process.max_val\n",
      "    activation_post_process_19_scale = self.activation_post_process_19.scale\n",
      "    activation_post_process_19_zero_point = self.activation_post_process_19.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_19 = torch.fused_moving_avg_obs_fake_quant(dequantize_37, activation_post_process_19_observer_enabled, activation_post_process_19_fake_quant_enabled, activation_post_process_19_activation_post_process_min_val, activation_post_process_19_activation_post_process_max_val, activation_post_process_19_scale, activation_post_process_19_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_37 = activation_post_process_19_observer_enabled = activation_post_process_19_fake_quant_enabled = activation_post_process_19_activation_post_process_min_val = activation_post_process_19_activation_post_process_max_val = activation_post_process_19_scale = activation_post_process_19_zero_point = None\n",
      "    activation_post_process_19_scale_0 = self.activation_post_process_19_scale_0\n",
      "    activation_post_process_19_zero_point_0 = self.activation_post_process_19_zero_point_0\n",
      "    quantize_per_tensor_38 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_19, activation_post_process_19_scale_0, activation_post_process_19_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_19 = activation_post_process_19_scale_0 = activation_post_process_19_zero_point_0 = None\n",
      "    res2_1_2 = getattr(getattr(self.res2, \"1\"), \"2\")(quantize_per_tensor_38);  quantize_per_tensor_38 = None\n",
      "    dequantize_39 = res2_1_2.dequantize();  res2_1_2 = None\n",
      "    activation_post_process_20_observer_enabled = self.activation_post_process_20.observer_enabled\n",
      "    activation_post_process_20_fake_quant_enabled = self.activation_post_process_20.fake_quant_enabled\n",
      "    activation_post_process_20_activation_post_process_min_val = self.activation_post_process_20.activation_post_process.min_val\n",
      "    activation_post_process_20_activation_post_process_max_val = self.activation_post_process_20.activation_post_process.max_val\n",
      "    activation_post_process_20_scale = self.activation_post_process_20.scale\n",
      "    activation_post_process_20_zero_point = self.activation_post_process_20.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_20 = torch.fused_moving_avg_obs_fake_quant(dequantize_39, activation_post_process_20_observer_enabled, activation_post_process_20_fake_quant_enabled, activation_post_process_20_activation_post_process_min_val, activation_post_process_20_activation_post_process_max_val, activation_post_process_20_scale, activation_post_process_20_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_39 = activation_post_process_20_observer_enabled = activation_post_process_20_fake_quant_enabled = activation_post_process_20_activation_post_process_min_val = activation_post_process_20_activation_post_process_max_val = activation_post_process_20_scale = activation_post_process_20_zero_point = None\n",
      "    activation_post_process_20_scale_0 = self.activation_post_process_20_scale_0\n",
      "    activation_post_process_20_zero_point_0 = self.activation_post_process_20_zero_point_0\n",
      "    quantize_per_tensor_40 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_20, activation_post_process_20_scale_0, activation_post_process_20_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_20 = activation_post_process_20_scale_0 = activation_post_process_20_zero_point_0 = None\n",
      "    _scale_1 = self._scale_1\n",
      "    _zero_point_1 = self._zero_point_1\n",
      "    add_4 = torch.ops.quantized.add(quantize_per_tensor_40, quantize_per_tensor_32, _scale_1, _zero_point_1);  quantize_per_tensor_40 = quantize_per_tensor_32 = _scale_1 = _zero_point_1 = None\n",
      "    dequantize_41 = add_4.dequantize();  add_4 = None\n",
      "    activation_post_process_21_observer_enabled = self.activation_post_process_21.observer_enabled\n",
      "    activation_post_process_21_fake_quant_enabled = self.activation_post_process_21.fake_quant_enabled\n",
      "    activation_post_process_21_activation_post_process_min_val = self.activation_post_process_21.activation_post_process.min_val\n",
      "    activation_post_process_21_activation_post_process_max_val = self.activation_post_process_21.activation_post_process.max_val\n",
      "    activation_post_process_21_scale = self.activation_post_process_21.scale\n",
      "    activation_post_process_21_zero_point = self.activation_post_process_21.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_21 = torch.fused_moving_avg_obs_fake_quant(dequantize_41, activation_post_process_21_observer_enabled, activation_post_process_21_fake_quant_enabled, activation_post_process_21_activation_post_process_min_val, activation_post_process_21_activation_post_process_max_val, activation_post_process_21_scale, activation_post_process_21_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_41 = activation_post_process_21_observer_enabled = activation_post_process_21_fake_quant_enabled = activation_post_process_21_activation_post_process_min_val = activation_post_process_21_activation_post_process_max_val = activation_post_process_21_scale = activation_post_process_21_zero_point = None\n",
      "    activation_post_process_21_scale_0 = self.activation_post_process_21_scale_0\n",
      "    activation_post_process_21_zero_point_0 = self.activation_post_process_21_zero_point_0\n",
      "    quantize_per_tensor_42 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_21, activation_post_process_21_scale_0, activation_post_process_21_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_21 = activation_post_process_21_scale_0 = activation_post_process_21_zero_point_0 = None\n",
      "    drop2 = self.drop2(quantize_per_tensor_42);  quantize_per_tensor_42 = None\n",
      "    dequantize_43 = drop2.dequantize();  drop2 = None\n",
      "    activation_post_process_22_observer_enabled = self.activation_post_process_22.observer_enabled\n",
      "    activation_post_process_22_fake_quant_enabled = self.activation_post_process_22.fake_quant_enabled\n",
      "    activation_post_process_22_activation_post_process_min_val = self.activation_post_process_22.activation_post_process.min_val\n",
      "    activation_post_process_22_activation_post_process_max_val = self.activation_post_process_22.activation_post_process.max_val\n",
      "    activation_post_process_22_scale = self.activation_post_process_22.scale\n",
      "    activation_post_process_22_zero_point = self.activation_post_process_22.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_22 = torch.fused_moving_avg_obs_fake_quant(dequantize_43, activation_post_process_22_observer_enabled, activation_post_process_22_fake_quant_enabled, activation_post_process_22_activation_post_process_min_val, activation_post_process_22_activation_post_process_max_val, activation_post_process_22_scale, activation_post_process_22_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_43 = activation_post_process_22_observer_enabled = activation_post_process_22_fake_quant_enabled = activation_post_process_22_activation_post_process_min_val = activation_post_process_22_activation_post_process_max_val = activation_post_process_22_scale = activation_post_process_22_zero_point = None\n",
      "    activation_post_process_22_scale_0 = self.activation_post_process_22_scale_0\n",
      "    activation_post_process_22_zero_point_0 = self.activation_post_process_22_zero_point_0\n",
      "    quantize_per_tensor_44 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_22, activation_post_process_22_scale_0, activation_post_process_22_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_22 = activation_post_process_22_scale_0 = activation_post_process_22_zero_point_0 = None\n",
      "    conv5_0 = getattr(self.conv5, \"0\")(quantize_per_tensor_44);  quantize_per_tensor_44 = None\n",
      "    dequantize_45 = conv5_0.dequantize();  conv5_0 = None\n",
      "    activation_post_process_23_observer_enabled = self.activation_post_process_23.observer_enabled\n",
      "    activation_post_process_23_fake_quant_enabled = self.activation_post_process_23.fake_quant_enabled\n",
      "    activation_post_process_23_activation_post_process_min_val = self.activation_post_process_23.activation_post_process.min_val\n",
      "    activation_post_process_23_activation_post_process_max_val = self.activation_post_process_23.activation_post_process.max_val\n",
      "    activation_post_process_23_scale = self.activation_post_process_23.scale\n",
      "    activation_post_process_23_zero_point = self.activation_post_process_23.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_23 = torch.fused_moving_avg_obs_fake_quant(dequantize_45, activation_post_process_23_observer_enabled, activation_post_process_23_fake_quant_enabled, activation_post_process_23_activation_post_process_min_val, activation_post_process_23_activation_post_process_max_val, activation_post_process_23_scale, activation_post_process_23_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_45 = activation_post_process_23_observer_enabled = activation_post_process_23_fake_quant_enabled = activation_post_process_23_activation_post_process_min_val = activation_post_process_23_activation_post_process_max_val = activation_post_process_23_scale = activation_post_process_23_zero_point = None\n",
      "    activation_post_process_23_scale_0 = self.activation_post_process_23_scale_0\n",
      "    activation_post_process_23_zero_point_0 = self.activation_post_process_23_zero_point_0\n",
      "    quantize_per_tensor_46 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_23, activation_post_process_23_scale_0, activation_post_process_23_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_23 = activation_post_process_23_scale_0 = activation_post_process_23_zero_point_0 = None\n",
      "    conv5_2 = getattr(self.conv5, \"2\")(quantize_per_tensor_46);  quantize_per_tensor_46 = None\n",
      "    dequantize_47 = conv5_2.dequantize();  conv5_2 = None\n",
      "    activation_post_process_24_observer_enabled = self.activation_post_process_24.observer_enabled\n",
      "    activation_post_process_24_fake_quant_enabled = self.activation_post_process_24.fake_quant_enabled\n",
      "    activation_post_process_24_activation_post_process_min_val = self.activation_post_process_24.activation_post_process.min_val\n",
      "    activation_post_process_24_activation_post_process_max_val = self.activation_post_process_24.activation_post_process.max_val\n",
      "    activation_post_process_24_scale = self.activation_post_process_24.scale\n",
      "    activation_post_process_24_zero_point = self.activation_post_process_24.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_24 = torch.fused_moving_avg_obs_fake_quant(dequantize_47, activation_post_process_24_observer_enabled, activation_post_process_24_fake_quant_enabled, activation_post_process_24_activation_post_process_min_val, activation_post_process_24_activation_post_process_max_val, activation_post_process_24_scale, activation_post_process_24_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_47 = activation_post_process_24_observer_enabled = activation_post_process_24_fake_quant_enabled = activation_post_process_24_activation_post_process_min_val = activation_post_process_24_activation_post_process_max_val = activation_post_process_24_scale = activation_post_process_24_zero_point = None\n",
      "    activation_post_process_24_scale_0 = self.activation_post_process_24_scale_0\n",
      "    activation_post_process_24_zero_point_0 = self.activation_post_process_24_zero_point_0\n",
      "    quantize_per_tensor_48 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_24, activation_post_process_24_scale_0, activation_post_process_24_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_24 = activation_post_process_24_scale_0 = activation_post_process_24_zero_point_0 = None\n",
      "    conv6_0 = getattr(self.conv6, \"0\")(quantize_per_tensor_48);  quantize_per_tensor_48 = None\n",
      "    dequantize_49 = conv6_0.dequantize();  conv6_0 = None\n",
      "    activation_post_process_25_observer_enabled = self.activation_post_process_25.observer_enabled\n",
      "    activation_post_process_25_fake_quant_enabled = self.activation_post_process_25.fake_quant_enabled\n",
      "    activation_post_process_25_activation_post_process_min_val = self.activation_post_process_25.activation_post_process.min_val\n",
      "    activation_post_process_25_activation_post_process_max_val = self.activation_post_process_25.activation_post_process.max_val\n",
      "    activation_post_process_25_scale = self.activation_post_process_25.scale\n",
      "    activation_post_process_25_zero_point = self.activation_post_process_25.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_25 = torch.fused_moving_avg_obs_fake_quant(dequantize_49, activation_post_process_25_observer_enabled, activation_post_process_25_fake_quant_enabled, activation_post_process_25_activation_post_process_min_val, activation_post_process_25_activation_post_process_max_val, activation_post_process_25_scale, activation_post_process_25_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_49 = activation_post_process_25_observer_enabled = activation_post_process_25_fake_quant_enabled = activation_post_process_25_activation_post_process_min_val = activation_post_process_25_activation_post_process_max_val = activation_post_process_25_scale = activation_post_process_25_zero_point = None\n",
      "    activation_post_process_25_scale_0 = self.activation_post_process_25_scale_0\n",
      "    activation_post_process_25_zero_point_0 = self.activation_post_process_25_zero_point_0\n",
      "    quantize_per_tensor_50 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_25, activation_post_process_25_scale_0, activation_post_process_25_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_25 = activation_post_process_25_scale_0 = activation_post_process_25_zero_point_0 = None\n",
      "    conv6_2 = getattr(self.conv6, \"2\")(quantize_per_tensor_50);  quantize_per_tensor_50 = None\n",
      "    dequantize_51 = conv6_2.dequantize();  conv6_2 = None\n",
      "    activation_post_process_26_observer_enabled = self.activation_post_process_26.observer_enabled\n",
      "    activation_post_process_26_fake_quant_enabled = self.activation_post_process_26.fake_quant_enabled\n",
      "    activation_post_process_26_activation_post_process_min_val = self.activation_post_process_26.activation_post_process.min_val\n",
      "    activation_post_process_26_activation_post_process_max_val = self.activation_post_process_26.activation_post_process.max_val\n",
      "    activation_post_process_26_scale = self.activation_post_process_26.scale\n",
      "    activation_post_process_26_zero_point = self.activation_post_process_26.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_26 = torch.fused_moving_avg_obs_fake_quant(dequantize_51, activation_post_process_26_observer_enabled, activation_post_process_26_fake_quant_enabled, activation_post_process_26_activation_post_process_min_val, activation_post_process_26_activation_post_process_max_val, activation_post_process_26_scale, activation_post_process_26_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_51 = activation_post_process_26_observer_enabled = activation_post_process_26_fake_quant_enabled = activation_post_process_26_activation_post_process_min_val = activation_post_process_26_activation_post_process_max_val = activation_post_process_26_scale = activation_post_process_26_zero_point = None\n",
      "    activation_post_process_26_scale_0 = self.activation_post_process_26_scale_0\n",
      "    activation_post_process_26_zero_point_0 = self.activation_post_process_26_zero_point_0\n",
      "    quantize_per_tensor_52 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_26, activation_post_process_26_scale_0, activation_post_process_26_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_26 = activation_post_process_26_scale_0 = activation_post_process_26_zero_point_0 = None\n",
      "    conv6_3 = getattr(self.conv6, \"3\")(quantize_per_tensor_52);  quantize_per_tensor_52 = None\n",
      "    dequantize_53 = conv6_3.dequantize();  conv6_3 = None\n",
      "    activation_post_process_26_observer_enabled_1 = self.activation_post_process_26.observer_enabled\n",
      "    activation_post_process_26_fake_quant_enabled_1 = self.activation_post_process_26.fake_quant_enabled\n",
      "    activation_post_process_26_activation_post_process_min_val_1 = self.activation_post_process_26.activation_post_process.min_val\n",
      "    activation_post_process_26_activation_post_process_max_val_1 = self.activation_post_process_26.activation_post_process.max_val\n",
      "    activation_post_process_26_scale_1 = self.activation_post_process_26.scale\n",
      "    activation_post_process_26_zero_point_1 = self.activation_post_process_26.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_27 = torch.fused_moving_avg_obs_fake_quant(dequantize_53, activation_post_process_26_observer_enabled_1, activation_post_process_26_fake_quant_enabled_1, activation_post_process_26_activation_post_process_min_val_1, activation_post_process_26_activation_post_process_max_val_1, activation_post_process_26_scale_1, activation_post_process_26_zero_point_1, 0.01, 0, 127, -1, False, False);  dequantize_53 = activation_post_process_26_observer_enabled_1 = activation_post_process_26_fake_quant_enabled_1 = activation_post_process_26_activation_post_process_min_val_1 = activation_post_process_26_activation_post_process_max_val_1 = activation_post_process_26_scale_1 = activation_post_process_26_zero_point_1 = None\n",
      "    activation_post_process_26_scale_2 = self.activation_post_process_26_scale_1\n",
      "    activation_post_process_26_zero_point_2 = self.activation_post_process_26_zero_point_1\n",
      "    quantize_per_tensor_54 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_27, activation_post_process_26_scale_2, activation_post_process_26_zero_point_2, torch.quint8);  fused_moving_avg_obs_fake_quant_27 = activation_post_process_26_scale_2 = activation_post_process_26_zero_point_2 = None\n",
      "    res3_0_0 = getattr(getattr(self.res3, \"0\"), \"0\")(quantize_per_tensor_54)\n",
      "    dequantize_55 = res3_0_0.dequantize();  res3_0_0 = None\n",
      "    activation_post_process_28_observer_enabled = self.activation_post_process_28.observer_enabled\n",
      "    activation_post_process_28_fake_quant_enabled = self.activation_post_process_28.fake_quant_enabled\n",
      "    activation_post_process_28_activation_post_process_min_val = self.activation_post_process_28.activation_post_process.min_val\n",
      "    activation_post_process_28_activation_post_process_max_val = self.activation_post_process_28.activation_post_process.max_val\n",
      "    activation_post_process_28_scale = self.activation_post_process_28.scale\n",
      "    activation_post_process_28_zero_point = self.activation_post_process_28.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_28 = torch.fused_moving_avg_obs_fake_quant(dequantize_55, activation_post_process_28_observer_enabled, activation_post_process_28_fake_quant_enabled, activation_post_process_28_activation_post_process_min_val, activation_post_process_28_activation_post_process_max_val, activation_post_process_28_scale, activation_post_process_28_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_55 = activation_post_process_28_observer_enabled = activation_post_process_28_fake_quant_enabled = activation_post_process_28_activation_post_process_min_val = activation_post_process_28_activation_post_process_max_val = activation_post_process_28_scale = activation_post_process_28_zero_point = None\n",
      "    activation_post_process_28_scale_0 = self.activation_post_process_28_scale_0\n",
      "    activation_post_process_28_zero_point_0 = self.activation_post_process_28_zero_point_0\n",
      "    quantize_per_tensor_56 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_28, activation_post_process_28_scale_0, activation_post_process_28_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_28 = activation_post_process_28_scale_0 = activation_post_process_28_zero_point_0 = None\n",
      "    res3_0_2 = getattr(getattr(self.res3, \"0\"), \"2\")(quantize_per_tensor_56);  quantize_per_tensor_56 = None\n",
      "    dequantize_57 = res3_0_2.dequantize();  res3_0_2 = None\n",
      "    activation_post_process_29_observer_enabled = self.activation_post_process_29.observer_enabled\n",
      "    activation_post_process_29_fake_quant_enabled = self.activation_post_process_29.fake_quant_enabled\n",
      "    activation_post_process_29_activation_post_process_min_val = self.activation_post_process_29.activation_post_process.min_val\n",
      "    activation_post_process_29_activation_post_process_max_val = self.activation_post_process_29.activation_post_process.max_val\n",
      "    activation_post_process_29_scale = self.activation_post_process_29.scale\n",
      "    activation_post_process_29_zero_point = self.activation_post_process_29.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_29 = torch.fused_moving_avg_obs_fake_quant(dequantize_57, activation_post_process_29_observer_enabled, activation_post_process_29_fake_quant_enabled, activation_post_process_29_activation_post_process_min_val, activation_post_process_29_activation_post_process_max_val, activation_post_process_29_scale, activation_post_process_29_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_57 = activation_post_process_29_observer_enabled = activation_post_process_29_fake_quant_enabled = activation_post_process_29_activation_post_process_min_val = activation_post_process_29_activation_post_process_max_val = activation_post_process_29_scale = activation_post_process_29_zero_point = None\n",
      "    activation_post_process_29_scale_0 = self.activation_post_process_29_scale_0\n",
      "    activation_post_process_29_zero_point_0 = self.activation_post_process_29_zero_point_0\n",
      "    quantize_per_tensor_58 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_29, activation_post_process_29_scale_0, activation_post_process_29_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_29 = activation_post_process_29_scale_0 = activation_post_process_29_zero_point_0 = None\n",
      "    res3_1_0 = getattr(getattr(self.res3, \"1\"), \"0\")(quantize_per_tensor_58);  quantize_per_tensor_58 = None\n",
      "    dequantize_59 = res3_1_0.dequantize();  res3_1_0 = None\n",
      "    activation_post_process_30_observer_enabled = self.activation_post_process_30.observer_enabled\n",
      "    activation_post_process_30_fake_quant_enabled = self.activation_post_process_30.fake_quant_enabled\n",
      "    activation_post_process_30_activation_post_process_min_val = self.activation_post_process_30.activation_post_process.min_val\n",
      "    activation_post_process_30_activation_post_process_max_val = self.activation_post_process_30.activation_post_process.max_val\n",
      "    activation_post_process_30_scale = self.activation_post_process_30.scale\n",
      "    activation_post_process_30_zero_point = self.activation_post_process_30.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_30 = torch.fused_moving_avg_obs_fake_quant(dequantize_59, activation_post_process_30_observer_enabled, activation_post_process_30_fake_quant_enabled, activation_post_process_30_activation_post_process_min_val, activation_post_process_30_activation_post_process_max_val, activation_post_process_30_scale, activation_post_process_30_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_59 = activation_post_process_30_observer_enabled = activation_post_process_30_fake_quant_enabled = activation_post_process_30_activation_post_process_min_val = activation_post_process_30_activation_post_process_max_val = activation_post_process_30_scale = activation_post_process_30_zero_point = None\n",
      "    activation_post_process_30_scale_0 = self.activation_post_process_30_scale_0\n",
      "    activation_post_process_30_zero_point_0 = self.activation_post_process_30_zero_point_0\n",
      "    quantize_per_tensor_60 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_30, activation_post_process_30_scale_0, activation_post_process_30_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_30 = activation_post_process_30_scale_0 = activation_post_process_30_zero_point_0 = None\n",
      "    res3_1_2 = getattr(getattr(self.res3, \"1\"), \"2\")(quantize_per_tensor_60);  quantize_per_tensor_60 = None\n",
      "    dequantize_61 = res3_1_2.dequantize();  res3_1_2 = None\n",
      "    activation_post_process_31_observer_enabled = self.activation_post_process_31.observer_enabled\n",
      "    activation_post_process_31_fake_quant_enabled = self.activation_post_process_31.fake_quant_enabled\n",
      "    activation_post_process_31_activation_post_process_min_val = self.activation_post_process_31.activation_post_process.min_val\n",
      "    activation_post_process_31_activation_post_process_max_val = self.activation_post_process_31.activation_post_process.max_val\n",
      "    activation_post_process_31_scale = self.activation_post_process_31.scale\n",
      "    activation_post_process_31_zero_point = self.activation_post_process_31.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_31 = torch.fused_moving_avg_obs_fake_quant(dequantize_61, activation_post_process_31_observer_enabled, activation_post_process_31_fake_quant_enabled, activation_post_process_31_activation_post_process_min_val, activation_post_process_31_activation_post_process_max_val, activation_post_process_31_scale, activation_post_process_31_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_61 = activation_post_process_31_observer_enabled = activation_post_process_31_fake_quant_enabled = activation_post_process_31_activation_post_process_min_val = activation_post_process_31_activation_post_process_max_val = activation_post_process_31_scale = activation_post_process_31_zero_point = None\n",
      "    activation_post_process_31_scale_0 = self.activation_post_process_31_scale_0\n",
      "    activation_post_process_31_zero_point_0 = self.activation_post_process_31_zero_point_0\n",
      "    quantize_per_tensor_62 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_31, activation_post_process_31_scale_0, activation_post_process_31_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_31 = activation_post_process_31_scale_0 = activation_post_process_31_zero_point_0 = None\n",
      "    _scale_2 = self._scale_2\n",
      "    _zero_point_2 = self._zero_point_2\n",
      "    add_5 = torch.ops.quantized.add(quantize_per_tensor_62, quantize_per_tensor_54, _scale_2, _zero_point_2);  quantize_per_tensor_62 = quantize_per_tensor_54 = _scale_2 = _zero_point_2 = None\n",
      "    dequantize_63 = add_5.dequantize();  add_5 = None\n",
      "    activation_post_process_32_observer_enabled = self.activation_post_process_32.observer_enabled\n",
      "    activation_post_process_32_fake_quant_enabled = self.activation_post_process_32.fake_quant_enabled\n",
      "    activation_post_process_32_activation_post_process_min_val = self.activation_post_process_32.activation_post_process.min_val\n",
      "    activation_post_process_32_activation_post_process_max_val = self.activation_post_process_32.activation_post_process.max_val\n",
      "    activation_post_process_32_scale = self.activation_post_process_32.scale\n",
      "    activation_post_process_32_zero_point = self.activation_post_process_32.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_32 = torch.fused_moving_avg_obs_fake_quant(dequantize_63, activation_post_process_32_observer_enabled, activation_post_process_32_fake_quant_enabled, activation_post_process_32_activation_post_process_min_val, activation_post_process_32_activation_post_process_max_val, activation_post_process_32_scale, activation_post_process_32_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_63 = activation_post_process_32_observer_enabled = activation_post_process_32_fake_quant_enabled = activation_post_process_32_activation_post_process_min_val = activation_post_process_32_activation_post_process_max_val = activation_post_process_32_scale = activation_post_process_32_zero_point = None\n",
      "    activation_post_process_32_scale_0 = self.activation_post_process_32_scale_0\n",
      "    activation_post_process_32_zero_point_0 = self.activation_post_process_32_zero_point_0\n",
      "    quantize_per_tensor_64 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_32, activation_post_process_32_scale_0, activation_post_process_32_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_32 = activation_post_process_32_scale_0 = activation_post_process_32_zero_point_0 = None\n",
      "    drop3 = self.drop3(quantize_per_tensor_64);  quantize_per_tensor_64 = None\n",
      "    dequantize_65 = drop3.dequantize();  drop3 = None\n",
      "    activation_post_process_33_observer_enabled = self.activation_post_process_33.observer_enabled\n",
      "    activation_post_process_33_fake_quant_enabled = self.activation_post_process_33.fake_quant_enabled\n",
      "    activation_post_process_33_activation_post_process_min_val = self.activation_post_process_33.activation_post_process.min_val\n",
      "    activation_post_process_33_activation_post_process_max_val = self.activation_post_process_33.activation_post_process.max_val\n",
      "    activation_post_process_33_scale = self.activation_post_process_33.scale\n",
      "    activation_post_process_33_zero_point = self.activation_post_process_33.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_33 = torch.fused_moving_avg_obs_fake_quant(dequantize_65, activation_post_process_33_observer_enabled, activation_post_process_33_fake_quant_enabled, activation_post_process_33_activation_post_process_min_val, activation_post_process_33_activation_post_process_max_val, activation_post_process_33_scale, activation_post_process_33_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_65 = activation_post_process_33_observer_enabled = activation_post_process_33_fake_quant_enabled = activation_post_process_33_activation_post_process_min_val = activation_post_process_33_activation_post_process_max_val = activation_post_process_33_scale = activation_post_process_33_zero_point = None\n",
      "    activation_post_process_33_scale_0 = self.activation_post_process_33_scale_0\n",
      "    activation_post_process_33_zero_point_0 = self.activation_post_process_33_zero_point_0\n",
      "    quantize_per_tensor_66 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_33, activation_post_process_33_scale_0, activation_post_process_33_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_33 = activation_post_process_33_scale_0 = activation_post_process_33_zero_point_0 = None\n",
      "    classifier_0 = getattr(self.classifier, \"0\")(quantize_per_tensor_66);  quantize_per_tensor_66 = None\n",
      "    dequantize_67 = classifier_0.dequantize();  classifier_0 = None\n",
      "    activation_post_process_33_observer_enabled_1 = self.activation_post_process_33.observer_enabled\n",
      "    activation_post_process_33_fake_quant_enabled_1 = self.activation_post_process_33.fake_quant_enabled\n",
      "    activation_post_process_33_activation_post_process_min_val_1 = self.activation_post_process_33.activation_post_process.min_val\n",
      "    activation_post_process_33_activation_post_process_max_val_1 = self.activation_post_process_33.activation_post_process.max_val\n",
      "    activation_post_process_33_scale_1 = self.activation_post_process_33.scale\n",
      "    activation_post_process_33_zero_point_1 = self.activation_post_process_33.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_34 = torch.fused_moving_avg_obs_fake_quant(dequantize_67, activation_post_process_33_observer_enabled_1, activation_post_process_33_fake_quant_enabled_1, activation_post_process_33_activation_post_process_min_val_1, activation_post_process_33_activation_post_process_max_val_1, activation_post_process_33_scale_1, activation_post_process_33_zero_point_1, 0.01, 0, 127, -1, False, False);  dequantize_67 = activation_post_process_33_observer_enabled_1 = activation_post_process_33_fake_quant_enabled_1 = activation_post_process_33_activation_post_process_min_val_1 = activation_post_process_33_activation_post_process_max_val_1 = activation_post_process_33_scale_1 = activation_post_process_33_zero_point_1 = None\n",
      "    classifier_1 = getattr(self.classifier, \"1\")(fused_moving_avg_obs_fake_quant_34);  fused_moving_avg_obs_fake_quant_34 = None\n",
      "    activation_post_process_35_observer_enabled = self.activation_post_process_35.observer_enabled\n",
      "    activation_post_process_35_fake_quant_enabled = self.activation_post_process_35.fake_quant_enabled\n",
      "    activation_post_process_35_activation_post_process_min_val = self.activation_post_process_35.activation_post_process.min_val\n",
      "    activation_post_process_35_activation_post_process_max_val = self.activation_post_process_35.activation_post_process.max_val\n",
      "    activation_post_process_35_scale = self.activation_post_process_35.scale\n",
      "    activation_post_process_35_zero_point = self.activation_post_process_35.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_35 = torch.fused_moving_avg_obs_fake_quant(classifier_1, activation_post_process_35_observer_enabled, activation_post_process_35_fake_quant_enabled, activation_post_process_35_activation_post_process_min_val, activation_post_process_35_activation_post_process_max_val, activation_post_process_35_scale, activation_post_process_35_zero_point, 0.01, 0, 127, -1, False, False);  classifier_1 = activation_post_process_35_observer_enabled = activation_post_process_35_fake_quant_enabled = activation_post_process_35_activation_post_process_min_val = activation_post_process_35_activation_post_process_max_val = activation_post_process_35_scale = activation_post_process_35_zero_point = None\n",
      "    activation_post_process_35_scale_0 = self.activation_post_process_35_scale_0\n",
      "    activation_post_process_35_zero_point_0 = self.activation_post_process_35_zero_point_0\n",
      "    quantize_per_tensor_68 = torch.quantize_per_tensor(fused_moving_avg_obs_fake_quant_35, activation_post_process_35_scale_0, activation_post_process_35_zero_point_0, torch.quint8);  fused_moving_avg_obs_fake_quant_35 = activation_post_process_35_scale_0 = activation_post_process_35_zero_point_0 = None\n",
      "    classifier_2 = getattr(self.classifier, \"2\")(quantize_per_tensor_68);  quantize_per_tensor_68 = None\n",
      "    dequantize_69 = classifier_2.dequantize();  classifier_2 = None\n",
      "    activation_post_process_36_observer_enabled = self.activation_post_process_36.observer_enabled\n",
      "    activation_post_process_36_fake_quant_enabled = self.activation_post_process_36.fake_quant_enabled\n",
      "    activation_post_process_36_activation_post_process_min_val = self.activation_post_process_36.activation_post_process.min_val\n",
      "    activation_post_process_36_activation_post_process_max_val = self.activation_post_process_36.activation_post_process.max_val\n",
      "    activation_post_process_36_scale = self.activation_post_process_36.scale\n",
      "    activation_post_process_36_zero_point = self.activation_post_process_36.zero_point\n",
      "    fused_moving_avg_obs_fake_quant_36 = torch.fused_moving_avg_obs_fake_quant(dequantize_69, activation_post_process_36_observer_enabled, activation_post_process_36_fake_quant_enabled, activation_post_process_36_activation_post_process_min_val, activation_post_process_36_activation_post_process_max_val, activation_post_process_36_scale, activation_post_process_36_zero_point, 0.01, 0, 127, -1, False, False);  dequantize_69 = activation_post_process_36_observer_enabled = activation_post_process_36_fake_quant_enabled = activation_post_process_36_activation_post_process_min_val = activation_post_process_36_activation_post_process_max_val = activation_post_process_36_scale = activation_post_process_36_zero_point = None\n",
      "    return fused_moving_avg_obs_fake_quant_36\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print('Original model:')\n",
    "print(model)\n",
    "print('')\n",
    "print('Quantized model:')\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# save the model and check the model size\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 43333.998\n",
      "model:  int8  \t Size (KB): 10948.934\n",
      "3.96 times smaller\n"
     ]
    }
   ],
   "source": [
    "f=print_size_of_model(model,\"fp32\")\n",
    "q=print_size_of_model(quantized_model,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_validation_epoch_end(outputs):\n",
    "    batch_losses = [x['val_loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()\n",
    "    batch_accs = [x['val_acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()\n",
    "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "def quant_evaluate(model, val_loader):\n",
    "    outputs = [quant_validation_step(model,batch) for batch in val_loader]\n",
    "    return quant_validation_epoch_end(outputs)\n",
    "def quant_validation_step(model, batch):\n",
    "    print(1)\n",
    "    images, labels = batch \n",
    "    out = model(images)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    acc = accuracy(out, labels)\n",
    "    return {'val_loss': loss.detach(), 'val_acc': acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "valid_dl = DeviceDataLoader(valid_dl, devi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = to_device(quantized_model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\fx\\graph_module.py\", line 304, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<eval_with_key>.14\", line 12, in forward\n",
      "    fused_moving_avg_obs_fake_quant = torch.fused_moving_avg_obs_fake_quant(to, activation_post_process_0_observer_enabled, activation_post_process_0_fake_quant_enabled, activation_post_process_0_activation_post_process_min_val, activation_post_process_0_activation_post_process_max_val, activation_post_process_0_scale, activation_post_process_0_zero_point, 0.01, 0, 127, -1, False, False);  to = activation_post_process_0_observer_enabled = activation_post_process_0_fake_quant_enabled = activation_post_process_0_activation_post_process_min_val = activation_post_process_0_activation_post_process_max_val = activation_post_process_0_scale = activation_post_process_0_zero_point = None\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument observer_on in method wrapper_CUDA___fused_moving_avg_obs_fq_helper)\n",
      "\n",
      "Call using an FX-traced Module, line 12 of the traced Module's generated forward function:\n",
      "    activation_post_process_0_zero_point = self.activation_post_process_0.zero_point\n",
      "    fused_moving_avg_obs_fake_quant = torch.fused_moving_avg_obs_fake_quant(to, activation_post_process_0_observer_enabled, activation_post_process_0_fake_quant_enabled, activation_post_process_0_activation_post_process_min_val, activation_post_process_0_activation_post_process_max_val, activation_post_process_0_scale, activation_post_process_0_zero_point, 0.01, 0, 127, -1, False, False);  to = activation_post_process_0_observer_enabled = activation_post_process_0_fake_quant_enabled = activation_post_process_0_activation_post_process_min_val = activation_post_process_0_activation_post_process_max_val = activation_post_process_0_scale = activation_post_process_0_zero_point = None\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    activation_post_process_0_scale_0 = self.activation_post_process_0_scale_0\n",
      "\n",
      "    activation_post_process_0_zero_point_0 = self.activation_post_process_0_zero_point_0\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument observer_on in method wrapper_CUDA___fused_moving_avg_obs_fq_helper)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eval_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mquant_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_accuracy)\n",
      "Cell \u001b[1;32mIn[65], line 8\u001b[0m, in \u001b[0;36mquant_evaluate\u001b[1;34m(model, val_loader)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquant_evaluate\u001b[39m(model, val_loader):\n\u001b[1;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mquant_validation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quant_validation_epoch_end(outputs)\n",
      "Cell \u001b[1;32mIn[65], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquant_evaluate\u001b[39m(model, val_loader):\n\u001b[1;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mquant_validation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quant_validation_epoch_end(outputs)\n",
      "Cell \u001b[1;32mIn[65], line 13\u001b[0m, in \u001b[0;36mquant_validation_step\u001b[1;34m(model, batch)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch \n\u001b[1;32m---> 13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out, labels)\n\u001b[0;32m     15\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy(out, labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\fx\\graph_module.py:738\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\fx\\graph_module.py:315\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[1;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_with_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m topmost_framesummary\u001b[38;5;241m.\u001b[39mfilename:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    312\u001b[0m         _WrappedCall\u001b[38;5;241m.\u001b[39m_generate_error_message(topmost_framesummary),\n\u001b[0;32m    313\u001b[0m         file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    314\u001b[0m     )\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# noqa: TRY200\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument observer_on in method wrapper_CUDA___fused_moving_avg_obs_fq_helper)"
     ]
    }
   ],
   "source": [
    "eval_accuracy = quant_evaluate(quantized_model, valid_dl)\n",
    "\n",
    "\n",
    "\n",
    "print(eval_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Floating point FP32: \n",
      "3.39 s ± 576 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Quantized INT8: \n",
      "4.25 s ± 95.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "speedtest_inputs = torch.randn(100, 1, 48, 48, requires_grad=True).to(device)\n",
    "\n",
    "print(\"Floating point FP32: \")\n",
    "%timeit model.forward(speedtest_inputs)\n",
    "\n",
    "print(\"Quantized INT8: \")\n",
    "%timeit quantized_model.forward(speedtest_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(), \"quant_model_0.556.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(quantized_model) # Export to TorchScript\n",
    "model_scripted.save('quant_model_scripted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
