{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Train Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Validation Classes - ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "print(os.listdir(data_dir))\n",
    "classes_train = os.listdir(data_dir + \"/train\")\n",
    "classes_valid = os.listdir(data_dir + \"/validation\")\n",
    "print(f'Train Classes - {classes_train}')\n",
    "print(f'Validation Classes - {classes_valid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tt.Compose([tt.Grayscale(num_output_channels=1),\n",
    "                         tt.RandomHorizontalFlip(),\n",
    "                         tt.RandomRotation(30),\n",
    "                         tt.ToTensor()])\n",
    "\n",
    "valid_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "best_model = 0\n",
    "result_dir = './photos'\n",
    "print(os.listdir(result_dir))\n",
    "result_tfms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds = [result_tfms(PIL.Image.open('./photos/'+path).resize((48, 48)))for path in os.listdir(result_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DataLoader(result_ds, batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(data_dir + '/train', train_tfms)\n",
    "valid_ds = ImageFolder(data_dir + '/validation', valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_res_batch(dl):\n",
    "    for images in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        print(images[0].shape)\n",
    "        ax.imshow(make_grid(images[:3], nrow=3).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device\n",
    "print('GPU: ' + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dl = DeviceDataLoader(result_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаги обучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    def pred_step(self, batch):\n",
    "        images = batch \n",
    "        out = self(images)\n",
    "        return out\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        global best_model, new_model\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        new_model = result['val_acc']\n",
    "        if new_model > best_model:\n",
    "            best_model = new_model\n",
    "            torch.save(model.state_dict(), './models/emotion_detection_acc'+str(best_model)+'.pth')\n",
    "            print('save ', './models/emotion_detection_acc'+str(best_model)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ELU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 128)\n",
    "        self.conv2 = conv_block(128, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256)\n",
    "        self.conv4 = conv_block(256, 256, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv5 = conv_block(256, 512)\n",
    "        self.conv6 = conv_block(512, 512, pool=True)\n",
    "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(6), \n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = to_device(xb,device)\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.drop2(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.drop3(out)\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = to_device(ResNet(1, len(classes_train)), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(1, len(classes_train))\n",
    "model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "first_parameter = next(model.parameters())\n",
    "input_shape = first_parameter.size()\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 41.149MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu used 43154432 memory\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpu used {torch.cuda.max_memory_allocated(device=None)} memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def predict(model, pred_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.pred_step(batch) for batch in pred_loader]\n",
    "    return [torch.max(el, dim=1)[1] for el in outputs]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        \n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, max_lr, model, train_loader, val_loader, device,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        print(result)\n",
    "        #model.epoch_end(epoch, result)\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 130\n",
    "max_lr = 0.0008\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig_mapping = get_default_qat_qconfig_mapping(\"fbgemm\")\n",
    "example_inputs = torch.randn(batch_size, 1, 48, 48, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(1, len(classes_train))\n",
    "model.load_state_dict(torch.load('./models/emotion_detection_acc0.5452366471290588.pth'))\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prepared_model_static = prepare_qat_fx(model, qconfig_mapping, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_dl):\n",
    "    prepared_model_static(data)\n",
    "    if batch_idx % 10 == 0:\n",
    "        break\n",
    "        print(\"Batch %d/%d complete, continue ...\" %(batch_idx+1, len(valid_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = convert_fx(prepared_model_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "ResNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res1): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res3): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (drop3): Dropout(p=0.5, inplace=False)\n",
      "  (classifier): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Quantized model:\n",
      "GraphModule(\n",
      "  (conv1): Module(\n",
      "    (0): QuantizedConv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.1847992241382599, zero_point=62, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "  )\n",
      "  (conv2): Module(\n",
      "    (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.16406957805156708, zero_point=64, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res1): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.13893195986747742, zero_point=71, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.11479467898607254, zero_point=62, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (drop1): QuantizedDropout(p=0.5, inplace=False)\n",
      "  (conv3): Module(\n",
      "    (0): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.13184614479541779, zero_point=65, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "  )\n",
      "  (conv4): Module(\n",
      "    (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.11200389266014099, zero_point=57, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res2): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.10115267336368561, zero_point=61, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.09020169824361801, zero_point=59, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (drop2): QuantizedDropout(p=0.5, inplace=False)\n",
      "  (conv5): Module(\n",
      "    (0): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.10123937577009201, zero_point=64, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "  )\n",
      "  (conv6): Module(\n",
      "    (0): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.0968499481678009, zero_point=56, padding=(1, 1))\n",
      "    (2): QuantizedELU(alpha=1.0)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res3): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.08198423683643341, zero_point=57, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07025375962257385, zero_point=64, padding=(1, 1))\n",
      "      (2): QuantizedELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (drop3): QuantizedDropout(p=0.5, inplace=False)\n",
      "  (classifier): Module(\n",
      "    (0): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): QuantizedLinear(in_features=512, out_features=7, scale=0.12937161326408386, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, xb):\n",
      "    to = xb.to('cpu', non_blocking = True);  xb = None\n",
      "    _scale_0 = self._scale_0\n",
      "    _zero_point_0 = self._zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(to, _scale_0, _zero_point_0, torch.quint8);  to = _scale_0 = _zero_point_0 = None\n",
      "    conv1_0 = getattr(self.conv1, \"0\")(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    conv1_2 = getattr(self.conv1, \"2\")(conv1_0);  conv1_0 = None\n",
      "    conv2_0 = getattr(self.conv2, \"0\")(conv1_2);  conv1_2 = None\n",
      "    conv2_2 = getattr(self.conv2, \"2\")(conv2_0);  conv2_0 = None\n",
      "    conv2_3 = getattr(self.conv2, \"3\")(conv2_2);  conv2_2 = None\n",
      "    res1_0_0 = getattr(getattr(self.res1, \"0\"), \"0\")(conv2_3)\n",
      "    res1_0_2 = getattr(getattr(self.res1, \"0\"), \"2\")(res1_0_0);  res1_0_0 = None\n",
      "    res1_1_0 = getattr(getattr(self.res1, \"1\"), \"0\")(res1_0_2);  res1_0_2 = None\n",
      "    res1_1_2 = getattr(getattr(self.res1, \"1\"), \"2\")(res1_1_0);  res1_1_0 = None\n",
      "    _scale_1 = self._scale_1\n",
      "    _zero_point_1 = self._zero_point_1\n",
      "    add_3 = torch.ops.quantized.add(res1_1_2, conv2_3, _scale_1, _zero_point_1);  res1_1_2 = conv2_3 = _scale_1 = _zero_point_1 = None\n",
      "    drop1 = self.drop1(add_3);  add_3 = None\n",
      "    conv3_0 = getattr(self.conv3, \"0\")(drop1);  drop1 = None\n",
      "    conv3_2 = getattr(self.conv3, \"2\")(conv3_0);  conv3_0 = None\n",
      "    conv4_0 = getattr(self.conv4, \"0\")(conv3_2);  conv3_2 = None\n",
      "    conv4_2 = getattr(self.conv4, \"2\")(conv4_0);  conv4_0 = None\n",
      "    conv4_3 = getattr(self.conv4, \"3\")(conv4_2);  conv4_2 = None\n",
      "    res2_0_0 = getattr(getattr(self.res2, \"0\"), \"0\")(conv4_3)\n",
      "    res2_0_2 = getattr(getattr(self.res2, \"0\"), \"2\")(res2_0_0);  res2_0_0 = None\n",
      "    res2_1_0 = getattr(getattr(self.res2, \"1\"), \"0\")(res2_0_2);  res2_0_2 = None\n",
      "    res2_1_2 = getattr(getattr(self.res2, \"1\"), \"2\")(res2_1_0);  res2_1_0 = None\n",
      "    _scale_2 = self._scale_2\n",
      "    _zero_point_2 = self._zero_point_2\n",
      "    add_4 = torch.ops.quantized.add(res2_1_2, conv4_3, _scale_2, _zero_point_2);  res2_1_2 = conv4_3 = _scale_2 = _zero_point_2 = None\n",
      "    drop2 = self.drop2(add_4);  add_4 = None\n",
      "    conv5_0 = getattr(self.conv5, \"0\")(drop2);  drop2 = None\n",
      "    conv5_2 = getattr(self.conv5, \"2\")(conv5_0);  conv5_0 = None\n",
      "    conv6_0 = getattr(self.conv6, \"0\")(conv5_2);  conv5_2 = None\n",
      "    conv6_2 = getattr(self.conv6, \"2\")(conv6_0);  conv6_0 = None\n",
      "    conv6_3 = getattr(self.conv6, \"3\")(conv6_2);  conv6_2 = None\n",
      "    res3_0_0 = getattr(getattr(self.res3, \"0\"), \"0\")(conv6_3)\n",
      "    res3_0_2 = getattr(getattr(self.res3, \"0\"), \"2\")(res3_0_0);  res3_0_0 = None\n",
      "    res3_1_0 = getattr(getattr(self.res3, \"1\"), \"0\")(res3_0_2);  res3_0_2 = None\n",
      "    res3_1_2 = getattr(getattr(self.res3, \"1\"), \"2\")(res3_1_0);  res3_1_0 = None\n",
      "    _scale_3 = self._scale_3\n",
      "    _zero_point_3 = self._zero_point_3\n",
      "    add_5 = torch.ops.quantized.add(res3_1_2, conv6_3, _scale_3, _zero_point_3);  res3_1_2 = conv6_3 = _scale_3 = _zero_point_3 = None\n",
      "    drop3 = self.drop3(add_5);  add_5 = None\n",
      "    classifier_0 = getattr(self.classifier, \"0\")(drop3);  drop3 = None\n",
      "    dequantize_34 = classifier_0.dequantize();  classifier_0 = None\n",
      "    classifier_1 = getattr(self.classifier, \"1\")(dequantize_34);  dequantize_34 = None\n",
      "    classifier_1_scale_0 = self.classifier_1_scale_0\n",
      "    classifier_1_zero_point_0 = self.classifier_1_zero_point_0\n",
      "    quantize_per_tensor_35 = torch.quantize_per_tensor(classifier_1, classifier_1_scale_0, classifier_1_zero_point_0, torch.quint8);  classifier_1 = classifier_1_scale_0 = classifier_1_zero_point_0 = None\n",
      "    classifier_2 = getattr(self.classifier, \"2\")(quantize_per_tensor_35);  quantize_per_tensor_35 = None\n",
      "    dequantize_36 = classifier_2.dequantize();  classifier_2 = None\n",
      "    return dequantize_36\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print('Original model:')\n",
    "print(model)\n",
    "print('')\n",
    "print('Quantized model:')\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# save the model and check the model size\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 43169.862\n",
      "model:  int8  \t Size (KB): 10863.994\n",
      "3.97 times smaller\n"
     ]
    }
   ],
   "source": [
    "f=print_size_of_model(model,\"fp32\")\n",
    "q=print_size_of_model(quantized_model,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_validation_epoch_end(outputs):\n",
    "    batch_losses = [x['val_loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()\n",
    "    batch_accs = [x['val_acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()\n",
    "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "def quant_evaluate(model, val_loader):\n",
    "    outputs = [quant_validation_step(model,batch) for batch in val_loader]\n",
    "    return quant_validation_epoch_end(outputs)\n",
    "def quant_validation_step(model, batch):\n",
    "    print(1)\n",
    "    images, labels = batch \n",
    "    out = model(images)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    acc = accuracy(out, labels)\n",
    "    return {'val_loss': loss.detach(), 'val_acc': acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "valid_dl = DeviceDataLoader(valid_dl, devi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = to_device(quantized_model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "{'val_loss': 1.224511742591858, 'val_acc': 0.5483987331390381}\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy = quant_evaluate(quantized_model, valid_dl)\n",
    "\n",
    "\n",
    "\n",
    "print(eval_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Floating point FP32: \n",
      "7.52 s ± 247 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Quantized INT8: \n",
      "8.29 s ± 172 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "speedtest_inputs = torch.randn(100, 1, 48, 48, requires_grad=True).to(device)\n",
    "\n",
    "print(\"Floating point FP32: \")\n",
    "%timeit model.forward(speedtest_inputs)\n",
    "\n",
    "print(\"Quantized INT8: \")\n",
    "%timeit quantized_model.forward(speedtest_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(), \"quant_model_0.556.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(quantized_model) # Export to TorchScript\n",
    "model_scripted.save('quant_model_scripted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
